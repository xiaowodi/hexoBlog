
---
title: 2021年8月秋招复习笔记
---



# Java基础复习（JDK1.8）



## 容器篇

### 1.ArrayList

关键源码

```java
    /**
     * Default initial capacity.
     */
		// 默认初识容量为10
    private static final int DEFAULT_CAPACITY = 10;

    /**
     * Shared empty array instance used for empty instances.
     */
    private static final Object[] EMPTY_ELEMENTDATA = {};

    /**
     * Shared empty array instance used for default sized empty instances. We
     * distinguish this from EMPTY_ELEMENTDATA to know how much to inflate when
     * first element is added.
     */
    private static final Object[] DEFAULTCAPACITY_EMPTY_ELEMENTDATA = {};

    transient Object[] elementData; // non-private to simplify nested class access

		// 调用空参数构造方法之后，列表还是一个空的列表
    public ArrayList() {
        this.elementData = DEFAULTCAPACITY_EMPTY_ELEMENTDATA;
    }

	    /**
     * The maximum size of array to allocate.
     * Some VMs reserve some header words in an array.
     * Attempts to allocate larger arrays may result in
     * OutOfMemoryError: Requested array size exceeds VM limit
     */
		// 最大可库容的容量阈值， 当容量超过这个值的时候，ArrayList的最大容量为Integer.MAX_VALUE
    private static final int MAX_ARRAY_SIZE = Integer.MAX_VALUE - 8;

		// 添加元素
    public boolean add(E e) {
        ensureCapacityInternal(size + 1);  // Increments modCount!!
        elementData[size++] = e;
        return true;
    }
    private void ensureCapacityInternal(int minCapacity) {
        ensureExplicitCapacity(calculateCapacity(elementData, minCapacity));
    }
    private static int calculateCapacity(Object[] elementData, int minCapacity) {
        if (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) {
            return Math.max(DEFAULT_CAPACITY, minCapacity);
        }
        return minCapacity;
    }
    private void ensureExplicitCapacity(int minCapacity) {
        modCount++;

        // overflow-conscious code
        if (minCapacity - elementData.length > 0)
            grow(minCapacity);
    }
		// 扩容关键性代码
    private void grow(int minCapacity) {
        // overflow-conscious code
        int oldCapacity = elementData.length;
        int newCapacity = oldCapacity + (oldCapacity >> 1);  // 每次扩容为原来的1.5倍
        if (newCapacity - minCapacity < 0)
            newCapacity = minCapacity; //扩容后如果不够，那么就直接扩容为当前容量+1
        if (newCapacity - MAX_ARRAY_SIZE > 0)
            newCapacity = hugeCapacity(minCapacity);
        // minCapacity is usually close to size, so this is a win:
        elementData = Arrays.copyOf(elementData, newCapacity);  // 通过拷贝数据中的元素到新的数组，进行扩容
    }
		// 判断是否超过最大的扩容阈值
    private static int hugeCapacity(int minCapacity) {
        if (minCapacity < 0) // overflow
            throw new OutOfMemoryError();
        return (minCapacity > MAX_ARRAY_SIZE) ?
            Integer.MAX_VALUE :
            MAX_ARRAY_SIZE;
    }
```

**总结：**

- ArrayList底层是通过数组实现的
- ArrayList的默认容量为10；
- ArrayList为懒加载，只有在添加了第一个元素之后才会真正分配空间
- 扩容时，每次扩容为原来容量的1.5倍：原来的容量值+容量值>>1
- 如果扩容后容量超过Integer.MAX_VALUE-8，ArrayList的容量就为Integer的最大值
- 每次扩容时，是通过将旧的数组中的元素拷贝到扩容后的新的数组中
- 查询、更新元素效率高



### 2.LinkedList

- LinkedList底层是通过双向链表实现的
- 可以当成Stack与Queue来实现
- 插入，删除元素效率高



### 3.HashMap

[HashMap原理](https://editor.csdn.net/md/?articleId=113561579)

小总结：

- 初识主数组长度：16（1<<4）
- 主数组最大长度：2^30
- 默认的负载因子0.75
- 链表树化阈值1：8
- 链表树化阈值2：主数组table长度超过64
- 红黑树退化成链表阈值：树节点少于6

> HashMap的主数组长度需要满足2的幂次方
>
> 比如输入为1，table长度为2
>
> 输入长度为15，table长度为16

#### HashMap插入元素底层原理

1. 插入元素前对key的HashCode进行扰动函数hash()计算

   1. > 现获取key的hashCode的值h，然后将h与h的高16位进行异或运算
      >
      > 其目的是在进行路由寻址的时候，能够保证在元素个数较少的情况下，路由地址会同时保持高16位和低16位的共同特征

2. 插入元素

   1. > **路由公式： i=hash & (table.length - 1)**
      >
      > 为什么是table.length - 1，而不是table.length呢？
      >
      > 因为table.length为2的幂次方计算出来（1000000000），0多1少
      >
      > 直接与hash进行&运算的时候，都会变为0，更容易发生hash冲突
      >
      > -1的目的就是将众多的0变为1，*与运算之后的值更不容易相同，缓解hash冲突。*

   2. >路由地址计算出来后，就要插入元素
      >
      >**插入情景1**：主数组i位置为null（没有冲突），直接插入元素
      >
      >**插入情景2**：主数组i位置存在元素，且key值相同，就新的value值覆盖旧的value值
      >
      >**插入情景3**：主数组i位置存在元素，且没有树化，尾插法插入链表
      >
      >​                      如果链表长度超过8，同时主数组长度打到64， 才开始树化
      >
      >**插入情景4**：主数组i位置存在元素，且已经树化成红黑树，向红黑树中插入元素

3. 元素插入达到阈值，进行扩容

   1. > 阈值计算：当前主数组长度 * 负载因子

   2. 主数组每次扩容原来的一倍：通过向左移位来实现（避免经过乘法器，耗性能）

   3. 扩容情景：

      1. 主数组对应的slot内没有元素（null），不做处理

      2. 主数组对应的slot内有元素，但是没有链化，直接用改元素的扰动值hash直接与（新的数组长度-1）进行&运算，计算出新的位置

      3. 主数组对应的slot内有元素，但是已经链化了

         1. > 这时就需要进行高低链分链
            >
            > 用key的hash值（扰动后的）与扩容前的旧容量进行&运算，如果为0，即为低链；不为0,即为高链
            >
            > 低链的元素扩容后还在**原索引**位置；高链的元素扩容后在**原索引+旧容量**处

         2. 如果是红黑树进行分链的时候，可能元素会少于6个，这个时候就需要退化成链表

<font color=red>HashMap在高并发的情况下，链表会出现环形链表</font>

<font color=green>这里提及一道经典面试题：如何判断一个链表是否有环？？？</font>

使用快慢指针（double pointer）

slow和fast：slow每次走一步，fast每次走两步；如果两个指针能相遇，一定存在环儿；

（<font color=blue>生活中的例子：两个人跑圈，快的人在第二圈的时候一定会遇到慢的那个人</font>）

### 4.线程安全的容器

上述提及的容器类都是线程不安全的容器类，在并发环境下应该避免使用

#### 线程安全的List

```java
List list = new ArrayList<>(); // 效率高，不支持并发


List list = new Vector<>(); // 线程安全，但是效率低
/**
vector 在添加元素的一些操作的方法，添加了synchronized关键，进行上锁，效率比较低
*/

List list = Collections.synchronizedList(new ArrayList<>()); // 线程安全，小数量完全可以
/*
synchronizedList内部是通过在具体的操作上包裹synchronized关键字，而不是粗暴的同步整个方法

*/


List list = new CopyOnWriteArrayList<>(); // 线程安全，JUC包下的类（写时复制）,适用于多线程环境

/*
每次添加元素的时候，先将集合中的元素复制到一个长度+1的新的数组中，然后将新增的元素添加到新的数组中，然后再将数组引用指向新的数组中。

这就保证了：读和写是在不同的对象上进行的，所以不存在资源竞争关系，不需要加锁
					读写分离思想
*/
```

#### 线程安全的HashMap

```java
// Collections.synchronizedMap
Map<Object, Object> synchronizedMap = Collections.synchronizedMap(new HashMap<>());

// ConcurrentHashMap
ConcurrentHashMap<Object, Object> map = new ConcurrentHashMap<>();
```

##### ConCurrentHashMap

1.7 [ConCurrentHashMap小灰漫画](https://zhuanlan.zhihu.com/p/31614308)













## JVM篇

JVM：java虚拟机，能够识别.class文件，能够将class文件中的字节码指令进行识别并调用操作系统向上的API完成动作。

JRE：Java运行时环境。主要包括两个部分：JVM的标准实现和java的一些基本类库。相对于jvm来说，jre多出来一部分java类库

JDK：Java开发工具包。是整个Java开发的核心，继承了jre和一些好用的小工具。

### 1.JVM的内存构成

JAVA内存构成包括：**堆、java栈**、本地方法栈、程序计数器

jdk1.8之后，方法区（元空间并不在jvm中了，而是使用本地内存）

#### 1.1 程序计数器（PC寄存器）

##### 作用

- 字节码解释器通过改变程序计数器来一次读取指令，从而实现代码的流程控制
- 在多线程情况下，程序计数器记录的是当前线程执行的位置，从而当线程切换回来时，就知道上次执行到哪了。

##### 特点

- 是一块较小的内存空间
- 线程私有，每条线程都有自己的程序计数器
- 生命周期：随着线程的创建而创建，随着线程的结束而销毁
- 是唯一一个不会出现OutOfMemroyError的内存区域

#### 1.2 Java虚拟机栈（Java栈）

##### 定义

Java虚拟机栈是描述Java方法运行过程的内存模型

Java虚拟机栈会为每一个即将运行的Java方法创建一块叫做“栈帧”的区域，用于存放该方法运行过程中的一些信息，如：

- 局部变量表
- 操作数栈
- 动态链接
- 方法出口信息
- .......

![875223b19a3ea457678d5a09acb950e0](https://github.com/wangzhiwubigdata/God-Of-BigData/raw/master/JVM/JVM%E5%86%85%E5%AD%98%E7%BB%93%E6%9E%84.resources/6F3902DB-275A-4FC6-8E3A-754DE6F987BA.jpg)



##### 压栈出栈过程

当方法运行过程中需要创建局部变量时，就将局部变量的值存入栈帧中的局部变量表中。

Java虚拟机栈的栈顶的栈帧是当前正在执行的活动栈，也就是当前正在执行的方法，PC寄存器也会指向这个地址。只有这个活动的栈帧的本地变量可以被操作数栈使用，当在这个栈帧中调用另一个方法，与之对应的栈帧又会被创建，新创建的栈帧压入栈顶，变为当前的活动栈帧。

方法结束之后，当前栈帧被移除，栈帧的返回值变成新的活动栈帧中操作数栈的一个操作数。如果没有返回值，那么新的活动栈帧中操作数栈的操作数没有变化。

> 由于Java虚拟机栈是线程对应的，数据不是线程共享的，因此不同关系数据一致性问题，也不会存在同步锁的问题。

##### Java栈的特点

- 局部变量表随着栈帧的创建而创建，它的大小在编译时确定，创建时只需分配事先规定的大小即可。在方法运行过程中，局部变量表的大小不会发生改变。
- Java栈会出现两种异常：StackOverFlowError 和 OutOfMemoryError
  - StackOverFlowError若Java虚拟机栈的大小不允许动态扩展，那么当线程请求栈的深度超过当前Java虚拟机栈的最大深度时，抛出StackOverFlowError异常
  - OutOfMemoryError若允许动态扩展，那么当线程请求栈时内存用完了，无法再动态扩展时，抛出OutOfMemoryError异常。
- Java栈也是线程私有的，随着线程的创建而创建，随着线程的结束而销毁。

> 出现StackOverFlowError时，内存空间可能还有很多。

#### 1.3 本地方法栈（C栈）

##### 本地方法栈的定义：

本地方法栈是为JVM运行Native方法准备的空间，由于很多Native方法都是用C语言实现的，所以它通常又叫做C栈。它与Java虚拟机栈实现的功能类似，只不过本地方法栈是描述本地方法运行过程的内存模型。

##### 栈帧变化过程

本地方法被执行时，在本地方法栈也会创建一块栈帧，用于存放该方法的局部变量表、操作数栈、动态链接、方法出口信息等。

方法执行结束后，相应的栈帧也会出栈，并释放本地内存空间。也会抛出`StackOverFlowError`和`OutOfMemoryError`异常。

> 如果Java虚拟机本身不支持Native方法，或者本身不依赖与传统栈，那么可以不提供本地方法栈。如果本地支持方法栈，那么这个栈一般会在线程创建的时候按线程分配。

#### 1.4 Java堆Heap

##### 定义

堆是用来存放对象的内存空间，几乎所有的对象都存储在堆中

##### 特点

- 线程共享，整个Java虚拟机只有一个堆，所有的线程都访问同一个堆。而程序计数器、Java栈、本地方法栈都是一个线程对应一个。
- 在虚拟机启动时创建。
- 是垃圾回收的主要场所
- 进一步可分为：新生代（Eden区， From Survivor、To Survivor）、老年代

不同的区域存放不同生命周期的对象，这样可以根据不同的区域使用不同的垃圾回收算法，更具有针对性。

堆的大小既可以固定也可以扩展，但对于主流的虚拟机，堆的大小是可扩展的，因为当线程请求分配内存，但堆已满，且内存已无法再扩展时，就抛出`OutOfMemoryError`

> Java堆所使用的内存不需要保证是连续的。而由于堆是被所有线程共享的，所以对它的访问需要注意同步问题，方法和对应的属性都需要保证一致性。

##### 堆的划分

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200824152328561.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3Mjc2ODM1,size_16,color_FFFFFF,t_70#pic_center)

**逻辑上：**

- 新生代（Young）
  - Eden
  - 幸存区：From 和To
- 老年代（Old）
- 元空间

**物理上：**

物理上分为 **新生代+老年代**，而元空间使用的是直接内存

##### 内存分配策略

###### 1. 对象优先分配在Eden区

大多数情况下，对象在新生代Eden区中分配。当Eden去没有足够空间进行分配时，虚拟机将发起一次Minor GC。

👇**Minior GC vs Major GC**

- Minior GC：回收新生代（包括Eden和Survivor区域），因为Java对象大多都具备朝生夕灭的特性，所以Minior GC非常频繁，一般回收速度也比较快。
- Major GC：回收老年代，Major GC的速度一般会比Minor GC慢10倍以上。

###### 2.大对象直接进入老年代

大对象是指需要大量连续内存空间的Java对象，如很长的字符串或数据。

虚拟机提供了一个`-XX:PretenureSizeThreshold`参数，令大于这个设置值的对象直接在老年代分配，这样做的目的是避免在Eden以及两个Survivor区之间发生大量的内存复制。

> 只要分配的对象的内存大小大于这个参数的时候就会直接分配到老年代

###### 3. 长期存活的对象将进入老年代（默认15）

JVM给每个对象定义了一个对象年龄计数器。当新生代发生一次Minor GC后，存活下来的对象年龄+1，当年龄超过`-XX:MaxTenuringThreshold, 默认为15`设置的值时，就将超过该值的所有对象转移到老年代中。

###### 4. 动态年龄判定

Survivor空间中相同年龄所有对象大小的总和大于Survivor空间的一半

（`-XX:TargetSurvivorRatio 默认值为50， 意味Survivor区对象使用率阈值为50%` ），年龄大于或等于该年龄的对象直接进入老年代。

##### 老年代空间分配担保

什么是空间分配担保？

> 在发生Minor GC之前，虚拟机会检查老年代最大可用的连续空间是否大于新生代所有对象的总空间：
>
> ​	如果大于，则此次Minor GC是安全的
>
> ​	如果小于, 则虚拟机会查看HandlePromotionFailure设置值是否允许担保失败。
>
> ​			如果`HandlePromotionFailure=true`，那么会继续检查老年代最大可用连续空间是否大于`历次晋升到老年代的对象的平均大小`，如果大于，则尝试进行一次Minor GC，但是这次Minor GC 依然是有风险的（<font color=green>因为Minor Gc之后，再次进入到老年代的对象的总大小有可能超过老年代最大可用连续空间</font>）
>
> ​			如果小于或者`HandlePromotionFailure=false`则改为进行一次Full GC

为什么要进行空间担保？

> 是因为新生代采用**复制收集算法**，假如大量对象在Minor GC后仍然存活（最极端情况为内存回收后新生代中所有对象均存活），而Survivor空间是比较小的，这时就需要老年代进行分配担保，把Survivor无法容纳的对象放到老年代。**老年代要进行空间分配担保，前提是老年代得有足够空间来容纳这些对象**，但一共有多少对象在内存回收后存活下来是不可预知的，**因此只好取之前每次垃圾回收后晋升到老年代的对象大小的平均值作为参考**。使用这个平均值与老年代剩余空间进行比较，来决定是否进行Full GC来让老年代腾出更多空间。

总结起来：新生代存在大量存活的对象，Survivor无法容纳这些对象，老年代要进行空间分配担保；担保前要判断自生有没有能力，如果没有能力就需要触发Full GC。

#### 1.5 方法区

##### 方法区定义

Java虚拟机规范中定义方法区是堆的一个逻辑部分。方法区存放一下信息：

- 已经被虚拟机加载的类信息
- 常量
- 静态变量
- 即时编译器编译后的代码

##### 方法区的特点

- 线程共享。方法区是堆的一个逻辑部分，因此和堆一样，都是线程共享的。整个虚拟机中只有一个方法区。
- 内存回收效率低。方法区中的信息一般需要长期存在，回收一遍之后可能只有少量信息无效。主要回收目标是：对常量池的回收；对类型的卸载

##### 运行时常量池

方法区中存放：**类信息、常量、静态变量、即时编译器编译后的代码**。常量就存放在运行时常量池中



### 2. 垃圾收集策略&算法

#### 2.1 判断对象是否存活

1. **引用计数法**

> 在对象头维护着一个counter计数器，对象被引用一次则计数器+1；若引用失效则计数器-1.当计数器为0时，就认为该对象无效了。
>
> 引用计数算法的实现简单，判定效率也很高，在大部分情况下它都是一个不错的算法。但是主流的 Java 虚拟机里没有选用引用计数算法来管理内存，主要是因为它很难解决对象之间循环引用的问题。
>
> > 举个栗子👉对象 objA 和 objB 都有字段 instance，令 objA.instance = objB 并且 objB.instance = objA，由于它们互相引用着对方，导致它们的引用计数都不为 0，于是引用计数算法无法通知 GC 收集器回收它们。

2. **可达性分析（GC Roots）**

基本思路就是通过一些列名为”GC Roots“的对象作为起始点，开始向下搜索，如果一个对象到GCRoots没有任何引用链，就说明这个对象已经没有引用了，就可以作为垃圾。

也即给定一个集合的引用作为根出发，通过引用关系遍历对象图，能被遍历到的（可达到的）对象就被判定为存活，没有被遍历到的就自然判定为死亡对象。

###### 哪些对象可以作为GC Roots

- 虚拟机栈中的引用的对象
- 方法区中的类静态属性引用的对象
- 方法区中的常量引用的对象
- synchronized同步的对象

GC Roots并不包括堆中对象引用的对象，这样就不会有循环引用的问题。



#### 2.1 垃圾收集算法

###### 标记-清除算法

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200824152623936.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3Mjc2ODM1,size_16,color_FFFFFF,t_70#pic_center)

缺点：内存会产生碎片化



###### 标记-复制

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200824152644220.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3Mjc2ODM1,size_16,color_FFFFFF,t_70#pic_center)

缺点：预留一半的内存区域；整个内存空间只有一半可以使用



###### 标记-整理

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200824152724174.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3Mjc2ODM1,size_16,color_FFFFFF,t_70#pic_center)

缺点：标记整理虽然可以解决内存碎片化问题，也不存在内存空间浪费，但是需要移动存活的对象，但是，当内存中存活对象多，并且都是一些微小对象，而且垃圾对象较少时，要移动大量的存活对象才能换取少量的内存空间。



###### 分代收集算法

一块独立的内存区域只能使用一种垃圾回收算法，根据对象生命周期特征，将其划分到不同的区域，再对特定区域使用特定的垃圾回收算法，只有这样才能将垃圾回收算法的优点发挥到极致，这种组合的垃圾回收算法称之为：分代收集算法（分代回收算法）

根据对象存活周期的不同，将内存划分为几块。一般是把 Java 堆分为新生代和老年代，针对各个年代的特点采用最适当的收集算法。

- 新生代：复制算法
- 老年代：标记-清除算法、标记-整理算法



### 3. HotSpot垃圾收集器（7种）

#### 3.1 新生代垃圾收集器

##### 1. Serial GC收集器

![1c13b8e41120caccd15369497355b588.png](https://img-blog.csdnimg.cn/img_convert/1c13b8e41120caccd15369497355b588.png)

> 单线程，只会使用一个cpu或一条线程去完成垃圾收集工作，这也意味着在进行垃圾收集时，必须暂停其他所有的工作线程，直到它收集结束为止（<font color=red>臭名昭著的Stop The World</font>）
>
> **收集算法**：复制算法
>
> 与用户线程串行执行，单线程地好处就是减少上下文切换，减少系统资源的开销。但这种方式的缺点也很明显，在GC的过程中，会暂停程序的执行。若GC不是频繁发生，这或许是一个不错的选择，否则将会影响程序的执行性能。 对于新生代来说，区域比较小，停顿时间短，所以比较使用。
>
> **参数**：`-XX:+UseSerialGC` 
>
> 在JDK Client模式，不指定JVM参数，默认是串行垃圾收集器

##### 2. ParNew收集器

ParNew收集器是Serial GC的多线程版本，除了使用多线程进行垃圾收集外，其余行为包括Serial收集器可用的所有控制参数、收集算法(复制算法)、Stop The World、对象分配规则、回收策略等与Serial收集器完全相同，两者共用了相当多的代码。

![111fbad04e83f6fc486c78406621ae05.png](https://img-blog.csdnimg.cn/img_convert/111fbad04e83f6fc486c78406621ae05.png)

ParNew收集器除了使用了多线程收集外，其他与Serial收集器相比并无太多创新之外，但是它是许多运行在Server模式下的虚拟机首选的新生代收集器，其中有一个与性能无关的重要原因是，除了Serial收集器外，目前只有它能和CMS收集器配合工作。

> **算法**：复制算法
>
> 用于新生代
>
> GC时需要暂停所有用户线程，直到GC结束
>
> **参数**：
>
> ​	`-XX:+UseConcMarkSweepGC`：指定使用CMS后，会默认使用ParNew作为新生代收集器
>
> ​	`-XX:+UseParNewGC`：强制指定使用ParNew
>
> ​	`-XX:ParallelGCThreads`：指定垃圾收集的线程数量，ParNew默认开启的收集线程与CPU的数量相同

##### 3. Parallel Scavenge收集器（吞吐量优先）

Parallel收集器同样也采用了复制算法，并行回收和STW机制；和ParNew不同之处在于，Parallel收集器的目标则是达到一个可控制的吞吐量，也被称为吞吐量优先的垃圾收集器。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20210128165345527.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3c3MzM1MTIz,size_16,color_FFFFFF,t_70)

> 参数：
>
> `-XX:MaxGCPauseMillis` 设置最大停顿时间STW，这参数设置的越小，停顿时间可能会缩短，但也会导致吞吐量下降，当值垃圾收集发生的更频繁。
>
> `-XX:GCtimeRatio` 垃圾收集时间占时间总比   用于衡量吞吐量
>
> ​	垃圾收集执行时间占应用程序执行时间的比例计算方法：`1/(n+1)`
>
> ​	例如`-XX:GCTimeRatio=19`,那么设置了垃圾收集时间占总时间的5% = 1/(19+1);
>
> ​	默认值是99，即1%；
>
> `-XX:UseAdaptiveSizePolicy` 设置Parallel收集器具有自适应调节功能；开启这个参数后，就不用手工指定一些细节参数了，如`新生代大小 -Xmn`、`Eden与Survivor区的比例 -XX:SurvivorRation`、`晋升老年代的对象年龄 -XX:MaxTenuringThreshold`

JVM会根据当前系统运行情况收集性能监控信息，动态调整这些参数，以提供最合适的停顿时间或最大的吞吐量，这种调节方式称为GC自适应的调节策略(GC Ergonomiscs)；

另外值得注意的一点是，Parallel Scavenge收集器无法与CMS收集器配合使用，所以在JDK 1.6推出Parallel Old之前，如果新生代选择Parallel Scavenge收集器，老年代只有Serial Old收集器能与之配合使用。



#### 3.2 老年代垃圾收集器

##### 1. Serial Old收集器

![abdedea73525f71775a338dbceeedd2a.png](https://img-blog.csdnimg.cn/img_convert/abdedea73525f71775a338dbceeedd2a.png)

> **算法**：标记-整理
>
> 可作为CMS收集器的后备预案，并在CMS发生”Concurrent Mode Failure“ 时使用。



##### 2. Parallel Old

![e1b908c08120b3323a3d5ca408bc569b.png](https://img-blog.csdnimg.cn/img_convert/e1b908c08120b3323a3d5ca408bc569b.png)

Parallel Scavenge收集器的老年代版本，并行收集器，吞吐量优先

> **算法**：标记-整理
>
> **参数**：`-XX:UseparallelOldGC` 指定使用Parallel Old收集器



##### 3. CMS并发清除（Concurrent Mark Sweep）

这个收集器有与工作线程执行**并发**的能力。

> **算法**：标记-清除
>
> **特点**：收集过程中不需要暂停用户线程，以获取最短回收停顿时间为目标

![ae5a11b458e117b8f30fdc064821647c.png](https://img-blog.csdnimg.cn/img_convert/ae5a11b458e117b8f30fdc064821647c.png)

CMS GC过程分四步：

1. **初始标记**（initial mark）

   > 单线程执行， 需要STW，但仅仅把GC Roots的直接关联可达的对象给标记一下，由于直接关联对象比较小，所以这里的速度非常快。

2. **并发标记**（Concurrent mark）

   > 对于初识标记过程所标记的初识标记对象，进行并发跟踪标记
   >
   > 此时其他线程仍可以继续工作。此处时间较长，但不停顿，并不能保证可以标记出所有的存活对象；

3. **重新标记**（remark）

   > 在并发标记的过程中，由于可能还会产生新的垃圾，所以此时需要重新标记新产生的垃圾。
   >
   > 此处执行**并行标记**，与用户线程不并发，所以依然是STW
   >
   > 且停顿时间比初识标记稍长，但远比并发标记短。

4. **并发清除**（Concurrent sweep）

   >  并发清除之前所有标记的垃圾；
   >
   > 其他用户线程仍可以工作，不需要停顿。

Tips：初始标记和并发标记仍然需要STW

**初始标记**仅仅标记一下GC Roots能直接关联到的对象，速度很快；

**并发标记**就是进行GC Roots Tracing的过程；

而**重新标记**阶段则是为了修正并发标记期间因用户程序继续运行而导致标记产生变动的那一部分对象的标记记录，这个阶段的停顿时间一般会比初始标记阶段长，但远比并发标记的时间短。

由于整个过程中耗时最长的并发标记和并发清除过程，收集器线程都可以与用户线程一起工作，所以整体上说，CMS收集器的内存回收过程是与用户线程一共并发执行的。

> **参数**：
>
> `-XX:+UseConcMarkSweepGC`：使用CMS收集器
>
> `-XX:+UseCMSCompactAtFullCollection`：Full GC后，进行一次碎片整理；整理过程是独占的，会引起停顿时间变长。
>
> `-XX:+CMSFullGCsBeforeCompaction`：设置进行几次Full GC后，进行一次碎片整理
>
> `-XX:ParallelCMSThreads`：设置CMS的线程数量（一般情况约等于可用CPU数量）

**优点**：

总体来说，与Parallel Old垃圾收集器相比，CMS减少了执行老年代垃圾收集时应用暂停的时间；但却增加了新生代垃圾收集时应用暂停的时间，降低了吞吐量而且需要占用更大的堆空间；

由于耗时的**并发标记**和**并发清除**阶段都不需要暂停工作，所以整体的回收是低停顿的。

由于CMS以上特性，缺点也是比较明显的。

**缺点**：

1. 对CPU资源非常敏感

2. 浮动垃圾

   由于CMS并发清理阶段用户线程还在运行着，伴随程序运行自然就还会有新的垃圾不断产生，这一部分垃圾出现在标记过程之后，CMS无法在当次收集中处理掉它们，只好留待下一次GC时再清理掉。这一部分垃圾就称为“浮动垃圾”。

   由于在垃圾收集阶段用户线程还需要运行，那就还需要预留有足够的内存空间给用户线程使用，因此CMS收集器不能像其他收集器那样等到老年代几乎完全被填满了再进行收集，也可以认为CMS所需要的空间比其他垃圾收集器大；

   `-XX:CMSInitiatingOccupancyFraction`: 设置CMS预留内存空间

3. ”Concurrent Mode Failure“失败

   如果如果CMS运行期间预留的内存无法满足程序需要，就会出现一次“Concurrent Mode Failure”失败，这时虚拟机将启动后备预案：临时启用Serial Old收集器来重新进行老年代的垃圾收集，这样会导致另一次Full GC的产生。这样停顿时间就更长了，代价会更大，所以 `-XX:CMSInitiatingOccupancyFraction`不能设置得太大。

4. 产生大量内存碎片

   这个问题并不是CMS的问题，而是算法的问题。由于CMS基于"标记-清除"算法，清除后不进行压缩操作，所以会产生碎片

   "标记-清除"算法介绍时曾说过：

   产生大量不连续的内存碎片会导致分配大内存对象时，无法找到足够的连续内存，从而需要提前触发另一次Full GC动作。

   **碎片解决方法：**

   -  `-XX:+UseCMSCompactAtFullCollection`

     - 使得CMS出现上面这种情况时不进行Full GC，而开启内存碎片的合并整理过程；但合并整理过程无法并发，停顿时间会变长；

   - `-XX:+CMSFullGCsBeforeCompation`

     - 设置执行多少次不压缩的Full GC后，来一次压缩整理

     - 为减少合并整理过程的停顿时间；

       默认为0，也就是说每次都执行Full GC，不会进行压缩整理；

       由于空间不再连续，CMS需要使用可用"空闲列表"内存分配方式，这比简单使用"碰撞指针"分配内存消耗大；

       

#### 3.3 G1收集器

G1(Garbage - First)名称的由来是G1跟踪各个Region里面的垃圾堆的价值大小(回收所获得的空间大小以及回收所需时间的经验值)，在后台维护一个优先列表，每次根据允许的收集时间，优先回收价值最大的Region。

注意：G1与前面的垃圾收集器有很大不同，它把新生代、老年代的划分取消了！

这样我们再也不用单独的空间对每个代进行设置了，不用担心每个代内存是否足够。

取而代之的是，G1算法将堆划分为若干个区域(Region)，它仍然属于分代收集器。不过，这些区域的一部分包含新生代，新生代的垃圾收集依然采用暂停所有应用线程的方式，将存活对象拷贝到老年代或者Survivor空间。老年代也分成很多区域，G1收集器通过将对象从一个区域复制到另外一个区域，完成了清理工作。这就意味着，在正常的处理过程中，G1完成了堆的压缩(至少是部分堆的压缩)，这样也就不会有CMS内存碎片问题的存在了。
![d278aed530716ea4981a02fb8590c946.png](https://img-blog.csdnimg.cn/img_convert/d278aed530716ea4981a02fb8590c946.png)



G1收集器运作过程

![b03dfc84f0de89fe474936cbd70aef32.png](https://img-blog.csdnimg.cn/img_convert/b03dfc84f0de89fe474936cbd70aef32.png)

1. 初识标记

   - 初始标记仅仅只是标记一下GC Roots能直接关联到的对象，速度很快

2. 并发标记

   - 进行GC Roots Tracing的过程，从刚才产生的集合中标记出存活对象；(也就是从GC Roots 开始对堆进行可达性分析，找出存活对象。)

     耗时较长，但应用程序也在运行；

     并不能保证可以标记出所有的存活对象；

3. 最终标记

   - 最终标记和CMS的重新标记阶段一样，也是为了修正并发标记期间因用户程序继续运作而导致标记产生变动的那一部分对象的标记记录，

     这个阶段的停顿时间一般会比初始标记阶段稍长一些，但远比并发标记的时间短，

     也需要“Stop The World”。(修正Remebered Set)

4. 筛选回收

   - 首先排序各个Region的回收价值和成本
   - 然后根据用户期望的GC停顿时间来制定回收计划；
   - 最后按计划回收一些价值高的Region中垃圾对象

   回收时采用”复制算法“，从一个或多个Region复制存活对象到堆上的另一个空间Region，并且再次过程中压缩和释放内存；

   可以并发进行，降低停顿时间，并增加吞吐量

   > **参数**：
   >
   > `-XX:+UseG1GC`：指定使用G1收集器
   >
   > `-XX:InitiatingHeapOccupancyPercent`: 当整个Java堆的占用率达到参数值时，开始并发标记阶段；默认为45；
   >
   > `-XX:MaxGCPauseMillis`： 为G1设置暂停时间目标，默认值为200毫秒；
   >
   > `-XX:G1HeapRegionSize`：设置每个Region大小，范围1MB到32MB；目标是在最小Java堆时可以拥有约2048个

#### 小结：

![8d9b5b42d191a4ad452074f204507378.png](https://img-blog.csdnimg.cn/img_convert/8d9b5b42d191a4ad452074f204507378.png)







### 2.类加载过程

类从被加载到虚拟机内存中开始，到卸载出内存位置，他的整个生命周期如下如：

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200810150148636.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2E2NDY3MDU4MTY=,size_16,color_FFFFFF,t_70)

1. **加载**
   - 通过一个类的全限定名来获取定义此类的二进制字节流
   - 将这个字节流所代表的静态存储结构转化为方法区的运行时数据结构
   - 在Java堆中生成一个代表这个类的java.lang.Class对象，作为方法区数据的访问入口
2. **验证**
   - 验证阶段作用是保证Class文件的字节流包含的信息是否符合JVM规范，不会给JVM造成危害。如果验证失败，就会抛出一个java.lang.VerifyError异常或子类异常。验证过程分为四个阶段：
     - 文件格式验证：验证字节流文件是否符合Class文件格式的规范，并且能被当前虚拟机正确的处理
     - 元数据验证：是对字节码描述的信息进行语义分析，以保证其描述信息符合Java语言的规范
     - 字节码验证：主要是进行数据流和控制流的分析，保证被校验类的方法在运行时不会危害JVM
     - 符号引用验证：符号引用验证发生在虚拟机将符号引用转化为直接引用的时候，这个转化动作将在解析阶段中发生。

<font color=green>符号引用（Symbolic References）说明</font>

> 符号引用以一组符号来描述所引用的目标，符号引用与虚拟机的内存布局无关，引用的目标不一定加载到内存中。
>
> <font color=blue>在java中，一个java类将会编译成一个class文件。在编译时，java类并不知道所引用的类的实际地址，因此只能使用符号引用来代替</font>
>
> 比如org.simple.People类引用了org.simple.Language类，在编译时People类并不知道Language类的实际内存地址，因此只能使用符号org.simple.Language（假设是这个，当然实际中是由类似于CONSTANT_Class_info的常量来表示的）来表示Language类的地址。各种虚拟机实现的内存布局可能有所不同，但是它们能接受的符号引用都是一致的，因为符号引用的字面量形式明确定义在Java虚拟机规范的Class文件格式中。

3. **准备**

   - 准备阶段为变量分配内存并设置类变量的初始化。在这个阶段分配的仅为类的变量（static修饰的变量），而不包括类的实例变量（实例变量在new 的时候初始化）。对已非final的变量，JVM会将其设置成”零值“，而不是其赋值语句的值：

     ```java
     private static int size = 12;
     ```

     那么这个阶段，size的值为0，而不是12。final修饰的类变量将会赋值成真实的值。

4. **解析**

   - 解析过程是将常量池内的符号引用替换成直接引用。主要包括四种类型引用的解析。**类或接口的解析**、**字段解析**、**方法解析**、**接口方法解析**。

5. **初始化**

   - 在准备阶段，类变量已经经过一次初始化了，在这个阶段，则是根据程序员通过程序制定的计划去初始化类的变量和其他资源。这些资源有static{}块，构造函数，父类的初始化等。

6. **使用**

7. **卸载**



### 3. 双亲委派机制



**java中的四种类加载器**

1. 启动（Bootstrap）类加载器

   > 启动类加载器是本地代码实现的类加载器，它负责将<JavaRuntimeHome>/lib下面的类库加载到内存中。由于启动类加载器涉及到虚拟机本地实现细节，开发者无法直接取到启动类加载器的引用。

2. 标准扩展（Extension）类加载器

   > 扩展类加载器负责将<JavaRuntimeHome>/lib/ext或者系统变量java.ext.dir指定位置中的类库加载到内存中。开发者可以直接使用标准扩展类加载器

3. 应用程序（Application）类加载器

   > 应用程序类加载器负责加载用户路径（classpath）上的类库





![在这里插入图片描述](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWcyMDIwLmNuYmxvZ3MuY29tL2Jsb2cvMjAwNDkzNC8yMDIwMDcvMjAwNDkzNC0yMDIwMDcyOTEyNDgyNjk0MC0xMDQyODAzODI0LnBuZw?x-oss-process=image/format,png#pic_center)

**双亲委派机制**

> 当一个类收到类加载请求时，它首先不会尝试自己去加载这个类，而是把这个请求委派给父类加载器去完成，每一层的类加载器都是如此，因此所有的加载请求都应该传送到启动类加载器中，只有当父类加载器反馈自己无法完成这个请求的时候（在它的加载路径下没有找到所需加载的Class）（从最顶层的BootStrap -》Extension-》Application-》自己定义的类加载器），子类加载器才会尝试自己加载。

**双亲委派机制的作用**

> 为了保证自己写的代码不污染java出厂自带的源代码。如果有人想替换系统级别的类：如String.java.篡改它的实现，但是在这种机制下这些系统的类已经被Bootstrap ClassLoader加载过了，所以并不会再去加载，从一定程度上防止了危险代码的植入。
>
> 1. 防止重复加载用一个.class。通过委托去上层，加载过了，就不用再加载一遍。保证数据安全
> 2. 保证了使用不同的类加载器最终得到的都是同一个Object对象。



### 3.创建（new）对象的过程

1. **检查类是否已经被加载**
   - 当JVM遇到一条字节码new指令时，首先检查该引用指向的类是否能够在常量池中被找到（也就是检查方法区中有没有该类的信息），如果没有，先加载这个类；有的话就执行下一步，为对象分配内存
2. **为对象分配内存空间**
   - 类加载检查通过后，接下来虚拟机会为对象分配内存。对象需要多大的内存在类加载完成后便可完全确定，为对象分配内存就是把一块确定大小的内存块从堆上划分出来。
3. **为对象字段设置零值**
   - 分配完内存后，需要对对象的字段进行零值初始化，（也就是对象的实例数据部分，对象的内存布局被分为三个部分：**对象头**、**实例数据**、**对齐填充**），对象头除外，零值初始化意思就是对对象的字段赋0值，或者null值。
4. **设置对象头**
   - 虚拟机需要对这个将要创建出来的对象，进行信息标记，包括是否为新生代/老年代，对象的hash码，元数据信息，这些标记存放在对象头信息中。
5. **执行构造方法**
   - 执行对象的构造方法，初始化对象，这样一个对象才算被成功创建。

### 4.对象的内存布局

> 提问：`Object o = new Object();` 请问一个object对象占多少内存空间？

Java对象的内存布局：对象头（Header）、实例数据（Instance Data）和对齐填充（Padding）。8字节对齐。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20201221191858529.png)

- Mark Word
  - 存储对象的hashCode、GC分代年龄、锁状态标志、线程只有的锁、偏向线程ID、偏向时间戳
- Class Pointer
  - 指向对象对应的Class对象（类对象）的内存地址
- Instance Date
  - 具体的数据大小，如对象含有一个int成员变量，即为4字节

在64bit的JVM中，MarkWord为64bit-8字节，这样一个`Object`对象占有**16个字节**



### 5. 四大引用类型



#### 1. 强引用

`StrongReference`是java的默认引用形式，使用时不需要显示定义。任何通过强引用所使用的对象，不管jvm内存是否充足，Java GC都不会主动回收具有强引用的对象。

> 如果一个对象具有强引用，那么垃圾收集器不会回收它
>
> 当JVM内存空间不足时，Java虚拟机宁愿抛出OurOfMemoryError错误，使程序异常终止，也不会回收强引用对象。

#### 2. 软引用

`SoftReference<String[]> softArr = new SoftReference<String[]>(new String[] {"a", "b", "c"});`

软引用在内存充足时，GC不会回收；如果内存不足时，GC会回收这个对象。

**应用场景**

> 实现内存敏感的高速缓存，比如网页缓存，图片缓存等。使用软引用能防止内存泄漏



#### 3. 弱引用

`WeakReference<String[]> weakArr=new WeakReference<String[]>(new String[]{"a","b","c"});`

如果一个对象只具有弱引用，无论内存充足与否，Java GC后对象都会被回收。

**应用场景**

> ThreadLocal

##### ThreadLocal 弱引用造成的数据泄漏问题

`ThreadLocalMap`内部Entry类

```java
static class Entry extends WeakReference<ThreadLocal<?>> {
  /** The value associated with this ThreadLocal. */
  Object value;

  Entry(ThreadLocal<?> k, Object v) {
    super(k);
    value = v;
  }
}
```

可以看出，ThreadLocal内部每个线程维护的本地变量map中的Entry的key是弱引用类型`WeakReference`, 不管JVM内存空间是否充足，在GC的时候，都会回收里面的key。

<font color=red>但是value依然是强引用类型</font>，这就会造成这种情况：GC回收的时候，把key进行了回收，变为了`null`,但是其对应的value还有值存在，但是无法被引用到了，这就造成了`内存泄漏`,因此，在实际使用ThreadLocal的过程中，使用完毕后需要及时调用`remove()`方法，避免造成数据泄漏。



#### 4. 虚引用

虚引用需要配合引用队列`ReferenceQueue`联合使用。当执行Java GC时如果一个对象只有虚引用，就会把这个对象加入到与之关联的`ReferenceQueue`中。

```java
//虚引用PhantomReference
    //必须和引用队列联合使用
    ReferenceQueue<String[]> rqueue = new ReferenceQueue<>();
    PhantomReference<String[]> phanArr = new PhantomReference<String[]>(new String[]{"a","b"},rqueue);
    /**
     *
     * 应用场景：
     *大多被用于引用销毁前的处理工作
     *
     */
```

当垃圾回收期准备回收一个对象时，如果发现它还有虚引用，就会在回收对象的内存之前，把这个虚引用加入到与之关联的引用队列中。

程序可以通过判断引用队列中是否已经加入了虚引用，来了解引用的对象是否将要被垃圾回收。程序如果发现某个虚引用已经被加入到引用队列中，那么就可以在所引用的对象的内存被回收之前采取必要的行动。



### 6. JVM常用调优参数

[Oracle关于JVM参数配置参考表](https://www.oracle.com/java/technologies/javase/vmoptions-jsp.html)

| 配置参数                             | 功能                                                         | 备注 |
| ------------------------------------ | ------------------------------------------------------------ | ---- |
| `-Xms`                               | 初识堆大小。如`-Xms256m`                                     |      |
| `-Xmx`                               | 最大堆大小。如`-Xmx1024m`                                    |      |
| `-Xmn`                               | 新生代大小。通常为`Xmx`的1/3或1/4.<br />新生代=Eden+2个Survivor空间。<br />实际可用空间为=Eden+1个Survivor，即90% |      |
| `-Xss`                               | 每个线程堆栈大小，默认为1M                                   |      |
| `-XX:NewRatio`                       | 老年代/新生代的比例，默认`-XX:NewRatio=2`,代表老年代：新生代=2：1 |      |
| `-XX:SurvivorRatio`                  | 新生代中Eden与Survivor的比值。默认值为8  `-XX:SurvivorRatio=8` |      |
| `java -XX:+PrintFlagsFinal -version` | 查看jvm所有参数选项的值                                      |      |
| `-XX:MaxTenuringThreshold`           | 新生代晋升老年的的年龄<br />默认值`-XX:MaxTenuringThreshold=15` |      |
| `-XX:MetaspaceSize`                  | 元空间大小                                                   |      |
| `-XX:MaxMetaspaceSize`               | 元空间最大空间大小                                           |      |
| `-XX:PretenureSizeThreshold`         | 大对象所占空间超过这个阈值，直接分配到老年代                 |      |
| `-XX:+PrintGCDetails`                | 打印GC信息                                                   |      |
| `关于设置垃圾收集器`                 |                                                              |      |
| `-XX:+UseSerialGC`                   | 新生代使用Serial GC， 老年代使用Serial old                   |      |
| `-XX:+UseParNewGC`                   | 新生代使用ParNew收集器，老年代使用Serial Old                 |      |
| `-XX:+UseConcMarkSweepGC`            | 新生代使用ParNew收集器，老年代使用CMS                        |      |
| `-XX:ParallelGCThreads=8`            | 这个参数指定并行GC线程的数量，<br />一般最好和cpu核心数相当。 |      |
| `-XX:+UseParallelOldGC`              | 新生代使用ParallelGC收集器，<br />老年代使用ParallelOldGC收集器 |      |
| `-XX:ConcGCThreads`                  | 设置CMS并发线程数                                            |      |
| `-XX:+UseG1GC`                       | 开启G1收集器                                                 |      |
| `关于锁`                             |                                                              |      |
| `-XX:+UseSpinning`                   | 启用自旋锁优化，jdk1.6之后默认开启                           |      |
| `-XX:PreBlockSpin`                   | 设置自旋多少次后升级为重量级锁；默认`-XX:PreBlockSpin=10`    |      |
|                                      |                                                              |      |





## 并发与多线程篇

### 1. 进程与线程

进程是程序的一次执行过程，是系统运行程序的基本单位，因此进程是动态的。系统运行一个程序即是一个进程从创建，运行到消亡的过程。同时，每个进程还占有某些系统资源如CPU时间，内存空间，文件，输入输出设备的使用权等等。

**进程与进程之间的通信方式**

1. 管道pipe
   - 通常指无名管道，unix系统IPC最古老的形式
   - 只能用于具有亲缘关系的进程之间的通信（父子进程，兄弟进程）
2. 命名管道FIFO
   - 在磁盘上有对应的节点，但是没有数据块。一旦建立，任何进程都可以通过文件名将其打开和进行读写，而不局限于父子进程，当然前提是进程对FIFO有适当的访问权。当不再被进程使用时，FIFO在内存中释放，但磁盘节点仍然存在。
3. 消息队列MessageQueue
   - 消息队列，就是一个消息的链表，是一系列保存在内核中消息的列表。用户进程可以向消息队列添加消息，也可以从消息队列读取消息。
   - 消息队列与管道通信相比，其优势是对每个消息指定特定的消息类型，接收的时候不需要按照队列次序，而是可以根据自定义条件接收特定类型的消息。
   - 进程间通过消息队列通信，主要是：创建或打开消息队列，添加消息，读取消息和控制消息队列
4. 共享存储SharedMemory
   - 共享内存允许两个或多个进程共享一个给定的存储区，这一段存储区可以被两个或两个以上的进程映射至自身的地址空间中，一个进程写入共享内存的信息，可以被其他使用这个共享内存的进程，通过一个简单的内存读取策略读出，从而实现了进程间的通信。
   - 采用共享内存进行通信的一个主要好处是**效率高**，因为进程可以直接读写内存，而不需要任何数据的拷贝，对于像管道和消息队列等通信方式，则需要在内核和用户空间进行四次的数据拷贝，而共享内存则只拷贝两次：` 一次从输入文件到共享内存`和 `一次从共享内存输出文件`
5. 信号量Semaphore
   - 信号量用于实现进程间的互斥与同步，而不是用于存储进程间通信数据。
6. 套接字Socket
   - 适合同一主机的不同进程间和不同主机的进程间进行全双工网络通信
7. 信号（sinal）



#### 线程的几种状态

`Thread.State`枚举类查看线程的各种状态

1. NEW（新建）
2. RUNNABLE（就绪）
3. BLOCKED（阻塞）
4. WAITING（等待）
5. TIMED_WAITING（超时等待）
6. TERMINATED（终止）



**wait/sleep都会导致线程的阻塞，有什么区别？**

- wait放开手去睡，放开手里的锁
- sleep握紧手去睡，醒了手里还有锁



#### Java中实现多线的方式

1. 继承Thread类，实现run方法
2. 实现Runnable接口，实现run方法
3. 实现Callable接口，实现call方法。注意：新建Thread的时候，Thread的构造方法中没有接收Callable的。（中间商赚差价！！！）所有我们需要找到一个既可以联系Runnable接口又联系Callable接口的类（FutureTask））
   - `FutureTask`中的`get()`方法会阻塞线程，一直等待线程计算完成后才执行下面的后续代码。一般放在最后。
   - 同一个futureTask对象只能被线程调用一次，当有新的线程调用了已经被调用过的futuretask对象时，这次只会复用上一次的结果，不会再执行一次。
4. 线程池`ExecutorService`(ThreadPoolExecutor类)





### 2. JUC

#### JUC强大的辅助类：

##### 1. CountDownLatch类

计数器不为0，`countDownLatch.await(); `方法后面的代码都被一直阻塞

每调用一个线程，就需要执行`countDownLatch.countDown()`,将其计数器减一

##### 2. CyclicBarrier

一句话：集齐七颗龙，召唤神龙。

没调用一个线程，就需要执行`cyclicBarrier.await()`,将计数器加1

**CyclicBarrier类与CountDownLatch类的区别**

| CountDownLatch                                               | CyclicBarrier                                                |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| 减计数方式                                                   | 加计数方式                                                   |
| 计算为0时释放所有等待的线程                                  | 计数达到指定值时释放所有等待线程                             |
| 计数为0时，无法重置                                          | 计数达到指定值时，计数置为0重新开始                          |
| 调用countDown()方法计数减一，调用await()方法只进行阻塞，对计数没任何影响 | 调用await()方法计数加1，若加1后的值不等于构造方法的值，则线程阻塞 |
| 不可重复利用                                                 | 可重复利用                                                   |

##### 信号量：Semaphore类（类似于PV操作）

两个关键性操作：

1. `semaphore.acquire()` 请求资源
2. `semaphore.release() ` 释放资源

> acquire：当一个线程调用acquire操作时，它要么成功，获取信号量（信号量-1）；要么一直等待下去，直到有线程释放了信号量，或者超时
>
> release：实际上会将信号量的值加1，然后唤醒等待的线程。

信号量的主要作用：

> 用于**对多个共享资源的互斥使用**
>
> 用于**并发线程数的控制**



### 阻塞队列（BlockingQueue）

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200823081540511.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3Mjc2ODM1,size_16,color_FFFFFF,t_70#pic_center)

线程1往阻塞队列中生产元素，线程2 从阻塞队列中消费元素。

阻塞队列满了，生产线程阻塞；

阻塞队列空了，消费线程阻塞。

***阻塞队列的用处***

> 在多线程领域：所谓阻塞，在某些情况下会 挂起 线程（即阻塞），一旦满足条件，被挂起的线程又会被自动唤醒。
> 为什么需要BlockingQueue，好处是我们不需要关心什么时候 需要阻塞线程，什么时候需要唤醒线程因为这一切BlockingQueue都给你一手包办了



| 实现类                                     | 说明                                                         |
| ------------------------------------------ | ------------------------------------------------------------ |
| <font color=red>ArrayBlockingQueue</font>  | 由数组结构组成的有界阻塞队列                                 |
| <font color=red>LinkedBlockingQueue</font> | 由链表结构组成的有界（但大小默认值为integer.MAX_VALUE）阻塞队列 |
| <font color=red>SynchrousQueue</font>      | 不存储元素的阻塞队列，也即单个元素的阻塞队列                 |
| PriorityBlockingQueue                      | 支持优先级排序的无界阻塞队列                                 |
| DelayQueue                                 | 使用优先级队列实现的延迟无界阻塞队列                         |
| LinkedTransferQueue                        | 由链表组成的无界阻塞队列                                     |
| LinkedBlockingDeque                        | 由链表组成的双向阻塞队列                                     |





### 3. 线程池

```java
public ThreadPoolExecutor(
  												// 常驻核心线程数
  												int corePoolSize,
  												// 最大线程数
                          int maximumPoolSize,
  												// 空闲线程的存活时间，
                          long keepAliveTime,
  												// 时间单位
                          TimeUnit unit,
  												// 阻塞队列：用于存放被提交但尚未被执行的任务，类似于银行的候客区：窗口已经满了，需要排队等待
                          BlockingQueue<Runnable> workQueue,
  												// 线程池中工作线程的线程工厂，用于创建线程。
                          ThreadFactory threadFactory,
  												// 拒绝策略（阻塞队列满了，无法再容纳更多的线程任务）
                          RejectedExecutionHandler handler) {
  if (corePoolSize < 0 ||
      maximumPoolSize <= 0 ||
      maximumPoolSize < corePoolSize ||
      keepAliveTime < 0)
    throw new IllegalArgumentException();
  if (workQueue == null || threadFactory == null || handler == null)
    throw new NullPointerException();
  this.acc = System.getSecurityManager() == null ?
    null :
  AccessController.getContext();
  this.corePoolSize = corePoolSize;
  this.maximumPoolSize = maximumPoolSize;
  this.workQueue = workQueue;
  this.keepAliveTime = unit.toNanos(keepAliveTime);
  this.threadFactory = threadFactory;
  this.handler = handler;
}
```



#### 线程池拒绝策略

| 拒绝策略                                 | 说明                                                         | 备注 |
| ---------------------------------------- | ------------------------------------------------------------ | ---- |
| `ThreadPoolExecutor.AbortPolicy(默认)`   | 直接抛出RejectedExecutionException异常阻止系统正常运行       |      |
| `ThreadPoolExecutor.CallerRunsPolicy`    | **调用者运行**机制：该策略既不会抛弃任务，也不会抛出异常，而是将某些任务回退到调用者，从而降低新任务的流量（谁让你找我的，你回去找谁去） |      |
| `ThreadPoolExecutor.DiscardOldestPolicy` | 抛弃队列中等待最久的任务，然后把当前任务加入队列中尝试再次提交当前任务。 |      |
| `ThreadPoolExecutor.DiscardPolicy`       | 该策略默默地丢弃无法处理的任务，不予任何处理也不抛出异常。如果任务允许丢失，这是最好的一种策略。 |      |





### 4. volatile&JMM内存模型



#### JMM内存模型

<img src="https://img-blog.csdnimg.cn/20200824153251847.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3Mjc2ODM1,size_16,color_FFFFFF,t_70#pic_center#" alt="在这里插入图片描述" style="zoom:50%;" />

> JMM是java的内存模型，JMM定义了程序中各个共享变量的访问规则，即在虚拟机中奖变量存储到内存和从内存读取变量这样的底层细节。
>
> 设计JMM主要的目的是：屏蔽各种硬件和操作系统的内存访问差异，以实现让Java程序在各种平台下都能达到一致的内存访问效果。
>
> 由于JVM运行程序的实体是线程，而每个线程创建时JVM都会为其创建一个工作内存（有些地方成为栈空间），工作内存是每个线程私有的数据区域，而Java内存模型中规定所有变量都存储在主存中，主存是共享内存区域，所有线程都可以访问。
>
> 但是线程对变量的操作（读取赋值等）必须在工作内存中进行，首先要将变量先从主存拷贝到线程自己的工作内存空间，然后对变量进行操作，操作完成后再将变量写回主存。
>
> 不能直接操作主存中的变量，各个线程中的工作内存中存储着主存中的变量副本，因此不同线程间无法访问对方的工作内存，线程间的通信（传值）必须通过主存来完成。



#### volatile

##### volatile特性：

1. **可见性**

   - 保证了不同线程对这个变量进行操作时的可见性，即一个线程修改了某个变量的值，这新值对其他线程来说是立即可见的

   - > **volatile底层实现可见性的原理**
     >
     > 以两核CPU为例（双核）
     >
     > 由于cpu的速度要比内存快的多，为了弥补这个性能差异，cpu内核都会有自己的高速缓冲区，当内核运行线程执行一段代码时，首先将这段代码的指令集进行缓存行填充到高速缓存，如果非volatile变量，当CPU执行修改了此变量之后，会将修改后的值回写到高速缓存，然后再刷新到内存中。如果刷新回内存之前，由于是共享变量，那么core2中的线程执行的代码也用到了这个变量，这时变量的值依然是旧的。
     >
     > volatile关键字就会解决这个问题
     >
     > > 首先被volatile关键字修饰的共享变量在转换成汇编语言时，会加上一个lock为前缀的指令，当cpu发现这个指令时，立即做两件事：
     > >
     > > 1. <font color=green>将当前内核高速缓存行的数据立刻回写到内存；</font>
     > > 2. <font color=green>使其他内核里缓存了该内存地址的高速缓存中的数据无效。重写从主存中读取该数据</font>

2. **禁止指令重排**

   - volatile内存区的读写，通过加屏障来禁止指令重排列
   - LoadLoad屏障：对于这样的语句`Load1; LoadLoad; Load2`, 在`Load2`以及后续读取操作要读取的数据被访问前，要保证`Load1`要读取的数据被读取完毕。
   - StoreStore屏障：对于这样的语句`Store1;StoreStore;Store2`, 在`Store2`以及后续写入操作执行前，保证`Store1`的写入操作对其它处理器可见。
   - LoadStore屏障：对于这样的语句`Load1;LoadStore;Store2`,在`Store2`以及后续写入操作被刷出前，保证`Load1`要读取的数据被读取完毕
   - StoreLoad屏障：对于这样的语句`Store1;StoreLoad;Load2`,在`Load2`以及后续所有读取操作执行前，保证`Store1`的写入对所有处理器可见。

3. **不保证原子性**



### 5. synchronized原理

[深入分析Synchronized原理](https://www.cnblogs.com/aspirant/p/11470858.html)

```java
public class SynchronizedDemo {

    public synchronized void method(){
        synchronized (this){
            System.out.println(
                    "Synchronized Demo"
            );
        }
    }

    public static void main(String[] args) {
    }
}
```

上述代码通过 `javap -c -l -p .class`反编译成字节码结果如下：

``` java
Compiled from "SynchronizedDemo.java"
public class SynchronizedDemo {
  public SynchronizedDemo();
    descriptor: ()V
    Code:
       0: aload_0
       1: invokespecial #1                  // Method java/lang/Object."<init>":()V
       4: return
    LineNumberTable:
      line 1: 0
    LocalVariableTable:
      Start  Length  Slot  Name   Signature
          0       5     0  this   LSynchronizedDemo;

  public synchronized void method();
    descriptor: ()V
    Code:
       0: aload_0
       1: dup
       2: astore_1
       3: monitorenter			// 一次 monitorenter
       4: getstatic     #2                  // Field java/lang/System.out:Ljava/io/PrintStream;
       7: ldc           #3                  // String Synchronized Demo
       9: invokevirtual #4                  // Method java/io/PrintStream.println:(Ljava/lang/String;)V
      12: aload_1
      13: monitorexit			// 两次monitorexit
      14: goto          22
      17: astore_2
      18: aload_1	
      19: monitorexit			// 两次monitorexit
      20: aload_2
      21: athrow
      22: return
    Exception table:
       from    to  target type
           4    14    17   any
          17    20    17   any
    LineNumberTable:
      line 4: 0
      line 5: 4
      line 8: 12
      line 9: 22
    LocalVariableTable:
      Start  Length  Slot  Name   Signature
          0      23     0  this   LSynchronizedDemo;

  public static void main(java.lang.String[]);
    descriptor: ([Ljava/lang/String;)V
    Code:
       0: return
    LineNumberTable:
      line 12: 0
    LocalVariableTable:
      Start  Length  Slot  Name   Signature
          0       1     0  args   [Ljava/lang/String;
}

```

1. `monitorenter`：每个对象都是一个监视器锁（**monitor**）。当**monitor**被占用时就会处于锁定状态，线程执行`monitorenter`指令时，尝试获取**monitor**的所有权，过程如下：

> 1. 如果**monitor**的进入数为0，则该线程进入**monitor**，然后将进入数设置为1，该线程即为**monitor**的所有者；
> 2. 如果线程已经占有该**monitor**，只是重新进入，则进入**monitor**的进入数加1；
> 3. 如果其他线程已经占用了**monitor**，则该线程进入阻塞状态，知道**monitor**的进入数为0，再重新尝试获取**monitor**的所有权

2. `monitorexit`:执行`monitorexit`的线程必须是`objectref`所对应的`monitor`的所有者。指令执行时，`monitor`的进入数减1，如果减1后进入数为0，那线程退出`monitor`，不再是这个`monitor`的所有者。其他被这个`monitor`阻塞的线程可以尝试去获取这个`monitor`的所有权

> monitorexit指令出现了两次，第1次为同步正常退出释放锁；第2次为发生异常退出释放锁；

**Synchronized**的语义底层是通过一个monitor的对象来完成的。





### 6.锁&锁升级

[浅谈偏向锁、轻量级锁、重量级锁](https://www.jianshu.com/p/36eedeb3f912)

Synchronized加锁时，进程会从**用户态**转换成**内核态**，让操作系统帮忙调度。用户态与内核态的转换是非常耗时的，所以说Synchronized是重要级的锁。

**关于Synchronized的升级**

参考文献https://blog.csdn.net/steven2xupt/article/details/108047270

由于synchronized性能问题在JDK1.6前饱受诟病，同时和@author Doug Lea大神写的目前在JUC下的AQS实现的锁差距太大，synchronized开发人员感觉脸上挂不住，所以在1.6版本进行了大幅改造升级，于是就出现了现在常通说的锁升级或锁膨胀的概念,整体思路就是能不打扰操作系统大哥就不打扰大哥，能在用户态解决的就不经过内核。

![img](https://img-blog.csdnimg.cn/2020081722362322.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3N0ZXZlbjJ4dXB0,size_16,color_FFFFFF,t_70#pic_center)

**升级过程**

1. 无锁态
2. 偏向锁
3. 轻量级锁（自旋锁：CAS）
4. 重量级锁



**MarkWord**

![img](https://img-blog.csdnimg.cn/20200817001540364.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3N0ZXZlbjJ4dXB0,size_16,color_FFFFFF,t_70#pic_center)

简单来说：

| 状态       | 标志位 | 存储内容                             |
| ---------- | ------ | ------------------------------------ |
| 未锁定     | 01     | 对象哈希码、对象分代年龄             |
| 轻量级锁定 | 00     | 指向锁记录的指针                     |
| 重量级锁定 | 10     | 执行重量级锁定的指针                 |
| GC标记     | 11     | 空(不需要记录信息)                   |
| 偏向锁     | 01     | 偏向线程ID、偏向时间戳、对象分代年龄 |



![img](https://upload-images.jianshu.io/upload_images/4491294-e3bcefb2bacea224.png)



对象内存布局中的markWord会记录当前只有锁的线程id，如果一个线程获取到锁，那么这个线程线程的id会被记录到markword中。

**锁升级的大致过程：**

> 从**无锁态**，如果有对象尝试获取锁，则进入**偏向锁转态**（这时只有一个线程在使用资源）；
>
> 如果一次还是这个线程使用资源，（尝试获取锁），先会比较markword中的线程id是否为这个线程，如果是，直接获得**偏向锁**。
>
> 如果有竞争（就是不同线程争抢锁），就会升级到轻量级锁（自旋锁CAS）；
>
> 如果竞争激烈，自旋了好久（**默认10次**）都没有竞争到锁，那么就会升级为**重量级锁**，然后**挂起此线程**，等待资源的释放后**重新竞争锁**

>  JDK1.6引入了自适应自旋锁，所谓自适应自旋锁，就意味着自旋的次数不再是固定的，具体规则如下：
>
> 自旋次数通常由前一次在同一个锁上的自旋时间及锁的拥有者的状态决定。如果线程【T1】自旋成功，自旋次数为17次，那么等到下一个线程【T2】自旋时，也会默认认为【T2】自旋17次成功，
>
> 如果【T2】自旋了5次就成功了，那么此时这个自旋次数就会缩减到5次。
>
> 自适应自旋锁随着程序运行和性能监控信息，从而使得虚拟机可以预判出每个线程大约需要的自旋次数





### 7.AQS（AbstractQueuedSynchronizer）：抽象队列同步器

#### CLH锁

AQS是JUC的核心，而CLH锁又是AQS的基础，说核心也不为过，因为AQS就是用了变种的CLH锁。如果要学好Java并发编程，那么必定要学好JUC；学好JUC，必定要先学好AQS；学好AQS，那么必定先学好CLH。因此，这就是我们为什么要学习CLH锁的原因。







## 常用设计模式



### 1.单例模式







### 2.工厂模式







### 3.模板模式







### 4.代理模式



```java
public class ProxyTest {
    public static void main(String[] args) {
        Proxy proxy = new Proxy();
        proxy.Request();
    }
}
//抽象主题
interface Subject {
    void Request();
}
//真实主题
class RealSubject implements Subject {
    public void Request() {
        System.out.println("访问真实主题方法...");
    }
}
//代理
class Proxy implements Subject {
    private RealSubject realSubject;
    public void Request() {
        if (realSubject == null) {
            realSubject = new RealSubject();
        }
        preRequest();
        realSubject.Request();
        postRequest();
    }
    public void preRequest() {
        System.out.println("访问真实主题之前的预处理。");
    }
    public void postRequest() {
        System.out.println("访问真实主题之后的后续处理。");
    }
}
```





### 5.建造者模式









### 6.观察者模式





### 7. 原型设计模式



原型（Prototype）模式的定义如下：用一个已经创建的实例作为原型，通过复制该原型对象来创建一个和原型相同或相似的新对象。在这里，原型实例指定了要创建的对象的种类。用这种方式创建对象非常高效，根本无须知道对象创建的细节。例如，Windows 操作系统的安装通常较耗时，如果复制就快了很多。在生活中复制的例子非常多，这里不一一列举了。

#### 原型模式的优点：

- [Java](http://c.biancheng.net/java/) 自带的原型模式基于内存二进制流的复制，在性能上比直接 new 一个对象更加优良。
- 可以使用深克隆方式保存对象的状态，使用原型模式将对象复制一份，并将其状态保存起来，简化了创建对象的过程，以便在需要的时候使用（例如恢复到历史某一状态），可辅助实现撤销操作。

#### 原型模式的缺点：

- 需要为每一个类都配置一个 clone 方法
- clone 方法位于类的内部，当对已有类进行改造的时候，需要修改代码，违背了开闭原则。
- 当实现深克隆时，需要编写较为复杂的代码，而且当对象之间存在多重嵌套引用时，为了实现深克隆，每一层对象对应的类都必须支持深克隆，实现起来会比较麻烦。因此，深克隆、浅克隆需要运用得当。

#### 原型模式的实现

![原型模式的结构图](http://c.biancheng.net/uploads/allimg/181114/3-1Q114101Fa22.gif)

由于 Java 提供了对象的 clone() 方法，所以用 Java 实现原型模式很简单。

原型模式包含以下主要角色。

1. 抽象原型类：规定了具体原型对象必须实现的接口。
2. 具体原型类：实现抽象原型类的 clone() 方法，它是可被复制的对象。
3. 访问类：使用具体原型类中的 clone() 方法来复制新的对象。

```java
//具体原型类
class Realizetype implements Cloneable {
    Realizetype() {
        System.out.println("具体原型创建成功！");
    }
    public Object clone() throws CloneNotSupportedException {
        System.out.println("具体原型复制成功！");
        return (Realizetype) super.clone(); // 浅拷贝
    }
}
//原型模式的测试类
public class PrototypeTest {
    public static void main(String[] args) throws CloneNotSupportedException {
        Realizetype obj1 = new Realizetype();
        Realizetype obj2 = (Realizetype) obj1.clone();
        System.out.println("obj1==obj2?" + (obj1 == obj2));
    }
}
```



[Java中的深浅拷贝（clone）](https://www.cnblogs.com/xzwblog/p/7230788.html)





## 列举Java几个异常

1. ` java.lang.OutOfMemoryError`
   - 当可用内存不足以让Java虚拟机分配给一个对象时抛出该错误。
2. ` java.lang.StackOverflowError`
   - 当一个应用递归调用的层次太深而导致堆栈溢出或者陷入死循环时抛出该错误
3. `java.lang.CloneNotSupportedException`
   - clone方法所在类没有继承Cloneable接口
4. `java.util.ConcurrentModificationException` 





# Linux 复习

## 常用命令

```bash
# 查看磁盘情况
df -h 

# 查看目录，或文件使用磁盘情况
du

# 查看内存使用情况
free -h

# http 工具

curl URL

# 进程
ps -ef

top

htop 交互式top命令


# 远程同步
scp

rsync   # 同步，同步之间会比较之前的文件，只会同步更改的内容


# 比较两个文件的差异
diff 文件1 文件2 -y -W

# 查看历史命令
history


# 服务管理命令

service

# 管理systemd的资源Unit
systemctl





```



### top详解

```shell
# 当前时间、系统已运行时间、当前登录用户的数量、最近5、10、15分钟内的平均负载
# load average数据是每隔5秒钟检查一次活跃的进程数，然后按特定算法计算出的数值。如果这个数除以逻辑CPU的数量，结果高于5的时候就表明系统在超负荷运转了
top - 15:56:34 up 46 days, 20:13,  0 users,  load average: 8.20, 9.77, 10.36
# tasks 统计，系统现在共有34个任务，1个正在运行，33
Tasks:  34 total,   1 running,  33 sleeping,   0 stopped,   0 zombie
# CPU 使用情况 
# us：用户空间占用情况
# sy：内核空间占用情况
# ni：改变过优先级的进程占用CPU的百分比
# id：空闲CPU百分比
# wa： IO等待占用CPU的百分比
# hi：硬中断占用cpu百分比
# si：软中断占用cpu百分比
%Cpu(s):  5.2 us,  5.1 sy,  0.6 ni, 89.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st
KiB Mem : 26390454+total, 18626068 free, 18532243+used, 59956032 buff/cache
KiB Swap:        0 total,        0 free,        0 used. 47536756 avail Mem 
```



# MySql复习





## 1. 数据库三范式

1. **第一范式：每个列不可再分**
2. **第二范式：不存在部分函数依赖**
3. **第三范式：不存在传递函数依赖**



## 2.MySql存储引擎

MySql中的数据，索引以及其他对象是如何存储的，是一套文件系统实现的（Storage Engine）

MY_SQL存储引擎有以下几种：

- MRG_MYISAM
- MyISM
- BLACKHOLE
- CSV
- MEMORY
- ARCHIVE
- InnoDB
- PERFORMANCE_SCHEMA



**InnoDB引擎**：InnoDB引擎提供了对数据库ACID事务的支持。并且还提供了行级锁和外键的约束。它的设计的目标就是处理大数据容量的数据库系统。

**MyISAM引擎**：不提供事务的支持，也不支持行级锁和外键

**Memory引擎**：所有的数据都在内存中，数据的处理速度快，但是安全性不搞。



**常见的*MyISAM*与*InnoDB*的比较**

|                                                              | MyISAM                                                       | InnoDB                                                       |
| ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 存储结构                                                     | 每张表被存放在三个文件：<br />frm-表格定义<br />MYD（MYData）-数据文件<br />MYI（MYIndex）-索引文件 | 所有的表都保存在同一个数据文件中<br />（也可能是多个文件，或者是独立的表空间文件）<br />InnoDB表的大小只受限于操作系统文件的大小，<br />一般为2GB |
| 存储空间                                                     | MyISAM可被压缩，存储空间较小                                 | InnoDB的表需要更多的内存和存储，<br />它会在主内存中建立专用的缓冲池<br />用于高速缓冲数据和索引 |
| 可移植性、备份及恢复                                         | 由于MyISAM的数据是以文件的形式存储，所以在跨平台的数据转移中会很方便。<br />在备份和恢复时可单独针对某个表进行操作。 | 数据和索引都是集中存储的                                     |
| 记录存储顺序                                                 | 按记录插入顺序保存                                           | 按主键大小有序插入                                           |
| 外键                                                         | **不支持**                                                   | **支持**                                                     |
| 事务                                                         | **不支持**                                                   | **支持**                                                     |
| 锁支持（锁是避免资源争用的一个机制，MySql锁对用户几乎是透明的） | **表级锁定**                                                 | **行级锁定**，**表级锁定**，锁定粒度小并发能力高             |
| SELECT                                                       | MyIsam更有                                                   |                                                              |
| INSERT、UPDATE、DELETE                                       |                                                              | InnoDB更优                                                   |
| 索引的实现方式                                               | B+树索引，myisam是堆表                                       | B+树索引，InnoDB是索引组织表                                 |
| 哈希索引                                                     | 不支持                                                       | 支持                                                         |
| 全文索引                                                     | 支持                                                         | 不支持                                                       |

**MyISAM索引与InnoDB索引的区别**

- InnoDB索引是聚簇索引（索引的存储顺序与实际的数据物理存储顺序保持一致），MyIsam索引是非聚簇索引
- InnoDB的主键索引的叶子结点存储着行数据，因此主键索引非常高效
- MyISAM索引的叶子结点存储的是行数据地址，需要再寻址一次才能得到数据
- InnoDB非主键索引的叶子结点存储的是主键和其他索引的列数据，因此查询时做到覆盖索引会非常高效。





## 3. 索引

### 3.1 什么是索引？

索引是一种数据结构。数据库索引，是数据库管理系统中一个排序的数据结构，以协助快速查询、更新数据库表中数据。索引的实现通常使用B树及其变种B+树。

### 3.2 索引的优点

- 可以大大加快数据的检索速度，这就是创建索引的最主要的原因
- 通过使用索引，可以在查询的过程中，使用优化器，提高系统的性能

### 3.3 索引的缺点

- 时间方面：创建索引和维护索引要耗费时间，具体地，当对表中的数据进行增加，删除和修改的时候，索引也要动态的维护，会降低增删改的执行效率。
- 空间方面：索引需要占物理空间



### 3.4 索引使用注意事项

索引虽好，但是索引使用不恰当就会造成索引失效

**索引使用口诀：**

1. **全值匹配我最爱**
   - 就是查询的列都是索引（覆盖索引，这中情况不需要查询实际的物理数据，只需要查询索引树即可）
2. **最佳左前缀法则**
   - 如果索引了多列，在使用时就要遵守最左前缀法则
     - 带头大哥不能死
     - 中间兄弟不能断
3. **索引列上不计算**
   - 使用sum，avg等计算或者 自动(手动)类型转换，都会导致索引失效
4. **范围之后全失效**
5. **%加在like右边**
6. **字符串里有引号**
   - 字符转不加单引号，会导致索引失效
7. 其他
   - 使用**!=，<>, is null； is not null ；or**都会导致索引失效



**非聚簇索引一定会产生回表吗？**

不一定。如果查询语句中的的列全部命中索引，那就不必再进行回表查询了。



## 4. MySql日志

MariaDB/MySql中日志包括：

1. 错误日志（Error log）：记录mysql服务启动时正确和错误的信息，还记录启动、停止、运行过程中的错误信息
2. 查询日志（general log）：记录建立的客户端连接和执行的语句。
3. 二进制日志（binlog）：记录所有更改数据的语句，可用于数据复制
4. 慢查询日志（show log）：记录所有执行时间超过long_query_time的所有查询或不使用索引的查询
5. 中继日志（relay log）：主从复制时使用的日志。
6. InnoDB引擎还有事务日志



### 4.1 二进制日志

[事务日志详解](https://www.cnblogs.com/f-ck-need-u/archive/2018/05/08/9010872.html)

MySql支持statement、row、mixed三种形式的记录方式。

- statement
  - 将所有的相关操作记录为SQL语句的形式
  - 这样的记录方式对某些特殊信息无法同步记录，例如uuid，now()等这样的动态变化的值。
- row
  - 基于行来记录，将相关行的每一列的值都在日志中保存下来
  - 这样的结果会导致日志文件变得非常大，但是保证了动态值的确定性。
- mixed
  - statement与row混合形式
  - 默认采用statement的方式记录，只有以下几种情况会采用row的形式来记录日志
    - 表的存储引擎为NDB，这是对表的DML操作都会以row的格式记录
    - 使用了uuid(), user(), current_user(), found_rows(), row_cuount()等不确定函数。但是测试发现对now()函数仍然会以statement格式记录，而sysdate()函数会以row格式记录。
    - 使用了insert delay语句
    - 使用了临时表



### 4.2 事务日志

InnoDB存储引擎的事务日志包括：undo log 和 redo log

redo log 通常是物理日志，记录的是数据页的物理页修改，而不是某一行或某几行修改成怎么样，它用来恢复提交后的物理数据页（恢复数据页，且只能恢复到最后一次提交的位置）

undo用来回滚行记录到某个版本。undo log一般是逻辑日志，根据每行记录进行记录。

#### 1. redo log

redo log不是二进制日志。虽然二进制日志也记录了innodb表的很多操作，也能实现重做的功能，但是他们之间有很大区别。

1. 二进制日志是在**存储引擎的上层**产生的，不管是什么存储引擎，对数据库进行了修改都会产生二进制日志。而redo log是innodb层产生的，只记录该存储引擎中表的修改。**并且二进制日志先于redo log被记录**。具体的见后文group commit小结。
2. 二进制日志记录操作的方法是逻辑性的语句。即便它是基于行格式的记录方式，其本质也还是逻辑的SQL设置，如该行记录的每列的值是多少。而redo log是在物理格式上的日志，它记录的是数据库中每个页的修改。
3. 二进制日志只在每次事务提交的时候一次性写入缓存中的日志"文件"(对于非事务表的操作，则是每次执行语句成功后就直接写入)。而redo log在数据准备修改前写入缓存中的redo log中，然后才对缓存中的数据执行修改操作；而且保证在发出事务提交指令时，先向缓存中的redo log写入日志，写入完成后才执行提交动作。
4. 因为二进制日志只在提交的时候一次性写入，所以二进制日志中的记录方式和提交顺序有关，且一次提交对应一次记录。而redo log中是记录的物理页的修改，redo log文件中同一个事务可能多次记录，最后一个提交的事务记录会覆盖所有未提交的事务记录。例如事务T1，可能在redo log中记录了 T1-1,T1-2,T1-3，T1* 共4个操作，其中 T1* 表示最后提交时的日志记录，所以对应的数据页最终状态是 T1* 对应的操作结果。而且redo log是并发写入的，不同事务之间的不同版本的记录会穿插写入到redo log文件中，例如可能redo log的记录方式如下：` T1-1,T1-2,T2-1,T2-2,T2*,T1-3,T1* `。
5. 事务日志记录的是物理页的情况，它具有幂等性，因此记录日志的方式极其简练。幂等性的意思是多次操作前后状态是一样的，例如新插入一行后又删除该行，前后状态没有变化。而二进制日志记录的是所有影响数据的操作，记录的内容较多。例如插入一行记录一次，删除该行又记录一次。

**Redo log的基本概念**

redo log包括两部分：一是内存中的日志缓冲(redo log buffer)，该部分日志是易失性的；二是磁盘上的重做日志文件(redo log file)，该部分日志是持久的。

在概念上，innodb通过***force log at commit\***机制实现事务的持久性，即在事务提交的时候，必须先将该事务的所有事务日志写入到磁盘上的redo log file和undo log file中进行持久化。

为了确保每次日志都能写入到事务日志文件中，在每次将log buffer中的日志写入日志文件的过程中都会调用一次操作系统的fsync操作(即fsync()系统调用)。因为MariaDB/MySQL是工作在用户空间的，MariaDB/MySQL的log buffer处于用户空间的内存中。要写入到磁盘上的log file中(redo:ib_logfileN文件,undo:share tablespace或.ibd文件)，中间还要经过操作系统内核空间的os buffer，调用fsync()的作用就是将OS buffer中的日志刷到磁盘上的log file中。



**redo log的格式**

因为InnoDB存储引擎数据的单元是页，所以redo log也是基于页的格式来记录的。InnoDB的页大小是16kb，一个页可以存放非常多的log blcok（512字节），而log block中记录的有时数据页的变化。



**InnoDB的恢复行为**

在启动InnoDB的时候，不管上次是正常关闭还是异常关闭，总是会进行恢复操作。

因为redo log记录的是数据页的物理变化，因此恢复的时候速度比逻辑日志（比如binlog）要快很多。而且，InnoDB自身也做了一定程度的优化，让恢复速度变得更快。



#### 2. undo log

undo log有两个作用：提供回滚和多个行版本控制（MVCC）

在数据修改的时候，不仅记录了redo，还记录了相应的undo，如果因为某些原因导致事务失败或回滚，可以借助该undo进行回滚。

undo log 和 redo log记录物理日志不一样，它是逻辑日志。<font color=red>可以认为当delete一条记录时，undo log中会记录一条对应的insert记录，反之亦然，当update一条记录时，它记录一条对应相反的update记录。</font>

当执行rollback时，就可以从undo log中的逻辑读取到相应的内容并进行回滚。有时候应用到行版本控制的时候，也是通过undo log来实现的：当读取的某一行被其他事务锁定时，它可以从undo log中分析出该行记录以前的数据是什么，从而提供该行版本信息，让用户实现非锁定一致性读取。

<font color=red>undo log 是采用段（segment）的方式来记录的，每个undo操作在记录的时候占用一个undo log segment。</font>

另外，undo log也会产生redo log，因为undo log也要实现持久性保护。

**undo log 的存储方式**

InnoDB存储引擎对undo 的管理采用段的方式。rollback segment称为回滚段，每个混滚段中有1024个undo log segment。



另一篇帖子中的介绍：http://www.llbiancheng.com/5623.html

**Undo**：意为取消，以撤销操作为目的，返回指定某个状态的操作。

**Undo Log**：数据库事务提交之前，会将事务修改数据的镜像（即修改前的旧版本）存放到 undo 日志里，当事务回滚时，或者数据库奔溃时，可以利用 undo 日志，即旧版本数据，撤销未提交事务对数据库产生的影响。。

- 对于 insert 操作，undo 日志记录新数据的 PK(ROW_ID)，回滚时直接删除；
- 对于 delete/update 操作，undo 日志记录旧数据 row，回滚时直接恢复；
- 他们分别存放在不同的buffer里。

**Undo Log 是为了实现事务的原子性而出现的产物。**

**Undo Log 实现事务原子性**：事务处理过程中，如果出现了错误或者用户执行了 ROLLBACK 语句，MySQL 可以利用 Undo Log 中的备份将数据恢复到事务开始之前的状态。

InnoDB 发现可以基于 Undo Log 来实现多版本并发控制。

**Undo Log 在 MySQL InnoDB 存储引擎中用来实现多版本并发控制。**

**Undo Log 实现多版本并发控制**：事务未提交之前，Undo Log 保存了未提交之前的版本数据，Undo Log 中的数据可作为数据旧版本快照供其他并发事务进行快照读。

关于Undo log是怎么实现MVCC的，请参考上篇文章：[吃透MySQL（九）：MVCC多版本并发控制](https://blog.csdn.net/u013277209/article/details/114360409)



**Redo**：顾名思义就是重做。以恢复操作为目的，重现操作。

**Redo Log**：指事务中操作的任何数据，将最新的数据备份到一个地方（Redo Log）。

**Redo Log 的持久化**：不是随着事务的提交才写入的，而是在事务的执行过程中，便开始写入 Redo Log 中，具体的落盘策略可以进行配置。

**Redo Log 是为了实现事务的持久性而出现的产物。**

**Redo Log 实现事务持久性**：防止在发生故障的时间点，缓冲池（buffer pool）尚有脏页未写入表的 IBD 文件中，在重启 MySQL 服务的时候，根据 Redo Log 进行重做，从而达到事务的未入磁盘数据进行持久化这一特性。

一旦事务成功提交且数据从缓冲池（buffer pool）持久化到表的 IBD 文件中之后，此时 Redo Log 中的对应事务数据记录就失去了意义，所 以 Redo Log 的写入是日志文件循环写入的过程，也就是覆盖写的过程。





### 4. 事务

#### 4.1 什么是事务？

事务是一个不可分割的数据库操作序列，也是数据库并发控制的基本单位，其执行的结果必须使数据库从一种一致性状态变到另一种一致性状态。事务是逻辑上的一组操作，要么都执行，要么都不执行。

#### 4.2 事务的四大特性（ACID）

- **原子性**（Atomicity）
- **一致性**（Consistency）
- **隔离性**（Isolation）
- **持久性**（Durability）



[参考文章](https://www.cnblogs.com/kismetv/p/10331633.html)

##### 4.2.1 原子性

原子性是指一个事务是一个不可分割的工作单位，其中的操作要么都做，要么都不做；如果事务中一个sql语句执行失败，则已执行的语句也必须回滚，数据库退回到事务前的状态。

- 实现原理：undo log

##### 4.2.2 持久性

持久性是指事务一旦提交，它对数据库的改变就应该是永久性的。就下来的其他操作或故障不应该对其有任何影响。

- 实现原理：redo log

##### 4.2.3 隔离性

与原子性、持久性侧重于研究事务本身有所不同，隔离性研究的是不同事务之间的相互影响。隔离性是指，事务内部的操作与其他事务是隔离的，并发执行的各个事务之间不能互相干扰。

隔离性追求的是并发情形下事务之间互不干扰。

隔离性探讨的问题主要分为两方面：

- （一个事务）写操作对（另一个事务）写操作的影响：锁机制保证隔离性
- （一个事务）写操作对（另一个事务）读操作的影响：MVCC保证隔离性

首先来看两个事务的写操作之间的相互影响。隔离性要求同一时刻只能有一个事务对数据进行写操作，InnoDB通过锁机制来保证这一点。

锁机制的基本原理可以概括为：事务在修改数据之前，需要先获得相应的锁；获得锁之后，事务便可以修改数据；该事务操作期间，这部分数据是锁定的，其他事务如果需要修改数据，需要等待当前事务提交或回滚后释放锁。

**行锁与表锁**

按照粒度，锁可以分为表锁、行锁以及其他位于二者之间的锁。表锁在操作数据时会锁定整张表，并发性能较差；行锁则只锁定需要操作的数据，并发性能好。但是由于加锁本身需要消耗资源(获得锁、检查锁、释放锁等都需要消耗资源)，因此在锁定数据较多情况下使用表锁可以节省大量资源。MySQL中不同的存储引擎支持的锁是不一样的，例如MyIsam只支持表锁，而InnoDB同时支持表锁和行锁，且出于性能考虑，绝大多数情况下使用的都是行锁。



**脏读、不可重复读和幻读**



并发情况下，读操作可能存在的三类问题：

1. 脏读：当前事务A可以读到其他事务B未提交的数据（脏数据），这种现象为脏读

   > ![img](https://img2018.cnblogs.com/blog/1174710/201901/1174710-20190128201003630-2050662608.png)
   >
   > 

2. 不可重复读：在事务A中先后两次读取同一个数据，两次读取的结果不一样，这种现象称为不可重复读。

   - 脏读与不可重复读的区别在于：前者读到的是其他事务未提交的数据，后者读到的是其他事务已提交的数据

   > ![img](https://img2018.cnblogs.com/blog/1174710/201901/1174710-20190128201011603-1317894910.png)

3. 幻读：在事务A中按照某个条件先后两次查询数据库，两次查询结果的条数不同，这种现象称为幻读。

   - 不可重复读与幻读的区别在于：前者是数据变了，后者是数据的行变了

   > ![img](https://img2018.cnblogs.com/blog/1174710/201901/1174710-20190128201021606-1089980279.png)

**事务的隔离级别**

SQL标准中定义了四种隔离级别，并规定了每种隔离级别下上述几个问题是否存在。一般来说，隔离级别越低，系统开销越低，可支持的并发越高，但隔离性也越差。隔离级别与读问题的关系如下：

![img](https://img2018.cnblogs.com/blog/1174710/201901/1174710-20190128201034603-681355962.png)

- Read UnCommitted 读取未提交内容
  - 在这个隔离级别，所有事务都可以“看到”为提交事务的执行结果。（会造成脏读、不可重复读、幻读）
- Read Committed 读取提交内容
  - 一个事务从开始到提交前，所做的任何数据改变都是不可见的，除非已经提交了。（解决了脏读，但是没有解决不可重复读  和 幻读）
- Repeatable Read 可重复读
  - MySql数据库默认的隔离级别
  - 它保证同一事务的多个实例在并发读取事务时，会“看到同样的”数据行。（解决了 脏读 和 不可重复读，但是没有解决  幻读）
- Serializable 可串行化
  - 该级别是最高级别的隔离级。它通过强制事务排序，使之不可能相互冲突，从而解决幻读问题。简而言之，SERIALIZABLE是在每个读的数据行上加锁。在这个级别，可能导致大量的超时`Timeout`和锁竞争`Lock Contention`现象，实际应用中很少使用到这个级别，但如果用户的应用为了数据的稳定性，需要强制减少并发的话，也可以选择这种隔离级



**MVCC多版本并发控制**

MVCC可以解决幻读

<font color=green>InnoDB的MVCC实现机制</font>

- InnoDB的MVCC实现，是通过保存数据在某个时间点的快照实现的。
- 一个事务，不管其执行多长时间，其内部看到的数据是一致的。事务在执行过程中不会相互影响

MySql中，每条实际的行数据除了我们定义的字段外，还有几个隐藏的列，其中有关于MVCC的重要字段有两个：**DATA_TRX_ID**和**DELETE_BIT**

- DATA_TRX_ID 标记了最新更新这条行记录的transaction id，每处理一个事务，其值自动+1

- DELETE_BIT 用于标识该记录是否被删除，这里的不是真正的删除数据，而是标志出来的删除。真正意义的删除是在commit的时候。



下面分别以select、delete、insert、update语句来说明：

- **INSERT**
  - InnoDB为每个新增行记录当前系统版本号（事务ID）作为创建ID（DATA_TRX_ID）
- **DELETE**
  - InnoDB为每个删除行记录当前系统版本号（事务ID）作为删除ID（DELETE_BIT）
- **UPDATE**
  - InnoDB复制了一行。这个新行的版本号使用了系统版本号。它也把系统版本号作为了删除行的版本。
- **SELECT**
  - InnoDB检查每行数据，确保他们符合两个标准
    - InnoDB只查找早于当前事务版本的数据行（也就是数据行的版本必须小于等于事务的版本），这确保当前事务读取的行都是事务之前已经存才的，或者是由当前事务创建或修改的行。
    - 行的删除操作的版本一定是未定义的或者大于当前事务版本号，确定了当前事务开始之前，行没有被删除
  - 符合了以上两点则返回查询结果

InnoDB中的MVCC实现方式：

- 事务以排它锁的形式修改原始数据
- 把修改前的数据存放于undo log，通过回滚指针与主数据关联
- 修改成功（commit）啥都不做，失败则恢复undo log中的数据（rollback）



二者最本质的区别是，当修改数据时是否要排他锁定，如果锁定了还算不算是MVCC？ 

 

Innodb的实现真算不上MVCC，因为并没有实现核心的多版本共存，undo log中的内容只是串行化的结果，记录了多个事务的过程，不属于多版本共存。但理想的MVCC是难以实现的，当事务仅修改一行记录使用理想的MVCC模式是没有问题的，可以通过比较版本号进行回滚；但当事务影响到多行数据时，理想的MVCC据无能为力了。

 

比如，如果Transaciton1执行理想的MVCC，修改Row1成功，而修改Row2失败，此时需要回滚Row1，但因为Row1没有被锁定，其数据可能又被Transaction2所修改，如果此时回滚Row1的内容，则会破坏Transaction2的修改结果，导致Transaction2违反ACID。

 

理想MVCC难以实现的根本原因在于企图通过乐观锁代替二段提交。修改两行数据，但为了保证其一致性，与修改两个分布式系统中的数据并无区别，而二提交是目前这种场景保证一致性的唯一手段。二段提交的本质是锁定，乐观锁的本质是消除锁定，二者矛盾，故理想的MVCC难以真正在实际中被应用，Innodb只是借了MVCC这个名字，提供了读的非阻塞而已。



##### 4.2.4 一致性

一致性是指事务执行结束后，**数据库的完整性约束没有被破坏，事务执行的前后都是合法的数据状态**。数据库的完整性约束包括但不限于：实体完整性（比如行的主键存在且唯一）、列完整性（如字段的类型，大小，长度要符合要求）、外键约束、用户自定义完整性。

可以说，一致性是事务追求的最终目标：前面提到的原子性，持久性和隔离性，都是为了保证数据库状态的一致性。此外，除了数据库层面的保障，一致性的实现也需要应用层面进行保障。



实现一致性的措施：

- 保证原子性、持久性和隔离，如果这些特性无法保证，事务的一致性也无法保证。
- 数据库本身提供保障，例如不允许向整型列插入字符串值，字符串长度不能超过列的限制等
- 应用层面进行保障，例如如果转账操作只扣除转账者的余额，而没有增加接受者的余额，无论数据库实现的多么完美，也无法保证状态的一致性。

​			



















## 5. 锁

MySQL里面的锁大致可以分成 **全局锁**、**表级锁**和**行级锁**这三类。

### 全局锁

全局锁就是对整个数据库实例加锁。

**全局锁的典型使用场景：做全库逻辑备份**，也就是把整库每个表都select出来存成文本。

但是全局锁会导致整个库进入只读状态，在实际的线上任务中，这很危险。



### 表级锁

Mysql中的表级别的锁有两种：

- 表锁

  - MyISAM引擎
    - **表共享读锁**
      - 不会阻塞其他线程对同一个表的读操作请求，但会阻塞其他线程的写操作请求；
    - **表独占写锁**
      - 一旦表被加上独占写锁，那么无论其他线程是读操作还是写操作，都会被阻塞。

  默认情况下，写锁比读锁具有更高的优先级；当一个锁释放后，那么它会优先相应写锁等待队列中的请求，然后再是读锁中等待的获取锁的请求。

  - InnoDB引擎
    - 表锁---------**-意向锁**
      - 由于表锁和行锁虽然作用范围不同，但是会相互冲突。当你要加表锁时，势必要先遍历表的所有记录，判断是否有**排它锁**。这种遍历检查的方式显然是一种低效的方式。InnoDB引入了**意向锁**，来检测表锁和行锁的冲突。
      - 意向锁也是表级锁，分为**读意向锁(IS)**，和**写意向锁(IX)**。当事务要在记录上加行锁时，要首先在表上加意向锁。这样判断表中是否有记录正在加锁就很简单了，只要看下表上是否有意向锁就行了。从而就能提升效率。
      - 意向锁之间不会产生冲突，它只会阻塞表级读锁或写锁。意向锁不与行锁发生冲突。

- 元数据锁（MDL）

  - 元数据锁MDL是系统默认加的
  - 当表的结构发生变化时，这个锁就会生效

> 表锁不会出现死锁，发生锁的冲突几率高，并发低
>
> MyISAM在执行查询语句（select）前，会自动给涉及的所有表加读锁，在执行insert、delete和update前，会自动给涉及的表加写锁。
>
> 读锁会阻塞写，写锁会阻塞读和写
>
> - MyISAM表的读操作，不会阻塞其他线程对同一表的读请求，但会阻塞对同一表的写请求。只有当读锁释放后，才会执行其他进程的写操作。
> - 对MyISAM表的写操作，会阻塞其他进程对同一表的读和写操作，只有当写锁释放后，才会执行其他进程的读写操作。
>
> MyISAM引擎不适合做写为主表的引擎，因为写锁后，其他线程不能做任何操作，大量的更新会使查询很难得到锁，从而造成永远阻塞。





### 行锁

**InnoDB中的行锁**

InnoDB实现了一下两种类型的行锁：

- 共享锁（S）：加了锁的记录，所有事务都能去读取但不能修改，同时阻止其他事务获得相同数据集的排它锁
- 排它锁（X）：允许已经获得排它锁的事务去更新数据，阻止其他事务获得相同数据集的共享锁和排它锁。

**锁模式的兼容矩阵**

下面表显示了了各种锁之间的兼容情况：

|      | X    | IX   | S    | IS   |
| ---- | ---- | ---- | ---- | ---- |
| X    |      |      |      |      |
| IX   |      | 兼容 |      | 兼容 |
| S    |      |      | 兼容 | 兼容 |
| IS   |      | 兼容 | 兼容 | 兼容 |

（注意上面的X与S是说表级的X锁和S锁，意向锁不和行级锁发生冲突）

如果一个事务请求的锁模式与当前的锁兼容，InnoDB就将请求的锁授予该事务；如果两者不兼容，那么该事务就需要等待锁的释放。



<font color=red>注意：</font>

<font color=green>InnoDB的行锁是作用在索引上的，哪怕建表的时候没有定义一个索引，InnoDB也会创建一个聚簇索引并将其作为锁作用的索引。</font>

<font color=green>行锁必须有索引才能实现，否则会自动锁全表。</font>

- 两个事务不能锁同一个索引
- insert，delete，update在事务中都会自动默认加上排它锁

## 6. 性能分析与优化

### Exlain查询执行计划

```sql
explain select 
								id
								,name
								,addr
				from		t1
        where		t1.id = 20
```

通过在sql前面添加explain关键字即可查询sql的执行计划

![img](https://gimg2.baidu.com/image_search/src=http%3A%2F%2Faliyunzixunbucket.oss-cn-beijing.aliyuncs.com%2Fjpg%2F6dd68b173df7809bc8dc27a30937a22d.jpg%3Fx-oss-process%3Dimage%2Fresize%2Cp_100%2Fauto-orient%2C1%2Fquality%2Cq_90%2Fformat%2Cjpg%2Fwatermark%2Cimage_eXVuY2VzaGk%3D%2Ct_100&refer=http%3A%2F%2Faliyunzixunbucket.oss-cn-beijing.aliyuncs.com&app=2002&size=f9999,10000&q=a80&n=0&g=0n&fmt=jpeg?sec=1630634532&t=cc5ce4c5d99be9f2529373c3cd481029)

如上图，sql的执行计划共有10个字段

1. **id**

   > 表的查询顺序，需要越大表就越先被查询，相同id，表从上之下依次执行

2. **select_type**

   > 查询类型：
   >
   > **SIMPLE**：不带有任何复杂查询
   >
   > **PRIMARY**：查询中若包含任何复杂的子部分，最外层查询则被标记为PRIMARY
   >
   > **SUBQUERY**：在select或where列表中包含了子查询
   >
   > **DERIVED**衍生：在from列表中包含的子查询标记为DERIVEN（MySql会递归执行这些子查询，把结果放在临时表里）
   >
   > **UNION**：若第二个select出现在union之后，则被标记为UNION
   >
   > ​				若UNION包含在from子句的子查询中，外层select将被标记为：DERIVED
   >
   > **UNION RESULT**：从UNION表获取结果的select

3. **table**

   - 显示这一行数据是关于哪张表的

4. **type**

   > **查询的访问类型**
   >
   > 常见的访问类型：system--const--eq_ref--ref--range--index--ALL
   >
   > 1. **System**:表中只有一行记录，这是const类型的特例
   > 2. **const**：表示通过索引一次就找到了，const用于比较primary key 或者unique索引，因为只匹配一行数据，所以很快
   > 3. **eq_ref**：唯一性索引扫描，对于每个索引键，表中只用一条记录与之匹配。常见于主键或唯一性索引
   > 4. **ref**：非唯一性索引扫描，返回匹配某个单独值的所有行，本质上也是一种索引访问。
   > 5. **range**：指索引给定范围的行，使用一个索引来选择行；between、<、>和in等的查询
   > 6. **index**：出现index是sql使用了索引但是没有通过索引进行过滤，一般是使用了覆盖索引或者利用索引进行排序，分组等。
   > 7. **ALL**：全表扫描
   >
   > index 与 ALL的区别：都是全表扫描，但是index遍历的只有索引树，而ALL是从硬盘中读取全部的表数据，前者的速度要快于后者。

   一般情况下，查询至少达到range级别，最好达到ref级别

   5. **possible_keys**

      > 显示可能应用在这张表中的索引，一个或者多个
      >
      > 查询涉及到的字段若存在索引，该索引将被列出
      >
      > **但是不一定被查询实际使用**

   6. **key**

      > 实际使用的索引。如果为NULL，则没有使用索引
      >
      > 查询中若使用了覆盖索引，则该索引仅出现在key列表中

   7. **key_len**

      > 表示索引中使用的字节数，通过该列计算查询中使用的索引长度，在不损失精确性的情况下，长度越短越好。
      >
      > key_len显示的值为索引字段的最大可能长度，并非实际使用长度，即key_len是根据表定义计算而得，不是通过表内检索出的。

   8. **ref**

      > 显示索引的哪一列被使用了，如果可能的话，是一个常数，哪些列或常量被用于查找索引列上的值

   9. **rows**

      > 根据表统计信息即索引选取情况，大致估算出找到所需的记录所需要读取的行数

   10. **Extra**

       1. **using filesort**
          - 说明mysql会对数据使用一个外部的索引排序，而不是按照表内的索引顺序进行读取
          - mysql中无法利用索引完成的排序操作称为：“文件排序”
       2. **using temporary**
          - 使用了临时表保存中间结果，mysql在对查询结果排序时，使用临时表。常见于排序order by和分组查询group by
       3. **using index**
          - using index表示相应的select操作中使用了覆盖索引，避免访问了表的数据行，效率不错！
          - 如果同时出现using where，表名索引被用来执行索引键值的查找；如果没有同时出现using where，表名索引只是用来读取数据而非利用索引执行查找
          - 利用索引进行了排序或者查找
       4. **using where**
       5. **using join buffer**









## 7. Mysql主从复制 & 集群



### 7.1 MySql主从复制

[docker 搭建mysql主从复制](https://blog.csdn.net/weixin_44617722/article/details/111996883)

[mysql授权用户](https://www.cnblogs.com/felix-h/p/11072743.html)

MySQL主从复制是指数据可以从一个MySQL数据库服务器主节点复制到一个或多个从节点。

MySQL主从复制默认采用异步复制方式，这样从节点不用一直访问主服务器来更新自己的数据，数据的更新可以在远程连接上进行，从节点可以复制主数据库中的所有数据库或者特定的数据，或者特定的表。

**Mysql主从复制原理**

1. master服务器将数据的改变记录到二进制日志binlog日志，当master上的数据发生改变时，则将其改变写入二进制日志中；
2. slave服务器会在一定时间间隔内对master二进制日志进行探测其是否发生改变，如果发生改变，则开始一个I/O Thread请求master二进制事件。
3. 同时主节点为每个I/O线程启动一个dump线程，用于向其发送二进制事件，并保存至从节点本地的**中继日志中（relay log）**，从节点将启动SQL线程从中继日志中读取二进制日志，在本地重放，似的其数据和主节点的保持一致，最后I/O Thread和SQL Thread将进入睡眠，等待下一下被唤醒。



- 从库会生成两个线程：一个I/O线程，一个SQL线程
- I/O线程会去请求主库的binlog，并将得到的binlog写到本地的relay-log（中继日志）文件中
- 主库会生成一个log dump线程，用来给从库I/O线程传binlog；
- SQL线程会读取relay log文件中的日志，并解析成sql语句逐一执行；

**注意**

1. master将操作语句记录到binlog日志中，然后授予slave远程连接的权限（master一定要开启binlog二进制日志功能；通常为了数据安全考虑，slave也开启binlog功能）。
2. slave开启两个线程：IO线程和SQL线程。其中：IO线程负责读取master的binlog内容到中继日志relay log里；SQL线程负责从relay log日志里读出binlog内容，并更新到slave的数据库里，这样就能保证slave数据和master数据保持一致了。 
3. Mysql复制至少需要两个Mysql的服务，当然Mysql服务可以分布在不同的服务器上，也可以在一台服务器上启动多个服务。
4. Mysql复制最好确保master和slave服务器上的Mysql版本相同（如果不能满足版本一致，那么要保证master主节点的版本低于slave从节点的版本）
5. master和slave两节点间时间需同步

![img](https://pic3.zhimg.com/80/v2-cf37bafd8a121454b5488c53ff2e0b2e_1440w.jpg)

具体细节

1. 从库通过手工执行change master to 语句连接主库，提供了连接的用户一切条件（user 、password、port、ip），并且让从库知道，二进制日志的起点位置（file名 position 号）； start slave
2. 从库的IO线程和主库的dump线程建立连接。
3. 从库根据change master to 语句提供的file名和position号，IO线程向主库发起binlog的请求。
4. 主库dump线程根据从库的请求，将本地binlog以events的方式发给从库IO线程。
5. 从库IO线程接收binlog events，并存放到本地relay-log中，传送过来的信息，会记录到[master.info](https://link.zhihu.com/?target=http%3A//master.info)中
6. 从库SQL线程应用relay-log，并且把应用过的记录到[relay-log.info](https://link.zhihu.com/?target=http%3A//relay-log.info)中，默认情况下，已经应用过的relay 会自动被清理purge

**主从复制优缺点**

- 读写分离：一主多从，主写，从读，分散压力。
- 缺点
  - 数据库服务存在单点故障（主库所在机器可能宕机）
  - 数据库服务器资源无法满足增长的读写请求
  - 高峰时数据库连接数经常超过上线
  - 同步机制为**异步**

### 7.2 Mysql集群





















## 8. 补充



### 8.1 SQL的生命周期

<img src="https://github.com/xiaowodi/Resources/blob/main/images/gitImages/Mysql%E5%9F%BA%E6%9C%AC%E6%9E%B6%E6%9E%84.png?raw=true" alt="Mysql基本架构.png" style="zoom:30%;" />





1. 首先客户端向服务器提交要执行的SQL；

   客户端需要通过**连接器**连接到**Server**，并验证这个客户端的权限等

2. 连接建立完成后，就可以执行sql，执行逻辑的第二步就是要**查询缓存**，如果缓存中有之前查询的结果，就直接返回给客户端。（缓存中类似于Key-Value的形式）

3. 如果缓存没有命中，接下来就需要**分析器**，经过*词法分析*，*语法分析*，来分析这个sql语句是否符合sql语法规范。

4. 对于可以执行的sql要经过**优化器**进行优化

5. 优化后的sql就会到**执行器**中，执行这个sql逻辑

   - 开始执行的时候，要先判断一下客户端对这个表有没有执行查询的权限，如果没有，就会返回权限错误。
   - 如果有权限，就打开表继续执行。打开表的时候，执行器就会根据表的引擎定义，去使用这个引擎提供的接口。













# Hadoop复习



## 1. 分布式文件存储系统HDFS

### 1.1 HDFS的机架感知策略

- 机架：存放服务器的架子，也叫机柜。一般来说一个机房有很多机柜，每个机柜有很多服务器

**副本存放策略**

HDFS分布式文件系统的内部有一个副本存放策略：以默认的副本数=3为例：

1. 第一个副本块存本机
2. 第二个副本块存放在跟本机同机架内的其他服务器节点
3. 第三个副本块存放在不同于本机架的一个服务器节点上

==好处：==

1. 如果本机数据损坏或者丢失，那么客户端可以从同机架的相邻节点获取数据，速度肯定要比跨机架获取数据要快。
2. 如果本机所在的机架出现问题，那么之前在存储的时候没有把所有副本都放在一个机架内，这就能保证数据的安全性，此种情况出现，就能保证客户端也能取到数据。

HDFS为了降低整体的网络带宽消耗和读取延时，HDFS集群一定会让客户端尽量去读取近的副本，那么按照以上解释的副本存放策略：

1. 如果在本机有数据，那么直接读取；
2. 如果在跟本机同机架的服务器节点中有该数据块，则直接读取
3. 如果该HDFS集群跨多个数据中心，那么客户端也一定会优先读取本数据中心的数据。

但是HDFS是如何确定两个节点是否属于同一个机架，如何确定不同服务器跟客户端的远近呢？那就是**机架感知**



### 1.2 **NameNode & DataNode & Secondary NameNode**

整个HDFS集群由Namenode和Datanode构成master-worker（主从）模式。Namenode负责构建命名空间，管理文件的元数据等，而Datanode负责实际存储数据，负责读写工作。

#### **NameNode**

NameNode存放文件系统树以及所有文件、目录的元数据。

元数据持久化为2种形式：

- namespace image
- edit log

在HDFS中，Namenode可能成为集群的单点故障，Namenode不可用时，整个文件系统是不可用的。HDFS针对单点故障提供了2种解决机制： 
1）**备份持久化元数据** 
将文件系统的元数据同时写到多个文件系统， 例如同时将元数据写到本地文件系统及NFS。这些备份操作都是同步的、原子的。

2）**Secondary Namenode** 
Secondary节点定期合并主Namenode的namespace image和edit log， 避免edit log过大，通过创建检查点checkpoint来合并。它会维护一个合并后的namespace image副本， 可用于在Namenode完全崩溃时恢复数据。

Secondary Namenode通常运行在另一台机器，因为合并操作需要耗费大量的CPU和内存。其数据落后于Namenode，因此当Namenode完全崩溃时，会出现数据丢失。 通常做法是拷贝NFS中的备份元数据到Second，将其作为新的主Namenode。 
在HA（High Availability高可用性）中可以运行一个Hot Standby，作为热备份，在Active Namenode故障之后，替代原有Namenode成为Active Namenode。

#### 1.3 **SecondaryNameNode工作原理**

[别扯了，Secondary NameNode工作原理就看这家](https://blog.csdn.net/u010848845/article/details/118491365)

![img](https://img-blog.csdnimg.cn/20210705153740561.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTA4NDg4NDU=,size_16,color_FFFFFF,t_70)

1 ）**第一阶段： NameNode 启动**

（ 1 ）第一次启动 NameNode 格式化后，创建 Fsimage 和 Edits 文件。如果不是第一次启动，直接加载编辑日志和镜像文件到内存。
（ 2 ）客户端对元数据进行增删改的请求。
（ 3 ） NameNode 记录操作日志，更新滚动日志。
（ 4 ） NameNode 在内存中对元数据进行增删改。

2 ）**第二阶段： Secondary NameNode 工作**

（ 1 ） Secondary NameNode 询问 NameNode 是否需要 CheckPoint 。直接带回 NameNode
是否检查结果。
（ 2 ） Secondary NameNode 请求执行 CheckPoint 。
（ 3 ） NameNode 滚动正在写的 Edits 日志。
（ 4 ）将滚动前的编辑日志和镜像文件拷贝到 Secondary NameNode 。
（ 5 ） Secondary NameNode 加载编辑日志和镜像文件到内存，并合并。
（ 6 ）生成新的镜像文件 fsimage.chkpoint 。
（ 7 ）拷贝 fsimage.chkpoint 到 NameNode 。
（ 8 ） NameNode 将 fsimage.chkpoint 重新命名成 fsimage 。

### 1.4 DataNode

数据节点负责存储和提取Block，读写请求可能来自nameNode，也可能直接来自客户端。数据节点周期性向NameNode汇报自己节点上所有存储的BLock相关信息。



### 1.5 HDFS 设计目标

- 存储非常大的文件
- 采用流式的数据访问方式

**不适合的应用类型：**

- 低延时的数据访问
  - 对延时要求在毫秒级别的应用，不适合采用HDFS。
- 大量小文件
  - 文件的元数据（如目录结构，文件block的节点列表，block-node mapping）保存在NameNode的内存中，整个文件系统的文件数量会受限于NameNode的内存大小。
- 多方读写，需要任意的文件修改
  - HDFS采用追加的方法写入数据，不支持文件任意offset修改。不支持多个写入器。

### 1.6 HDFS的文件存储格式

[HDFS的文件存储格式](https://www.cnblogs.com/wqbin/p/14635480.html)

可分为**行式存储**和**列式存储**两大类。

![img](https://upload-images.jianshu.io/upload_images/6450093-0c5b3f7a2eceaaef.jpg)

#### 行式存储

同一行的数据存储在一起，即连续存储。例如：`SequenceFile`,  `MapFile`, `Avro`, `Datafile`等格式都是使用行式存储的。

如果只需要访问行的一小部分列数据，也需要将整行的数据读入内存。举个例子：一行中有十列的数据，取数的时候只需要取两列的数据，那么就需要把整个行中的所有数据都需要读取出来。

- **SequenceFile**
- **MapFile**
- **Avro**
- **DataFile**

#### 列式存储

整个文件被切割为若干列数据，每一列数据一起存储。`Parquet`, `RCFile`, `ORCFile`.面对列式存储的数据，可以跳过不需要的列，适合于只处理行的一小部分字段的情况。但是这种格式的读写需要更多的内存空间，因为需要缓存行在内存中（为了获取多行中的某一列）。

同时不适合流失写入，因为一旦写入失败，当前文件无法恢复，而面对行的数据在写入失败时，可以重新同步到最后一个同步点。

- **Parquet**
- **RCFile**
- **ORCFile**



![img](https://upload-images.jianshu.io/upload_images/6450093-dbe2595ee1e293b1.png)





一般情况下，离线处理中的宽表会有很多的字段，而在进行分析的时候，只需要一小部分字段即可，所以实际生产中，列式存储的情况比较多。



**Parquet与ORC的对比**

![image](https://imgconvert.csdnimg.cn/aHR0cHM6Ly95cWZpbGUuYWxpY2RuLmNvbS9lOGI3ODEzNzIyMGM4OTUyOGVjZDA0NDY0NjJiZDI3Y2FmNGRmNTRkLnBuZw?x-oss-process=image/format,png)

![image](https://imgconvert.csdnimg.cn/aHR0cHM6Ly95cWZpbGUuYWxpY2RuLmNvbS9iZTU0N2YxNGY3YmZhZGZlNGQyMjJkOTFhNDIyMTk4NmJkNzU3ZDk2LnBuZw?x-oss-process=image/format,png)





小总结：ORC的压缩能力强，支持ACID，支持更新，删除等操作。但是嵌套式结构实现比较复杂。



### 1.7 HDFS的读写流程

#### 1.7.1 HDFS 的 写流程

![img](https://www.pianshen.com/images/782/2d5555fa9bb1b9bda4614b97abcc7d0e.png)

客户端发起写请求到NameNode，NameNode返回可用的资源，客户端根据资源使用情况对要写如的数据分块，逐一上传块到DataNode，DataNode获取上传块数据并写入磁盘，完成后报告给NameNode块信息，同时也告诉客户端写入成功，客户端继续后续块的写入，在此期间NameNode接受到DataNode块写入完成信息之后备份数直到满。

1. 首先客户端发起写请求到NameNode，NameNode检查目录是否存在，父目录是否存在。
2. NameNode通知客户端是否可以上传
3. client长传时，先对文件进行分块，默认block为128M。client向NameNode请求第一个block需要传输到哪个DataNode上。
4. NameNode接受到请求，返回可用的DataNode。假设备份副本数为3，那么就返回三个可用的DataNode。（client同机器d1,同机架的另一台服务器的d2， 不同机架的另一台服务器的d3）
5. client请求一台DataNode建立block传输管道，第一个datanode接受到请求后会继续调用第二个datanode，然后第二个datanode调用第三个datanode，将整个pipeline建立完成，逐级返回客户端（这个过程是串联的）
6. 三个datanode逐级应答客户端。
7. 客户端开始往d1节点上传第一个block，然后上传到d2，接下来是d3
8. 当第一个block传输完后，客户端再次请求namenode上传第二个接收的block的datanode节点，直到最后一个block上传完成为止。

#### 1.7.2 HDFS 的 读流程

![img](https://www.pianshen.com/images/955/28906ddfe55a52f88180366de7c6b3bb.png)

客户端发起读请求到NameNode，NameNode返回可使用的DataNode，客户端根据返回的资源到对应的DataNode上读取块数据，客户端合并文件数据。

1. client和namenode通信查询元数据（block所在的datanode节点），找到所在的datanode服务器
2. 挑选一台datanode（就近原则，然后随机）服务器请求建立socket流
3. datanode发送数据，从磁盘读取数据放入流，以packet为单位来做校验。
4. 客户端以packet为单位接收，先在本地缓存，然后写入目标文件，最后合并文件。







## 2. 分布式计算框架MapReduce



### 2.1 MapReduce工作流程

#### 2.1.1 MapTask工作机制

![img](https://img2020.cnblogs.com/blog/1748663/202007/1748663-20200726181803473-2052825806.png)



1. MapTask收集Mapper中的map()方法每次输出的key-value值，放入到环形缓冲区中。

   环形缓冲区默认大小100M

   环形缓冲区双向写入，一侧记录索引值，一侧记录真是的数据

2. 环形缓冲区中的数据达到80%的时候，开始进行反向溢写

3. 从缓冲区溢写出来的数据会根据**分区器**进行分区，且每个分区内，会通过**快速排序**，对key排序，保证每个分区中的数据是有序的。

4. 接下来将缓冲区本次溢写出来的且分区内有序的数据落盘（多临时小文件），待数据都处理完后，多个溢出文件会被合并成大的溢出文件（这个过程通过**归并排序**，使得这个大的溢出文件内部也是有序的）。

5. 如果MapTask开启了**Combiner**预聚合功能，那么在缓冲区溢出数据分区排序完之后，每个分区内会做一次预聚合的操作，将相同key的记录按照一定的规则进行聚合，然后落盘，合并。（开启预聚合功能可在一定程度上缓解数据倾斜带来的问题）

6. 每个MapTask所在机器上都会输出对应的Map阶段的结果。



#### 2.1.2 ReduceTask工作机制

![img](https://img2020.cnblogs.com/blog/1748663/202007/1748663-20200726181824985-212928464.png)



1. 每个**ReduceTask**从上阶段的各个MapTask所在机器上拷贝**当前ReduceTask负责的分区数据**到自己的缓冲区中（如果数据超过缓存区大小，则写到磁盘上）
2. 对于来自多个MapTask上的数据进行**归并排序**，合并成一个文件，将具有相同key的数据排列在一起，这样就实现了按照key进行分组，也可称之为局部排序
3. 每组数据经过reduce()方法进行处理
4. 最终将计算结果写到HDFS上。



### 2.2 Shuffle机制

![img](https://img2020.cnblogs.com/blog/1748663/202007/1748663-20200726181459479-554913934.png)

MapReduce整个Shuffle阶段横跨了MapTask和ReduceTask这两个任务阶段。





## 3. 集群资源管理器Yarn

### Yarn的基本架构

![img](https://img2020.cnblogs.com/blog/1748663/202007/1748663-20200726183159871-354972654.png)

Yarn主要有ResourceManager、NodeManager、ApplicationMaster和Container等组件

1. ResourceManager
   - 处理客户端请求
   - 监控NodeManager
   - 启动或监控ApplicationMaster
   - 资源的分配与调度
2. NodeManager
   - 管理单个节点上的资源
   - 处理来自ResourceManager的命令
   - 处理来自ApplicationMaster的命令
3. ApplicationMaster
   - 负责数据的切分
   - 为应用程序申请资源并分配给内部的任务
   - 任务的监控与容错
4. Container
   - Container是Yarn中的资源抽象，它封装了某个节点上的多维资源，如内存，CPU，磁盘，网络等。



### Yarn工作机制

![img](https://img2020.cnblogs.com/blog/1748663/202007/1748663-20200726183647821-1922505507.png)

1. 作业提交
   1. Client调用job.waitForCompletion方法，向整个集群提交MapReduce作业。
   2. Client回向RM申请一个Application
   3. RM给Client返回该job资源的提交和作业id
   4. Client提交jar包，切片信息和配置文件到指定的资源提交路径。
   5. Client提交完资源后，向RM申请运行ApplicationMaster。ApplicationMaster运行在刚刚申请的Container中。
2. 作业初始化
   6. 当RM收到Client的请求后，将请求封装成task，将该job添加到容量调度器中
   7. 某个空闲的NM领取到该Job，创建Container，并产生ApplicationMaster
   8. 下载Client提交的资源到NM本地
3. 任务分配
   9. ApplicationMaster向RM申请运行多个MapTask任务资源。
   10. RM将运行MapTask任务分配给另外两个NodeManager，另外两个NodeManager分别领取到任务并创建容器。
4. 任务运行
   11. MR向接收到任务的NodeManager发送程序启动脚本，这两个NodeManager分别启动MapTask，MapTask对数据分区排序。
   12. ApplicationMaster等待所有MapTask运行完毕后，向RM申请容器，运行ReduceTask
   13. ReduceTask向MapTask获取相应分区的数据。
   14. 程序运行完毕后，MR会向RM申请注销自己。
5. 进度和状态更新
   15. Yarn中的任务将其进度和状态（包括counter）返回给应用管理器，客户端每秒向应用管理器请求进度更新，展示给用户。
6. 作业完成
   16. 除了向应用管理器请求作业进度外, 客户端每5秒都会通过调用waitForCompletion()来检查作业是否完成。时间间隔可以通过mapreduce.client.completion.pollinterval来设置。作业完成之后, 应用管理器和Container会清理工作状态。作业的信息会被作业历史服务器存储以备之后用户核查。

## 4. 补充

### 4.1 Hadoop中的常用端口号



| 服务 | 节点名            | 默认端口 | 配置                                | 说明                                      |      |
| ---- | ----------------- | -------- | ----------------------------------- | ----------------------------------------- | ---- |
| HDFS | NameNode          | 50070    | dfs.namenode.http-address           | NameNode Web UI端口                       |      |
| HDFS | NameNode          | 8020     | fs.defaultFS                        | NameNode API连接默认端口                  |      |
| HDFS | NameNode          | 9870     | fs.defaultFS                        | 3.0版本后8020==>9870                      |      |
| HDFS | DataNode          | 50010    | dfs.datanode.address                | DAataNode初始化时向NameNode发送心跳       |      |
| HDFS | DataNode          | 50020    | dfs.datanode.ipc.address            | DataNode ipc服务器地址和端口              |      |
| HDFS | DataNode          | 50075    | dfs.datanode.http.address           | DataNode http服务器地址和端口             |      |
| HDFS | DataNode          | 50475    | dfs.datanode.https.address          | DataNode https服务器地址和端口            |      |
| Yarn | ResourceManager   | 8088     | yarn.resourcemanager.webapp.address | RM Web应用程序的http地址和端口            |      |
| YARN | JobHistory Server | 19888    | mapreduce.jobhistory.webapp.address | MapReduce JobHistory服务器WebUI的IP和端口 |      |



# Spark复习



## 1. Spark Core

### 1.1 转换算子

1. `map()`
   - 一进一出，每来一个元素执行一次map中的逻辑
2. `mapPartitions()`
   - 每个分区调用一次，会把每个分区中的元素包装成Iterator
3. `mapPartitionsWithIndex()`
   - 带有分区信息的`mapPartitions()`
4. `flatMap()`
   - 功能与 map类似，但是可以将集合进行 扁平化，可实现一进多出的或者不出的map
5. `glom()`
   - 将每个分区内的元素合并成一个数组
6. `groupBy(func)`
   - 按照func返回的值进行分组，将对应的值放入一个迭代器中Iterable；
7. `filter(func)`
   - 根据func返回的布尔值进行过滤
8. `sample(withReplacement, fraction, seed)`
   - 以指定的随机种子抽样出比例为`fraction`的数据（抽取到的数是`size*fraction`）, 需要注意的是得到的结果并不能保证准确的比例。
   - `withReplacement`表示时抽出的数据是否放回，（`true  or  false`）
   - `seed`用于指定随机数生成器 种子。一般用默认的，或者传入的当前的时间戳。

9. `distinct()`

   -  对RDD中的元素执行去重操作。

10. `coalesce(numPartitions)`

    - 缩减分区数到执行的数量，用哦关于大数据集过滤后，提高小数据集的执行效率

11. `replacePartition(numPartitions)`

    - 重新分区，底层调用coalesce，但是会指定shuffle，而coalesce默认不会进行shuflle。

12. `sortBy(func,[ascending], [numTasks])`

    - 使用func先对数据进行处理，按照处理后的结果进行排序，默认为升序。

    - ```scala
      rdd1.sortBy(x=>x, false) # 按照x进行降序排序。
      ```

13. `pipe(command)`

    - 把RDD中的每个元素通过管道的方式传递给shell脚本或命令。一个分区执行一次这个命令。
    - `rdd1.pipe("/pipe")`

14. 双Value型转换算则

    1. 并集`union`

       - rdd1.union(rdd2)

    2. 差集`subtract(otherDataSet)`

    3. 交集`intersection(otherDataset)`

    4. 笛卡尔积`cartesian(otherDataset)`

    5. 拉链`zip(otherDataSet)`

       - 拉链操作，需要注意的是，在Spark中，两个RDD的元素的数量和分区数都必须相同，否则会抛出异常，其实质上就是要求每个分区的元素的数量相同。

       - ```scala
         scala> val rdd1 = sc.parallelize(1 to 5)
         rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[34] at parallelize at <console>:24
         
         scala> val rdd2 = sc.parallelize(11 to 15)
         rdd2: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[35] at parallelize at <console>:24
         
         scala> rdd1.zip(rdd2).collect
         res17: Array[(Int, Int)] = Array((1,11), (2,12), (3,13), (4,14), (5,15))
         
         ```

**key-value型转换算子**

1. `reduceByKey(func, [numTasks])`
   - 按照key进行聚合运算
2. `groupByKey()`
   - 按照key进行分组

`reduceByKey 与 groupByKey`的区别

- reduceByKey按照key进行聚合，在shuffle之前有combine（预聚合的操作），结果是RDD[K, V]
- groupByKey按照key进行分组，直接进行shuffle，没有预聚合的操作。

3. `foldByKey`(默认为left)
   - 可以指定初始值的聚合操作
   - 返回值的RDD类型与初始值类型保持一致

reduceByKey与foldByKey的聚合逻辑（分区内的聚合逻辑和分区间的聚合逻辑都是一致的）

4. `aggregateByKey(zero)(seqOP, combOp,[numTasks])`

   - 可以指定初始值，指定分区内的聚合逻辑和分区间的聚合逻辑
   - **但是初始值还是需要人为来指定，**

5. `combineByKey`

   - 既可以动态指定零值，还可以指定分区内的聚合逻辑和分区间的聚合逻辑。

   - ```scala
       /**
        * Simplified version of combineByKeyWithClassTag that hash-partitions the resulting RDD using the
        * existing partitioner/parallelism level. This method is here for backward compatibility. It
        * does not provide combiner classtag information to the shuffle.
        *
        * @see [[combineByKeyWithClassTag]]
        */
       def combineByKey[C](
           createCombiner: V => C,
           mergeValue: (C, V) => C,
           mergeCombiners: (C, C) => C): RDD[(K, C)] = self.withScope {
         combineByKeyWithClassTag(createCombiner, mergeValue, mergeCombiners)(null)
       }
     ```

   - 一个函数是 根据第一个 元素的value的值进行初始值的设定；

   - 一个函数是，定义分区内元素聚合逻辑；

   - 一个函数是，定义分区间元素聚合逻辑。

6. `sortByKey()`
   - 按照key值进行排序
7. `cogroup`算子
   - cogroup算子 操作两个Key-Value形式的RDD， 最后将两个RDD合并成一个RDD，这个RDD的形式：RDD[(K, (Iterable[V], Iterable[W]))]

8. `join`

   ```scala
   import org.apache.spark.rdd.RDD
   import org.apache.spark.{SparkConf, SparkContext}
   
   object Join {
   	def main(args: Array[String]): Unit = {
   		val conf = new SparkConf().setMaster("local[*]").setAppName("AggregateByKey")
   		val sc = new SparkContext(conf)
   		val rdd1: RDD[(Int, String)] = sc.parallelize(Array((1, "a"), (1, "b"), (2, "c"), (4, "d")))
   		val rdd2: RDD[(Int, String)] = sc.parallelize(Array((1, "aa"), (1, "dd"), (3, "bb"), (2, "cc")))
   		//TODO 1.内连接
   		val rdd_join: RDD[(Int, (String, String))] = rdd1.join(rdd2)
   		//TODO 2.左外连接
   		val rdd_leftOuterJoin: RDD[(Int, (String, Option[String]))] = rdd1.leftOuterJoin(rdd2)
   		//TODO 3.右外连接
   		val rdd_rightOuterJoin: RDD[(Int, (Option[String], String))] = rdd1.rightOuterJoin(rdd2)
   		//TODO 4.全外连接
   		val rdd_fullOuterJoin: RDD[(Int, (Option[String], Option[String]))] = rdd1.fullOuterJoin(rdd2)
   		println(rdd_join.collect().toList)
   		println(rdd_leftOuterJoin.collect().toList)
   		println(rdd_rightOuterJoin.collect().toList)
   		println(rdd_fullOuterJoin.collect().toList)
   	}
   }
   结果：
   内连接：
   List((1,(a,aa)), (1,(a,dd)), (1,(b,aa)), (1,(b,dd)), (2,(c,cc)))
   左外连接：
   List((1,(a,Some(aa))), (1,(a,Some(dd))), (1,(b,Some(aa))), (1,(b,Some(dd))), (2,(c,Some(cc))), (4,(d,None)))
   右外连接：
   List((1,(Some(a),aa)), (1,(Some(a),dd)), (1,(Some(b),aa)), (1,(Some(b),dd)), (2,(Some(c),cc)), (3,(None,bb)))
   外连接（全外连接）：
   List((1,(Some(a),Some(aa))), (1,(Some(a),Some(dd))), (1,(Some(b),Some(aa))), (1,(Some(b),Some(dd))), (2,(Some(c),Some(cc))), (3,(None,Some(bb))), (4,(Some(d),None)))
   ```


### 1.2 行动算子

1. `collect()`
   - 以数组的形式返回RDD中所有的元素
2. `count()`
   - n返回RDD中元素的个数
3. `take(n)`
   - 返回RDD中前n个元素组成的数组
4. `first`
   - 返回RDD中的第一个元素，类似于`take(1)`
5. `takeOrder(n, [ordering])`
   - 返回排序后的前n个元素，默认是升序排列
6. `foreach`与`foreachPartition`
   - foreach用一般用于与外部存储进行通讯， 这里的foreach与scala中的foreach函数是不同的，Spark中的foreach是在Executor中进行遍历的， 而不是Driver端。
7. `countByKey()`
   - 统计每个key 的个数，底层是将key转换成(key,1)的形式
8. `reduce(func)`
   - 通过func函数聚集RDD中的所有元素，先聚集分区内数据，再聚合分区间数据。
9. `fold(zero)(func)`
10. `aggregate(zero)(分区内逻辑，分区间逻辑)`
    - 行动算子aggregate与转换算子aggregateByKey最大的区别在于，aggregate中的零值参与计算的次数不同，分区内会参与一次，分区间也会参与一次。
11. 各种saveAs...



## 2. RDD的依赖关系

1. 窄依赖
2. 宽依赖

### 1. 窄依赖

![img](https://img2.baidu.com/it/u=1912645117,1546810615&fm=15&fmt=auto&gp=0.jpg)

父RDD中的一个分区，至多只有一个子RDD的分区使用。

### 2. 宽依赖

![img](https://img1.baidu.com/it/u=3492215315,2225793381&fm=26&fmt=auto&gp=0.jpg)

父RDD中的一个分区，被子RDD的多个分区使用，这种依赖关系称为宽依赖。

会引起宽依赖的算子:`groupByKey`, `reduceByKey`, `join`,  `sortByKey`



### 3. Spark Job的划分

Spark的应用程序都是懒加载的，每调用一个action算子之后，调度器就创建一个执行图和启动一个Spark Job，每个job由多个stage组成。每个stage由多个tasks组成。而task就表示每个并行计算，并且会在多个执行器上执行。

#### job

Spark job 处于 Spark 执行层级结构中的最高层. 每个 Spark job 对应一个 action, 每个 action 被 Spark 应用中的驱动所程序调用.

可以把 Action 理解成把数据从 RDD 的数据带到其他存储系统的组件(通常是带到驱动程序所在的位置或者写到稳定的存储系统中)

只要一个 action 被调用, Spark 就不会再向这个 job 增加新的东西.

#### stages

从整体来看, 一个 stage 可以任务是“计算(task)”的集合, 这些每个“计算”在各自的 Executor 中进行运算, 而不需要同其他的执行器或者驱动进行网络通讯. 换句话说, 当任何两个 workers 之间开始需要网络通讯的时候, 这时候一个新的 stage 就产生了, 例如: shuffle 的时候.

这些创建 stage 边界的依赖称为 *ShuffleDependencies*. shuffle 是由宽依赖所引起的, 比如: sort, groupBy, 因为他们需要在分区中重新分发数据. 那些窄依赖的转换会被分到同一个 stage 中.

#### Tasks

stage 由 tasks 组成. 在执行层级中, task 是最小的执行单位. 每一个 task 表现为一个本地计算.

**一个 stage 中的所有 tasks 会对不同的数据执行相同的代码.(程序代码一样, 只是作用在了不同的数据上)**

一个 task 不能被多个执行器来执行, 但是, 每个执行器会动态的分配多个 slots 来执行 tasks, 并且在整个生命周期内会并行的运行多个 task. 每个 stage 的 task 的数量对应着分区的数量, 即每个 Partition 都被分配一个 Task .

![image-20210808112547253](source/2021年8月份秋招复习笔记/image-20210808112547253.png)

在大多数情况下，每个stage的所有task在下一个stage开启之前必须全部完成。



### 4. RDD的持久化

没碰到一个Action就会产生一个job，每个job开始计算的时候，总是从这个job最开始的RDD开始计算。

每个job总是从它血缘的开始开始计算，难免会有计算过程重复执行的情况。比如（中间过程产生了一些列的RDD，最终有两个action的时候，这两个action依赖同一个RDD，这样每次执行action的时候，就会重复计算）

如果整个RDD依赖的DAG图的血缘关系过长，就很可能出现分区数据损坏或丢失，则又要从头开始计算来达到容错的目的。

<font color=red>每个 job 都会重新进行计算, 在有些情况下是没有必要, 如何解决这个问题呢?</font>

Spark 一个重要能力就是可以持久化数据集在内存中. 当我们持久化一个 RDD 时, 每个节点都会存储他在内存中计算的那些分区, 然后在其他的 action 中可以重用这些数据. 这个特性会让将来的 action 计算起来更快(通常块 10 倍). 对于迭代算法和快速交互式查询来说, 缓存(Caching)是一个关键工具.

可以使用方法`persist()`或者`cache()`来持久化一个 RDD. 在第一个 action 会计算这个 RDD, 然后把结果的存储到他的节点的内存中. Spark 的 Cache 也是容错: 如果 RDD 的任何一个分区的数据丢失了, Spark 会自动的重新计算.

RDD 的各个 Partition 是相对独立的, 因此只需要计算丢失的部分即可, 并不需要重算全部 Partition

另外, 允许我们对持久化的 RDD 使用不同的存储级别.

例如: 可以存在磁盘上, 存储在内存中(堆内存中), 跨节点做复本.

可以给`persist()`来传递存储级别. `cache()`方法是使用默认存储级别(`StorageLevel.MEMORY_ONLY`)的简写方法.

**RDD的持久化级别**

| Storage Level                           | Meaning                                                      |
| --------------------------------------- | ------------------------------------------------------------ |
| MEMORY_ONLY                             | Store RDD as  deserialized Java objects in the JVM. If the RDD does not fit in memory, some  partitions will not be cached and will be recomputed on the fly each time  they’re needed. This is the default level. |
| MEMORY_AND_DISK                         | Store RDD as  deserialized Java objects in the JVM. If the RDD does not fit in memory,  store the partitions that don’t fit on disk, and read them from there when  they’re needed. |
| MEMORY_ONLY_SER  (Java and Scala)       | Store RDD as *serialized* Java objects (one byte  array per partition). This is generally more space-efficient than  deserialized objects, especially when using a [fast serializer](http://spark.apache.org/docs/2.1.1/tuning.html),  but more CPU-intensive to read. |
| MEMORY_AND_DISK_SER  (Java and Scala)   | Similar to  MEMORY_ONLY_SER, but spill partitions that don’t fit in memory to disk  instead of recomputing them on the fly each time they’re needed. |
| DISK_ONLY                               | Store the RDD  partitions only on disk.                      |
| MEMORY_ONLY_2,  MEMORY_AND_DISK_2, etc. | Same as the  levels above, but replicate each partition on two cluster nodes. |
| OFF_HEAP  (experimental)                | Similar to  MEMORY_ONLY_SER, but store the data in [off-heap   memory](http://spark.apache.org/docs/2.1.1/configuration.html#memory-management). This requires off-heap memory to be enabled. |

有一点需要说明的是, 即使我们不手动设置持久化, Spark 也会自动的对一些 shuffle 操作的中间数据做持久化操作(比如: reduceByKey). 这样做的目的是为了当一个节点 shuffle 失败了避免重新计算整个输入. 当时, 在实际使用的时候, 如果想重用数据, 仍然建议调用persist 或 cache

#### 检查点checkpoint

Spark 中对于数据的保存除了持久化操作之外，还提供了一种检查点的机制,检查点（本质是通过将RDD写入Disk做检查点）是为了通过 Lineage 做容错的辅助

Lineage 过长会造成容错成本过高，这样就不如在中间阶段做检查点容错，如果之后有节点出现问题而丢失分区，从做检查点的 RDD 开始重做 Lineage，就会减少开销。

检查点通过将数据写入到 HDFS 文件系统实现了 RDD 的检查点功能。

为当前 RDD 设置检查点。该函数将会创建一个二进制的文件，并存储到 checkpoint 目录中，该目录是用 SparkContext.setCheckpointDir()设置的。在 checkpoint 的过程中，该RDD 的所有依赖于父 RDD中 的信息将全部被移除。

对 RDD 进行 checkpoint 操作并不会马上被执行，必须执行 Action 操作才能触发, 在触发的时候需要对这个 RDD 重新计算.

#### 持久化与checkpoint的区别

1. 持久化只是将数据保存在BlockManager中，而RDD的Lineage是不变的。但是`checkpoint`执行完后，RDD已经没有之前所谓的依赖RDD了，而只有一个强行为其设置的`checkpoint`，RDD的Lineage改变了。
2. 持久化的数据丢失可能性更大，磁盘，内存都有可能存在数据丢失的情况。但是`checkpoint`的数据通常是存储在如HDFS等容错、高可用的文件系统，数据丢失的可能性较小。
3.  **注意:** 默认情况下，如果某个 RDD 没有持久化，但是设置了checkpoint，会存在问题. 本来这个 job 都执行结束了，但是由于中间 RDD 没有持久化，checkpoint job 想要将 RDD 的数据写入外部文件系统的话，需要全部重新计算一次，再将计算出来的 RDD 数据 checkpoint到外部文件系统。 所以，**建议对 checkpoint()的 RDD 使用持久化, 这样 RDD 只需要计算一次就可以了**.

## 3. Key-Value类型RDD的数据分区器

### 3.1 HashPartitioner

对于给定的key，计算key的hashCode，并除以分区的个数取余，最后返回值就是这个key所属的分区的ID

**HashPartitioner的弊端**： 可能导致每个分区中的数据量不均匀。容易出现数据倾斜。



### 3.2 RangePartitioner范围分区器

是将一定范围内的数据映射到某一个分区内，尽量保证每个分区中数据量的均匀，而且分区与分区之间是有序的，一个分区中的元素肯定都是比另一个分区内的元素小或者大。但是分区内的元素是不能保证顺序的。简单的说就是将一定范围内的数映射到某一个分区内。实现过程为：

1. 先从整个RDD中抽取样本数据（每个分区都要进行抽样），将样本数据排序，计算出每个分区的最大key值，形成一个Array[KEY]类型的数组变量rangeBounds（边界数组）。
2. 判断key在rangeBounds中所处的范围，给出该key值在下一个RDD中的分区id的下标；该分区器要求RDD中的KEY类型必须是可以排序的。

**范围分区器的核心**：

- 蓄水池抽样算法
- 边界数组
- 分区号的计算



**蓄水池抽样算法：**

给定一个数据流，数据流长度N很大，且N知道处理完所有数据之前都不可知，请问如何在只遍历一遍数据（O(N)）的情况下，能够随机选取出m个不重复的数据。

核心代码：

```java
int[] reservoir = new int[m];
//init, 先读取前m个数据
for(int i=0;i<m;i++){
    reservoir[i] = dataStream[i];
}
// 接下来处理剩余数据
for(int i=m;i<dataStream.length;i++){
    // 先获取一个[0,i]内的随机整数
    int d = rand.nextInt(i+1);
    //如果随机整数落在了[0, m-1]范围内，则替换蓄水池中的元素
    if(d<m){
        reservoir[d] = dataStream[i];
    }
}
```



**边界数组RangeBounds**

![在这里插入图片描述](source/2021年8月份秋招复习笔记/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3Mjc2ODM1,size_16,color_FFFFFF,t_70#pic_center)



假设一个数组中有4个元素，且有序。例如上图中的10,30,50,70。
那么根据这个数组中的4个元素，我们可以划分出5个区间：

> 0：小于10
> 1： 10-30
> 2：30-50
> 3：50-70
> 4：大于70

这样，我们就分出了5个相应的分区。这样的数组我们就叫它为边界数组。
一个边界数组中的元素**必须是有序的**，因此在RangePartitioner范围分区内需要对边界数组进行排序。
其 **边界数组长度+1 = 最后的分区数**



**分区号的计算**

1. 边界数组的长度小于等于128
   - 这个时候，没来一个key，轮询的方式遍历数组中的每个元素，然后确定分区号
2. 边界数组的长度大于128的时候
   - 通过二分查找来快速定位分区号。

## 4. 共享变量

正常情况下，传递给spark算子的（比如map，reduce等）的函数都是在远程的集群节点上执行，函数中用到的所有变量都是独立的拷贝。这些变量被拷贝到集群上的每个节点上 ，这些变量的更改不会传递回驱动程序。

但是Spark提供了两个可以跨task的共享变量：

- 累加器
- 广播变量

#### 累加器

累加器用来对信息进行聚合，通常在向 Spark 传递函数时，比如使用 map() 函数或者用 filter() 传条件时，可以使用驱动器程序中定义的变量，但是集群中运行的每个任务都会得到这些变量的一份新的副本，所以更新这些副本的值不会影响驱动器中的对应变量。

如果我们想实现所有分片处理时更新共享变量的功能，那么累加器可以实现我们想要的效果。

累加器是一种变量, 仅仅支持“add”, 支持并发. 累加器用于去实现计数器或者求和. Spark 内部已经支持数字类型的累加器, 开发者可以添加其他类型的支持.

1. 内置累加器`sc.LongAccumulator`
2. 自定义累加器`继承AccumulatorV2`

#### 广播变量

广播变量是在每个节点上保存一个只读的变量缓存，而不用给每个task来传一个copy。

```scala
scala> val broadcastVar = sc.broadcast(Array(1, 2, 3))
broadcastVar: org.apache.spark.broadcast.Broadcast[Array[Int]] = Broadcast(0)

scala> broadcastVar.value
res0: Array[Int] = Array(1, 2, 3)
```

1. 通过对一个类型T的对象调用SparkContext.broadcast创建出一个Broadcast[T]对象。任何可序列化的类型都可以这么实现。

2. 通过value属性访问该对象的值(在Java中为value()方法)。

3. 变量只会被发到各个节点一次，应作为只读值处理(修改这个值不会影响到别的节点)

## 2. SparkSQL

![在这里插入图片描述](source/2021年8月份秋招复习笔记/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3Mjc2ODM1,size_16,color_FFFFFF,t_70#pic_center-16284015946612)



### RDD， DataFrame和DataSet之间的关系

在SparkSQL中，Spark为我们提供了两个新的抽象，分别是DataFrame和DataSet。

RDD (Spark1.0) —> Dataframe(Spark1.3) —> Dataset(Spark1.6)

同样的数据得到这三种不同的数据结构，经过计算得到的结果相同，但是执行效率和执行方式不同。

#### RDD，DataFrame和DataSet三者的共性

1. RDD、DataFrame、DataSet全都是Spark平台下的分布式弹性数据集，为处理超大型数据提供便利。
2. 三者都是惰性机制，在进行创建、转换，如map方法时，不会立即执行，只有在遇到Action如foreach时，三者才会开始遍历运算。
3. 三者都会根据Spark的内存情况自动缓存运算，这样即使数据量跟大，也不会担心内存溢出。
4. 三者都有partition的概念
5. 三者有许多共同的函数，如map，filter，排序等。
6. 在对Dataframe和DataSet进行操作许多操作都需要`import spark.implicits._`这个包的支持
7. DataFrame和DataSet均可使用模式匹配获取各个字段的值和类型。

#### 三者的区别

**RDD**

1. RDD一般和spark mlib同时使用
2. RDD不支持sparkSql操作

**DataFrame**

1. 与RDD和DataSet不同，DataFrame每一行的类型固定为Row，每一列的值没法直接访问，只有通过解析才能获取各个字段的值。
2. DataFrame与DataSet一般不与spark mlib同时使用
3. DataFrame与DataSet均支持SparkSql的操作，比如select， groupby之类，还能注册临时表/视图，进行sql语句操作
4. DataFrame与DataSet支持一些特别方便的保存方式，比如保存csv，可以带上表头，这样每一列的字段名一目了然

**DataSet**

DataSet和DataFrame拥有完全相同的成员函数，区别只是每一行的数据类型不同。DataFrame其实是DataSet的一个特例。
DataFrame也可以叫DataSet[Row]，每一行的类型都是Row，不解析，每一行究竟有哪些字段，各个字段又是什么类型都无从得知，只能用上面的getAs方法或者模式匹配拿住特定的字段。而DataSet中，每一行是什么类型是不一定的，在自定义了case class之后可以很自由的获得每一行的信息。



### **Spark on Hive 与 Hive on Spark**

- Spark on Hive通过Spark-SQL使用hive语句，操作hive，底层运行的还是spark rdd
- Hive on Spark是把hive查询从mapreduce 的mr (Hadoop计算引擎)操作替换为spark rdd（spark 执行引擎） 操作。



## 3. Spark 内核

### Spark提交任务流程

#### 1. Yarn Cluster 模式

![image-20210808163959670](source/2021年8月份秋招复习笔记/image-20210808163959670-16284120015104.png)



1. 执行`spark-submit`脚本提交任务，实际是启动一个SparkSubmit的JVM进程；
2. SparkSubmit类中的main方法反射调用Client的main方法；
3. Client创建Yarn客户端，然后想Yarn发送执行指令`bin/java ApplicationMaster` 请求启动一个AM；
4. Yarn框架收到指令后，RM会选择一台可用的NM，并在其中启动ApplicationMaster（进程）；
5. `ApplicationMaster进程运行(此时RM还不知道AM启动与否，所以AM需要向RM注册)`，同时会启动一个`Driver子线程`，用于执行用户的作业（执行代码，初始化sc，任务切分）；
6. AM向RM进行注册（证明AM已经启动了），AM还要向RM申请资源`Container`
7. 获取到资源后，AM会向资源所在的NM（获取到的资源，可能在不同的机器上）发送指令`bin/java CoarseGrainedExecutorBackend`,启动一个粗粒度的ExecutorBackend
8. 相应的NodeManager上会启动相应的`ExecutorBackend进程`, 并向`Driver`进行反向注册
9. `Driver`上注册成功后，会向相应的NM返回注册成功信息，然后`ExecutorBackend`进程会创建一个`Executor`对象。
10. Driver内部指定用户提交作业的main方法，初始化sc，并进行任务的切分，然后分配给ExecutorBackend任务，并监控任务的执行。

#### Yarn Client模式

![image-20210808171159136](source/2021年8月份秋招复习笔记/image-20210808171159136-16284139206945.png)

### 

1. 执行spark-submit脚本提交任务，实际启动一个SparkSubmit的JVM进程
2. SparkSubmit伴生对象中的main方法反射调用用户代码（就是我们自己所编写的代码）的main方法。
3. 启动Driver（在SparkSubmit进程的main线程中运行，此时的Driver就不是一个子线程了），执行用户的作业，并创建`ScheduleBackend`与Yarn进行通信。
4. `YarnClientScheduleBackend`向RM发送指令`bin/java ExecutorLauncher`(底层本质就是`ApplicationMaster`)启动ExecutorLauncher进程
5. RM收到指令后会在指定的NM中启动ExecutorLauncher，实质上还是调用的ApplicationMaster的main方法。
6. ExecutorLauncher向AM注册，申请资源
7. 获取资源后ExecutorLauncher向相应的NM发送指令`bin/java CoarseGrainedExecutorBackend`启动一个粗粒度的ExecutorBackend；
8. 后面和cluster模式一致

**注意：**

driver不是一个子线程了，而是直接运行在SparkSubmit进程的main线程中，所以sparkSubmit进程不能退出。

而Cluster模式下，Driver是运行在远程集群上的，SparkSubmit进程提交完作业后即可以关闭。



## 4. Spark任务调度机制

### 4.1 Spark 任务调度概述

在介绍任务调度之前，先来明确一下Spark中几个重要的概念。

- **Job**
  - Job是以Action算子为界限，遇到一个Action算子则触发一个Job
- **Stage**
  - Stage是Job的子集，以RDD宽依赖（即Shuffle）为界限，遇到Shuffle做一次划分
- **Task**
  - Task是Stage的子集，以并行度（分区数）来衡量，这个Stage的分区数有多少，则这个Stage就有多少个Task，每个Stage中的多个Task运行同样的逻辑，但是作用在不同的数据上。



Spark的任务调度总体来说分为两路进行：一路是Stage级别的调度；一路是Task级别的调度。

![image-20210809095832482](source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20210809095832482-8474331.png)

Spark RDD通过其Transactions操作，形成了RDD血缘关系图DAG，最后通过Action的调用，出发Job并调执行。

`DAGScheduler`负责Stage级别的调度，主要是将Job切分成若干`Stages`，并将每个Stage打包成`TaskSet`交给`TaskScheduler`调度。

`TaskScheduler`负责Task级别的调度，将DAGScheduler传过来的`TaskSet`按照指定的调度策略分发到`Executor`上执行，调度过程中`SchedulerBackend`负责提供可用资源，其中`SchedulerBackend`有多种实现，分别对接不同的资源管理系统。

`Driver`初始化`SparkContext`过程中，会分别初始化`DAGScheduler`、`TaskScheduler`、`SchedulerBackend`以及`HeartbeatReceiver`，并启动`SchedulerBackend`以及`HeartbeatReceiver`。



`SchedulerBackend`通过`ApplicationMaster`申请资源，并不断从`TaskScheduler`中拿到合适的Task分发到`Executor`执行。

`HeartBeatReceiver`负责接收`Executor`的心跳信息，监控`Executor`的存活状态，并通知到`TaskScheduler`。



### 4.2 Spark Stage级别的调度

Spark的任务调度是从DAG切割开始，主要是由`DAGScheduler`来完成。当遇到一个Action操作后就会触发一个Job的计算，并交给`DAGScheduler`来提交。

![image-20210809102313917](source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20210809102313917-8475795.png)

1. Job由最终的RDD和Action方法封装而成；
2. `SparkContext`将Job交给`DAGScheduler`提交，它会根据RDD的血缘关系构成的DAG进行切分，将一个Job划分为若干Stages；
   - 具体划分策略是：由最终的RDD不断通过依赖回溯判断父依赖是否是**宽依赖**，即以`Shuffle`为界，划分Stage，窄依赖的RDD之间被划分到同一个Stage中，可以进行pipeline式的计算。
   - **划分的Stages分两类**：
     - 一类叫做`ResultStage`，为DAG最下游的Stage，由Action方法决定；
     - 一类叫做`ShuffleMapStage`，为下游的Stage准备数据。
3. Submit stage 提交阶段
   - `DAGScheduler.handleJobSubmitted`方法创建好`ResultStage`后会提交这个stage（submitStage方法），在提交一个`stage`的时候，会要先提交它的`parent stage`，也是通过递归的形式，直到一个`stage`的所有父阶段-`parent stage`都被提交了，才会提交本阶段，如果一个stage的parent还没有完成，则会把这个stage加入到`waitingStages`。也就是说，DAG图中前面的stage会被先提交。当一个stage的parent都准备好了，也就是执行完毕之后，它才会进入`submitMissingTasks`的环节。
   - stage的划分是<font color=green>从后向前</font>进行划分的，而真正的任务执行是<font color=green>从前向后</font>执行的。

4. Subimit task

   - `Stage`中的`Task`是在`DAGScheduler`（不是TaskScheduler）的`submitMissingTasks`方法中创建的，包括`ShuffleMapTask`和`ResultTask`，与`Stage`对应。归属于同一个stage的这批Task组成一个`TaskSet集合`,最后提交给`TaskScheduler`的就是这个`TaskSet`。

     ![img](source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/webp)

Stage级调度的整体流程：

![DAGScheduler.png](./source/2021年8月份秋招复习笔记/DAGScheduler.png)

<font color=red>注意:</font>

task的创建是在DAGScheduler的submitMissingTasks方法中创建的，而`TaskScheduler`负责Task的调度，不负责Task 的创建。

### 4.3 Spark Task级别的调度

Task的调度是由`TaskScheduler`与`SchedulerBackend`紧密合作，共同完成的。

`TaskScheduler`是task级别的调度器，主要作用是管理task的调度和提交，是Spark底层的调度器。

`SchedulerBackend`是`TaskScheduler`的后端服务，有独立的线程，所有的`Executor`都会注册到`SchedulerBackend`，主要作用是进行资源的分配、将`Task`分配给`Executor`等。

![TaskScheduler.png](./source/2021年8月份秋招复习笔记/TaskScheduler.png)

**TaskSetManager**负责监控管理同一个stage中的Tasks，TaskScheduler就是以TaskSetManager为单元来调度任务。包括任务推断、Task本地性，并对Task进行资源分配。

**具体工作流程**



![TaskScheduler.png](./source/2021年8月份秋招复习笔记/TaskScheduler工作流程.png)

1. `DAGScheduler`(`submitMissingTasks`方法中)调用`TaskScheduler.submitTask()`创建并TaskSet给`TaskScheduler`；

2. `TaskScheduler`拿到`TaskSet`后会创建一个`TaskSetManager`来管理它，并且把`TaskSetManager`添加到**rootPool**调度池中；

3. 调用`SchedulerBackend.revivieOffers()`方法

4. `SchedulerBackend`发送`ReviveOffers`消息给`DriverEndPoint`；

5. `DriverEndPoint`接收到`ReviveOffers`消息后，会调用`makeOffers()方法`创建`WorkerOffer`，并通过`TaskScheduler.resourceOffer()`返回Offer；

6. **TaskScheduler**会从**rootPool**资源调度池中按照特定的调度算法取出一个**TaskSetManager**，逐个给**TaskSet**的**Task**分配**WorkerOffer**，并将其封装成**TaskDescription**（包含Offer信息）

7. 调用`SchedulerBackend.DriverEndPoint`的**launchTasks**方法，将**TaskDescription**序列化并封装在**LaunchTask**消息中，发送给**Offer**指定的**Executor**。

   **LaunchTask**消息被**ExecutorBackend**接收到后，会将**Task**信息反序列化，传给`Executor.launTask()`,最后使用**Executor的线程池**中的线程来执行这个**Task**。

### 4.4 调度策略

TaskScheduler支持两种调度策略：**FIFO**和**FAIR**

- **FIFO**:先进入到rooPool资源池中的TaskSetManager优先被调度

- **FAIR**：公平调度

### 4.5 本地化调度

因为每个Stage中的task负责处理不同分区的数据，所以在task被分配到Executor上时，尽量保证Task与处理的数据保持较近的距离，这样可以避免一定的数据传输开销。

**TaskScheduler**从调度队列中拿到**TaskSetManager**后，那么接下来的工作就是**TaskSetManager**按照一定的规则一个个取出task给**TaskScheduler**，**TaskScheduler**再交给**SchedulerBackend**去发到**Executor**上执行。

**TaskSetManager**会根据每个Task的优先位置，确定`Task的本地化调度级别：Locality`， Locality一共有五种，优先级由高到低顺序：

| 名称          | 名称       | 备注                                                         |
| ------------- | ---------- | ------------------------------------------------------------ |
| PROCESS_LOCAL | 进程本地化 | task和数据在同一个Executor中，性能最好。                     |
| NODE_LOCAL    | 节点本地化 | task和数据在同一个节点中，但是task和数据不在同一个Executor中，数据需要在进程间进行传输 |
| RACK_LOCAL    | 机架本地化 | task和数据在同一个机架的两个不同的节点上，数据需要通过网络在节点之间进行传输。 |
| NO_PREF       |            | 对于task来说，从哪里获取数据都一样，没有好坏之分             |
| ANY           |            | task和数据可以在集群的任何地方，而且不在一个机架中，性能最差 |

在调度执行时，Spark调度总是会尽量让每个task以最高的本地级别来启动，当一个task以本地性级别启动，但是该本地性级别对应的所有节点都没有空闲资源而启动失败，此时并不会立马降低本地性级别启动，而是在某个时间长度内再次以本地性级别来启动该task，若超过限时时间则降级启动，去尝试下一个本地性级别。

可以通过调大每个类别的最大容忍延迟时间，在等待阶段对应的Executor可能就会有相应的资源去执行次task，这就在一定程度上提升了运行性能。

**失败重试和黑名单**

除了选择合适的Task调度运行外，还需要监控Task的执行状态，前面也提到，与外部打交道的是SchedulerBackend，Task被提交到Executor启动执行后，Executor会将执行状态上报给SchedulerBackend，SchedulerBackend则告诉TaskScheduler，TaskScheduler找到该Task对应的TaskSetManager，并通知到该TaskSetManager，这样TaskSetManager就知道Task的失败与成功状态，对于失败的Task，会记录它失败的次数，如果失败次数还没有超过最大重试次数，那么就把它放回待调度的Task池子中，否则整个Application失败。

在记录Task失败次数过程中，会记录它上一次失败所在的Executor Id和Host，这样下次再调度这个Task时，会使用黑名单机制，避免它被调度到上一次失败的节点上，起到一定的容错作用。黑名单记录Task上一次失败所在的Executor Id和Host，以及其对应的“拉黑”时间，

“拉黑”时间是指这段时间内不要再往这个节点上调度这个Task了。





## 5. Spark Shuffle解析

![ShuffleMapStage与ResultStage](source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/format,png.png)



Spark中会根据俄宽依赖来划分Stage。

在划分Stage时，最后一个Stage称为`ResultStage`, 前面所有的Stage被称为`ShuffleMapStage`

**ShuffleMapStage**的结束伴随着shuffle文件的写磁盘（**ShuffleWrite**）

**ResultStage**的开始会先进行shuffle文件的读磁盘（**ShuffleRead**）

（在MapReduce计算框架中，只要发生Shuffle就会伴随着数据落盘，而在Spark中，只有ShuffleMapStage结束时才会伴随着数据落盘）

### 5.1 HashShuffle

在spark-1.6之前默认的shuffle方式是hash，在spark-1.6版本之后使用sort-BaseShuflle，因为HashShuffle存在不足所以就替换了HashShuffle。Spark2.0之后，从源码中完全移除了HashShuffle。

#### 5.1.1 未优化的HashShuffle

<img src="source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/%E6%9C%AA%E4%BC%98%E5%8C%96%E7%9A%84HashShuffle.jpeg" alt="img" style="zoom:80%;" />

如上图所示：假设共有三个ReduceTask在等待MapTask落盘的数据文件。

每个MapTask处理完后的数据，会根据key的hash被分到不同的缓冲区中，进行将数据落盘。

如果采用未优化的HashShuffle，那么每个MapTask都会产生相应ReduceTask数据量的小文件。

**缺点**

1. map任务的中间结果首先存入内存（缓存），然后才写入磁盘，这对于内存的开销很大，当一个节点上的Map任务的输出结果集很大时，很容易导致内存紧张，发生OOM；
2. 生成很多的小文件。假设有M个MapTask，有N个ReduceTask，则会创建M*N个小文件，磁盘I/O将成为性能的瓶颈。

#### 5.1.2 优化后的HashShuffle

<img src="source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/优化的HashShuffle.png" alt="img" style="zoom:60%;" />



优化的HashShuffle过程就是启用合并机制，合并机制就是复用Buffer。在同一个进程中（一个core中），无论是有多少个task（），都只会有相应ReduceTask数量的一份数据（例如上述就是3个）

每一个MapTask所在的进程中，分别写入共同进程中的3分本地文件，上述图中有4个Task，但是只有两个Core，所以总共输出是 `2个Core * 3个分类文件 = 6 个本地小文件`

### 5.2 SortShuffle

#### 5.2.1 普通的SortShuffle

每个进程中只有一块缓冲区

<img src="./source/2021年8月份秋招复习笔记/普通SortShuffle.png" alt="img" style="zoom:80%;" />







在该模式下，数据会先写入一个数据结构，reduceByKey写入Map，一边通过Map局部聚合，一边写入内存。

如果内存中的数据达到**阈值**，就会将内存中的数据结构写入到磁盘，清空内存数据结构。

在**溢写磁盘**前，现根据**key进行排序**，排序过后的数据，会分批写入到磁盘文件中。默认批次为10000条，数据会以每批一万条写入到磁盘文件。写入磁盘文件通过缓冲区溢写的方式，每次溢写都会产生一个磁盘文件，也就是说一个Task过程会产生多个临时文件。

最后在每个Task中，将**所有的临时文件合并**，这就是**merge**过程，此过程将所有临时文件读取出来，一次写入到最终文件。意味着一个**Task**的所有数据都在这一个文件中。同时单独写一份索引文件，标识下游各个Task的数据在文件中的索引，Start offset和End offset。

#### 5.2.2 bypassSortShuffle

每个进程中有多个缓冲区

<img src="./source/2021年8月份秋招复习笔记/byPassSortShuffle.png" alt="img" style="zoom:80%;" />

bypass运行机制的出发条件（<font color=red>必须同时满足</font>）：

1. shuffle map task数量小于`spark.shuffle.sort.bypassMergeThreshold=200`参数的值，默认为200；
2. 不是聚合类的`shuffle`算子（没有预聚合功能的，比如：groupByKey）

该过程的磁盘写机制其实跟未经优化的`HashShuffleManager`是一模一样的，因为多要创建数据量惊人的磁盘文件，只是在最后一个磁盘文件的合并而已。因此少量的最终磁盘文件，也让该机制相对未经优化的HashShuffleManager来说，Shuffle read的性能会更好。（同时也是采用缓冲区溢写的方法落盘小文件，解决了HashShuffle中内存的问题）

而该机制与普通的SortShuffleManager运行机制的不同在于：不会进行排序。也就是说，启动该机制的最大好处在于，shuffle write过程中，不需要进行数据的排序操作，也就节省掉了这部分的性能开



## 6. Spark内存管理

![img](source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/src=http%253A%252F%252Fwx1.sinaimg.cn%252Fmw690%252F63918611gy1fe7btgzmz8j20le0fidhc.jpg&refer=http%253A%252F%252Fwx1.sinaimg.cn&app=2002&size=f9999,10000&q=a80&n=0&g=0n&fmt=jpeg)

Spark将内存从逻辑上区分为堆内内存和堆外内存，称为内存模型（MemoryMode）

- 这里的堆内内存不能与JVM中的Java堆直接画等号，它只是JVM堆内存的一部分，由JVM统一管理。
- 堆外内存则是Spark使用`sun.misc.Unsafe`的API直接在工作节点（Executor）的系统内存中开辟的空间。



内存池：对上述两种内存进行资源管理



**堆外内存**：为了进一步优化内存的使用以及提高Shuffle时排序的效率，Spark引入了堆外（Off-heap）内存，使之可以直接在工作节点的系统内存中开辟空间，存储经过序列化的二进制数据。

堆外内存意味着把 内存对象分配在Java虚拟机以外的内存，这些内存直接受操作系统管理（而不是虚拟机）。这样做的结果就是能保持一个较小的堆，以减少垃圾收集对应用的影响。

利用 JDK Unsafe API，Spark 可以直接操作系统堆外内存，减少了不必要的内存开销，以及频繁的 GC 扫描和回收，提升了处理性能。

堆外内存可以被精确地申请和释放（堆外内存之所以能够被精确的申请和释放，是由于内存的申请和释放不再通过JVM机制，而是直接向操作系统申请，JVM对于内存的清理是无法准确指定时间点的，因此无法实现精确的释放），而且序列化的数据占用的空间可以被精确计算，所以相比堆内内存来说降低了管理的难度，也降低了误差。



### 内存空间分配

#### 1.  静态内存管理

在 Spark1.6之前采用的静态内存管理机制下，存储内存、执行内存和其他内存的大小在 Spark 应用程序运行期间均为固定的，但用户可以在应用程序启动前进行配置.

##### 1.1 堆内内存管理

![img](source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/src=http%253A%252F%252Fupload-images.jianshu.io%252Fupload_images%252F9175374-a2a527f62646d62b.png&refer=http%253A%252F%252Fupload-images.jianshu.io&app=2002&size=f9999,10000&q=a80&n=0&g=0n&fmt=jpeg)

- **Storage内存**（存储内存）：主要用于存储Spark的cache数据，例如RDD的缓存、Broadcast变量，Unroll数据等。（**默认占用系统内存的60%**）
- **Execution内存**（执行内存）：主要用于存放Shuffle、Join、Sort、Aggregation等计算过程中的临时数据。（**默认占用系统内存的20%**）

- **Other**（有时也叫做用户内存）：主要用于存储RDD转换操作所需的数据，例如RDD依赖等信息。（**默认占用系统内存的20%**）
- **预留内存**（Reserved Memory）：系统预留内存，会用来存储Spark内部对象，防止OO

##### 1.2 堆外内存管理

堆外的空间分配较为简单，只有存储内存和执行内存。

可用的执行内存和存储内存占用的空间大小直接由参数 spark.memory.storageFraction 决定，由于堆外内存占用的空间可以被精确计算，所以无需再设定保险区域。

<img src="source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20210809202402983-8511844.png" alt="image-20210809202402983" style="zoom:50%;" />

静态内存管理机制实现起来较为简单，但如果用户不熟悉 Spark 的存储机制，或没有根据具体的数据规模和计算任务或做相应的配置，很容易造成“一半海水，一半火焰”的局面，即存储内存和执行内存中的一方剩余大量的空间，而另一方却早早被占满，不得不淘汰或移出旧的内容以存储新的内容。

由于新的内存管理机制的出现，这种方式目前已经很少有开发者使用，出于兼容旧版本的应用程序的目的，Spark 仍然保留了它的实现。



#### 2. 统一内存管理

Spark 1.6 之后引入的统一内存管理机制，与静态内存管理的区别在于存储内存和执行内存共享同一块空间，可以动态占用对方的空闲区域.

##### 2.1 统一堆内内存管理

![](source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%BB%9F%E4%B8%80%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86.jpeg)

**存储内存**与**执行内存**共占系统内存的**60%**，Other（用户内存占**40%**），预留内存：**300M**

统一内存管理最重要的优化在于**动态占用机制**，其规则如此下：

1. 设定基本的存储内存和执行内存区域`spark.storage.storageFraction`,该设定确定了双方各自拥有的空间的范围。
2. 双方空间都不足时，则存储到硬盘。若己方内存空间不足而对方空余时，可借用对方的空间。
3. 执行内存的空间被存储内存占用后，可让存储内存将占用的部分数据转存到硬盘，然后“归还”借用的空间。
4. 存储内存的空间被执行内存占用后，无法让运行内存“归还”，因为需要考虑Shuffle过程中的诸多因素，实现起来比较复杂。（<font color=red>执行优先，不能执行一半就把内存还回去了，得等此次执行完后才可返还内存</font>）

![img](source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/1228818-20180426212853794-858627420.png)

##### 2.2 统一堆外内存管理

<img src="source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20210809202631142-8511994.png" alt="image-20210809202631142" style="zoom:50%;" />

### 存储内存管理

RDD 作为 Spark 最基本的数据抽象, 是分区记录(partition)的只读集合, 只能基于在稳定物理存储中的数据集上创建, 或者在其他已有的 RDD 上执行转换(Transformation)操作产生一个新的 RDD.

转换后的 RDD 与原始的 RDD 之间产生的依赖关系, 构成了血统(Lineage). 凭借血统, Spark 可以保证每一个 RDD 都可以被重新恢复.

但 RDD 的所有转换都是惰性的, 即只有当行动(Action)发生时, Spark 才会创建任务读取 RDD, 然后才会真正的执行转换操作.

Task 在启动之初读取一个分区的时, 会先判断这个分区是否已经被持久化, 如果没有则需要检查 Checkpoint 或按照血统重新计算.

如果要在一个 RDD 上执行多次行动, 可以在第一次行动中使用 persis 或 cache 方法, 在内存或磁盘中持久化或缓存这个 RDD, 从而在后面的Action 时提示计算速度.

事实上, cache 方法是使用默认的 MEMORY_ONLY的存储级别将 RDD 持久化到内存, 所以缓存是一种特殊的持久化.

堆内内存和堆外内存的设计, 便可以对缓存 RDD 时使用的内存做统一的规划和管理。







### 执行内存管理



















## 7. Spark性能优化

### 7.1 Spark 常用配置参数

执行`submit`脚本的时候，可以指定一些配置参数，

例如：

```shell
/usr/local/spark/bin/spark-submit\
--class spark.WordCount \
--num-executor 80 \
...
/usr/locla/spark/spark.jar
```



下面列举几个常用的参数：

| 参数名                | 参数说明                              |
| --------------------- | ------------------------------------- |
| `num-executors`       | 设置executor的数量                    |
| `driver-memory`       | 设置driver端的内存大小                |
| `executor-memory`     | 设置每个executor的内存大小            |
| `executor-cores`      | 设置每个executor的cpu核心数           |
| `master yarn`         | 设置集群模式为Yarn                    |
| `deploy-mode cluster` | 设置yarn的部署模式为cluster（client） |

### 7.2 常规性能调优一：RDD复用

- 在对RDD进行算子时，要避免相同的算子和计算逻辑之下对 RDD 进行重复的计算:

  ![image-20210810095712565](source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20210810095712565-8560634-8561548.png )                               

  对上图中的RDD计算架构进行修改:

  ![image-20210810095721948](source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20210810095721948-8560643-8561543.png)

### 7.2 常规性能调优二：RDD持久化

- - 在Spark中，当多次对同一个 RDD 执行算子操作时，每一次都会对这个 RDD 的祖先 RDD 重新计算一次，这种情况是必须要避免的，对同一个RDD的重复计算是对资源的极大浪费，因此，必须对多次使用的RDD进行持久化，通过持久化将公共RDD的数据缓存到内存/磁盘中，之后对于公共RDD的计算都会从内存/磁盘中直接获取RDD数据。 对于RDD的持久化，有两点需要说明： 
  - RDD的持久化是可以进行序列化的，当内存无法将RDD的数据完整的进行存放的时候，可以考虑使用序列化的方式减小数据体积，将数据完整存储在内存中。
  - 如果对于数据的可靠性要求很高，并且内存充足，可以使用副本机制，对RDD数据进行持久化。当持久化启用了复本机制时，对于持久化的每个数据单元都存储一个副本，放在其他节点上面，由此实现数据的容错，一旦一个副本数据丢失，不需要重新计算，还可以使用另外一个副本。
- **RDD尽可能尽早的filter操作**



- 

### 7.3 常规性能调优三：并行度调节

`set spark.default.parallelism=500;`

```scala
val conf=new SparkConf().set("spark.default.parallelism", "500");
```

<font color=green>一般情况下，task数量应该设置为Spark作业总CPU Core数量的2~3倍。</font>

之所以没有推荐task数量与CPU core总数相等，是因为task的执行时间不同，有的task执行速度快而有的task执行速度慢，如果task数量与CPU core总数相等，那么执行快的task执行完成后，会出现CPU core空闲的情况。如果task数量设置为CPU core总数的2~3倍，那么一个task执行完毕后，CPU core会立刻执行下一个task，降低了资源的浪费，同时提升了Spark作业运行的效率。



### 7.4 常规性能调优四：广播大变量

默认情况下，task 中的算子中如果使用了外部的变量，每个 task 都会获取一份变量的复本，这就造成了内存的极大消耗。 - 一方面，如果后续对 RDD 进行持久化，可能就无法将 RDD 数据存入内存，只能写入磁盘，磁盘IO将会严重消耗性能； - 另一方面，task在创建对象的时候，也许会发现堆内存无法存放新创建的对象，这就会导致频繁的GC，GC会导致工作线程停止，进而导致Spark暂停工作一段时间，严重影响Spark性能。

假设当前任务配置了20个Executor，指定500个task，有一个20M的变量被所有task共用，此时会在500个task中产生500个副本，耗费集群10G的内存，如果使用了广播变量， 那么每个Executor保存一个副本，一共消耗400M内存，内存消耗减少了5倍。

广播变量在每个Executor保存一个副本，此Executor的所有task共用此广播变量，这让变量产生的副本数量大大减少。

在初始阶段，广播变量只在Driver中有一份副本。task在运行的时候，想要使用广播变量中的数据，此时首先会在自己本地的Executor对应的BlockManager中尝试获取变量，如果本地没有，BlockManager就会从Driver或者其他节点的BlockManager上远程拉取变量的复本，并由本地的BlockManager进行管理；之后此Executor的所有task都会直接从本地的BlockManager中获取变量。

### 7.5 算子调优

#### 1. mapPartitions

普通的 map 算子对 RDD 中的每一个元素进行操作，而 mapPartitions 算子对 RDD 中每一个分区进行操作。

如果是普通的map算子，假设一个 partition 有 1 万条数据，那么 map 算子中的 function 要执行1万次，也就是对每个元素进行操作。

如果是 mapPartition 算子，由于一个 task 处理一个 RDD 的partition，那么一个task只会执行一次function，function一次接收所有的partition数据，效率比较高。

mapPartitions算子也存在一些缺点：对于普通的map操作，一次处理一条数据，如果在处理了2000条数据后内存不足，那么可以将已经处理完的2000条数据从内存中垃圾回收掉；但是如果使用mapPartitions算子，但数据量非常大时，function一次处理一个分区的数据，如果一旦内存不足，此时无法回收内存，就可能会OOM，即内存溢出。

因此，mapPartitions算子适用于数据量不是特别大的时候，此时使用mapPartitions算子对性能的提升效果还是不错的。（当数据量很大的时候，一旦使用mapPartitions算子，就会直接OOM） 在项目中，应该首先估算一下RDD的数据量、每个partition的数据量，以及分配给每个Executor的内存资源，如果资源允许，可以考虑使用mapPartitions算子代替map。



#### 2. foreachPartition优化数据库操作

使用了foreachPartition算子后，可以获得以下的性能提升：

1. 对于我们写的function函数，一次处理一整个分区的数据
2. 对于一个分区的数据，创建唯一的数据库连接
3. 只需要向数据发送一次SQL语句和多组参数

在生产环境中，全部都会使用foreachPartition算子完成数据库操作。foreachPartition算子存在一个问题，与mapPartitions算子类似，如果一个分区的数据量特别大，可能会造成OOM，即内存溢出。



#### 3. filter与coalesce的配合使用

<img src="source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20210810102102079-8562063.png" alt="image-20210810102102079" style="zoom:50%;" />

在Spark任务中我们经常会使用filter算子完成RDD中数据的过滤，在任务初始阶段，从各个分区中加载到的数据量是相近的，但是一旦进过filter过滤后，每个分区的数据量有可能会存在较大差异。

在上图中, 第二个分区的数据过滤后只剩100条，而第三个分区的数据过滤后剩下800条，在相同的处理逻辑下，第二个分区对应的task处理的数据量与第三个分区对应的task处理的数据量差距达到了8倍，这也会导致运行速度可能存在数倍的差距，这也就是**数据倾斜问题**。

可以通过repartition与coalesce都可以进行重分区，其中repartition只是coalesce接口中shuffle为true的简易实现，coalesce默认情况下不进行shuffle。

> 可以在filter操作之后，使用coalesce算子针对每个partition的数据量各不相同的情况，压缩partition的数量，而且让每个partition的数据量尽量均匀紧凑，以便于后面的task进行计算操作，在某种程度上能够在一定程度上提升性能。



#### 4. repartition 解决SparkSQL低并行度问题

`set spark.default.parallelism=500;`对于SparkSQL是不生效的，用户设置的并行度只对于SparkSQL以外的所有Spark的stage生效。

Spark SQL的并行度不允许用户自己指定，Spark SQL自己会默认根据 hive 表对应的 HDFS 文件的 split 个数自动设置 Spark SQL 所在的那个 stage 的并行度，用户自己通spark.default.parallelism参数指定的并行度，只会在没Spark SQL的stage中生效。

由于Spark SQL所在stage的并行度无法手动设置，如果数据量较大，并且此stage中后续的transformation操作有着复杂的业务逻辑，而Spark SQL自动设置的task数量很少，这就意味着每个task要处理为数不少的数据量，然后还要执行非常复杂的处理逻辑，这就可能表现为第一个有 Spark SQL 的 stage 速度很慢，而后续的没有 Spark SQL 的 stage 运行速度非常快。

为了解决SparkSQL无法设置并行度和task数量的问题，可以使用repartition算子。

<img src="source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20210810102807976-8562489.png" alt="image-20210810102807976" style="zoom:50%;" />

Spark SQL这一步的并行度和task数量肯定是没有办法去改变了，但是，对于Spark SQL查询出来的RDD，立即使用repartition算子，去重新进行分区，这样可以重新分区为多个partition，从repartition之后的RDD操作，由于不再涉及 Spark SQL，因此 stage 的并行度就会等于你手动设置的值，这样就避免了 Spark SQL 所在的 stage 只能用少量的 task 去处理大量数据并执行复杂的算法逻辑。

#### 5. reduceByKey预聚合

实际生产中，尽量使用带有预聚合的算子，而避免使用（groupByKey这类没有预聚合的算子）

所谓预聚合：就是在map端就进行一次combine操作。



### 7.6 Shuffle调优

#### 1. 调节map端缓冲区大小

在 Spark 任务运行过程中，如果 shuffle 的map端处理的数据量比较大，但是map端缓冲的大小是固定的，可能会出现map端缓冲数据频繁spill溢写到磁盘文件中的情况，使得性能非常低下，通过调节map端缓冲的大小，可以避免频繁的磁盘 IO 操作，进而提升 Spark 任务的整体性能。

map端缓冲的默认配置是`32KB`, 如果每个task处理640KB的数据，那么会发生640/32 = 20次溢写，如果每个task处理64000KB的数据，机会发生64000/32=2000此溢写，这对于性能的影响是非常严重的。

```scala
val conf = new SparkConf().set("spark.shuffle.file.buffer", "64");
```

#### 2. 调节reduce端缓冲区大小

Spark Shuffle 过程中，shuffle reduce task 的 buffer缓冲区大小决定了reduce task 每次能够缓冲的数据量，也就是每次能够拉取的数据量，如果内存资源较为充足，适当增加拉取数据缓冲区的大小，可以减少拉取数据的次数，也就可以减少网络传输的次数，进而提升性能。

reduce端数据拉取缓冲区的大小可以通过spark.reducer.maxSizeInFlight参数进行设置，默认为48MB

```scala
val conf=new SparkConf().set("spark.reducer.maxSizeInFlight", "96");
```

#### 3. 调节reduce端拉取数据重试次数

```scala
val conf = new SparkConf().set("spark.shuffle.io.maxRetries", "6");
```

Spark Shuffle 过程中，reduce task 拉取属于自己的数据时，如果因为网络异常等原因导致失败会自动进行重试。对于那些包含了特别耗时的 shuffle 操作的作业，建议增加重试最大次数（比如60次），以避免由于 JVM 的full gc 或者网络不稳定等因素导致的数据拉取失败。在实践中发现，对于针对超大数据量（数十亿~上百亿）的shuffle 过程，调节该参数可以大幅度提升稳定性。

reduce 端拉取数据重试次数可以通过spark.shuffle.io.maxRetries参数进行设置，该参数就代表了可以重试的最大次数。如果在指定次数之内拉取还是没有成功，就可能会导致作业执行失败，默认为3，

#### 4. 调节reduce端拉取数据等待间隔

Spark Shuffle 过程中，reduce task 拉取属于自己的数据时，如果因为网络异常等原因导致失败会自动进行重试，在一次失败后，会等待一定的时间间隔再进行重试，可以通过加大间隔时长（比如60s），以增加shuffle操作的稳定性。

reduce端拉取数据等待间隔可以通过spark.shuffle.io.retryWait参数进行设置，默认值为5s，

```scala
val conf = new SparkConf()
  .set("spark.shuffle.io.retryWait", "60s")
```

#### 5. 调节SortShuffle排序操作阈值

对于SortShuffleManager，如果shuffle reduce task的数量小于某一阈值则shuffle write过程中不会进行排序操作，而是直接按照未经优化的HashShuffleManager的方式去写数据，但是最后会将每个task产生的所有临时磁盘文件都合并成一个文件，并会创建单独的索引文件。

当你使用SortShuffleManager时，如果的确不需要排序操作，那么建议将这个参数调大一些，大于shuffle read task的数量，那么此时map-side就不会进行排序了，减少了排序的性能开销，但是这种方式下，依然会产生大量的磁盘文件，因此shuffle write性能有待提高。 SortShuffleManager排序操作阈值的设置可以通过spark.shuffle.sort. bypassMergeThreshold这一参数进行设置，默认值为200，

```scala
val conf=new SparkConf().set("spark.shuffle.sort.bypassMergeThreshold", "400");
```

### 7.7 内存调优

增大堆外内存`--conf spark.executor.memoryOverhead=2048;`



## 8. Spark数据倾斜解决方案

Spark 中的数据倾斜问题主要指shuffle过程中出现的数据倾斜问题，是由于不同的key对应的数据量不同导致的不同task所处理的数据量不同的问题。

例如，reduce点一共要处理100万条数据，第一个和第二个task分别被分配到了1万条数据，计算5分钟内完成，第三个task分配到了98万数据，此时第三个task可能需要10个小时完成，这使得整个Spark作业需要10个小时才能运行完成，这就是数据倾斜所带来的后果。

注意，要区分开数据倾斜与数据量过量这两种情况，数据倾斜是指少数task被分配了绝大多数的数据，因此少数task运行缓慢；数据过量是指所有task被分配的数据量都很大，相差不多，所有task都运行缓慢。

数据倾斜的表现：

1. Spark 作业的大部分 task 都执行迅速，只有有限的几个task执行的非常慢，此时可能出现了数据倾斜，作业可以运行，但是运行得非常慢；

2. Spark 作业的大部分task都执行迅速，但是有的task在运行过程中会突然报出OOM，反复执行几次都在某一个task报出OOM错误，此时可能出现了数据倾斜，作业无法正常运行。

### 8.1 聚合原数据

1. 避免shuffle过程
   - 可以在hive端直接将每个key对应的数据拼接成一个大大的字符串，避免shuffle
2. 缩小key的粒度（增大数据倾斜的可能性，降低每个task的数据量）
3. 增大key的粒度（减少数据倾斜的可能性，增大每个task的数据量）

### 8.2 过滤掉导致倾斜的key

有些时候，导致数据倾斜的key可能为null，提前将这个无用的导致的倾斜的key过滤掉。

如果有真实的key导致了数据倾斜，我们可以单独抽取出来这个key，通过拼接随机数后缀，将key进行打散，进而可以在一定程度上缓解数据倾斜所带来的问题。

### 8.3 提高shuffle操作中的reduce并行度

当方案一和方案二对于数据倾斜的处理没有很好的效果时，可以考虑提高shuffle过程中的reduce端并行度，reduce端并行度的提高就增加了reduce端task的数量，那么每个task分配到的数据量就会相应减少，由此缓解数据倾斜问题。

对于Spark SQL中的shuffle类语句，比如group by、join等，需要设置一个参数，即`spark.sql.shuffle.partitions`，该参数代表了shuffle read task的并行度，该值默认是200，对于很多场景来说都有点过小。

提高reduce端并行度并没有从根本上改变数据倾斜的本质和问题（方案一和方案二从根本上避免了数据倾斜的发生），只是尽可能地去缓解和减轻shuffle reduce task的数据压力，以及数据倾斜的问题，适用于有较多key对应的数据量都比较大的情况。

该方案通常无法彻底解决数据倾斜，因为如果出现一些极端情况，比如某个key对应的数据量有100万，那么无论你的task数量增加到多少，这个对应着100万数据的key肯定还是会分配到一个task中去处理，因此注定还是会发生数据倾斜的。所以这种方案只能说是在发现数据倾斜时尝试使用的第一种手段，尝试去用最简单的方法缓解数据倾斜而已，或者是和其他方案结合起来使用。

在理想情况下，reduce端并行度提升后，会在一定程度上减轻数据倾斜的问题，甚至基本消除数据倾斜；但是，在一些情况下，只会让原来由于数据倾斜而运行缓慢的task运行速度稍有提升，或者避免了某些task的OOM问题，但是，仍然运行缓慢，此时，要及时放弃方案三，开始尝试后面的方案。

###  8.4 使用随机key实现双重聚合

当使用了类似于groupByKey、reduceByKey这样的算子时，可以考虑使用随机key实现双重聚合

<img src="source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20210810111030982-8565032.png" alt="image-20210810111030982" style="zoom:50%;" />



首先，通过map算子给每个数据的key添加随机数前缀，对key进行打散，将原先一样的key变成不一样的key，然后进行第一次聚合，这样就可以让原本被一个task处理的数据分散到多个task上去做局部聚合；

随后，去除掉每个key的前缀，再次进行聚合。

此方法对于有`groupByKey,reduceByKey`这类算子造成的数据倾斜有比较好的效果，仅仅适用于聚合类的shuffle操作，使用范围相对较窄。

### 8.5 将reduce join 转换成map join



<img src="source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20210810111500233-8565301.png" alt="image-20210810111500233" style="zoom:50%;" />

正常情况下，join操作都会执行shuffle过程，并且执行的是reduce join，也就是先将所有相同的key和对应的value汇聚到一个reduce task中，然后再进行join。

普通的join是会走shuffle过程的，而一旦shuffle，就相当于会将相同key的数据拉取到一个shuffle read task中再进行join，此时就是reduce join。

但是一个数据量特别大，一个数据量特别小，这两个数据进行join，就很有可能产生数据倾斜，导致大量的数据key都会聚集到一小部分reduce task中。

**解决办法**

通过将小数据量RDD中的数据 broadcast广播出去，然后通过map join的方式实现与reduce join相同的效果。此时就不会发生shuffle操作，也就不会发生数据倾斜了。

<img src="source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20210810111914642-8565556.png" alt="image-20210810111914642" style="zoom:50%;" />

### 8.6 sample采样对倾斜key单独进行join



### 8.7 使用随机数以及扩容进行join

如果在进行join操作时，RDD中有大量的key导致数据倾斜，那么进行分拆key也没什么意义，此时就只能使用最后一种方案来解决问题了，对于join操作，我们可以考虑对其中一个RDD数据进行扩容，另一个RDD进行稀释后再join。

我们会将原先一样的key通过附加随机前缀变成不一样的key，然后就可以将这些处理后的“不同key”分散到多个task中去处理，而不是让一个task处理大量的相同key。

这一种方案是针对有大量倾斜key的情况，没法将部分key拆分出来进行单独处理，需要对整个RDD进行数据扩容，对内存资源要求很高。

**核心思想**

![image-20210810153402154](source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20210810153402154-8580843.png)

选择一个RDD，使用flatMap进行扩容，对每条数据的key添加数值前缀（1~N的数值），将一条数据映射为多条数据；（扩容）

选择另外一个RDD，进行map映射操作，每条数据的key都打上一个随机数作为前缀（1~N的随机数）；（稀释）









# Hive 复习

### 1. 外部表与内部表

内部表又叫做管理表，创建表时不做任何指定，默认创建的就是内部表。想要创建外部表，则需要使用External进行修饰。

|              | 内部表                                                       | 外部表                                                       |
| ------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 数据存储位置 | 内部表数据存储的位置由`hive.metastore.warehouse.dir`参数指定，默认情况下表的数据存储在HDFS的`/user/hive/warehouse/数据库名.db/表名/`目录下 | 外部表数据的存储位置创建表时由`Location`参数指定；           |
| 导入数据     | 在导入数据到内部表，内部表将数据移动到自己的数据仓库目录下，数据的声明周期有Hive来进行管理 | 外部表不会将数据移动到自己的数据仓库目录下，只是在元数据中存储了数据的位置。 |
| 删除表       | 删除元数据（metadata）和文件                                 | 只删除元数据（metadata）                                     |



### 2. 分区与分桶

- 分区是将数据按照目录进行划分，将Hive的表的数据进行划分，可以在查询的时候只查询一部分的数据，从而避免全量查询。

- 分桶是在分区的基础上更细粒度的将数据进行划分。

  将整个数据内容按照某列取hash值，对桶的个数取模的方式决定该条记录存放在哪个桶中；具有相同hash值的数据进入到同一个桶，形成同一个文件

  ```sql
  set hive.enforce.bucketing=true;
  set mapreduce.job.reduces=4; # 分成4个桶
  
  create table user_buckets(id int, name string)
  clustered by(id) into 4 buckets
  row format delimited fields terminated by '\t';
  ```

  

### 3. Hive 抽样

1. 数据块抽样

   1. tablesample(n percent)

      > 根据hive表数据的大小按比例抽取数据，并保存到新的hive表中。

   2. tablesample(n M)

      > 指定抽样数据的大小，单位为M

   3. tablesample(n rows)

      > 指定抽样数据的行数，其中n代表每个map任务均取n行数据，map数量可通过hive表的简单查询语句确定

2. 分桶抽样

   hive中分桶其实就是根据某一个字段Hash取模，放入指定数据的桶中，比如将表table_1按照ID分成100个桶，其算法是hash(id) % 100，这样，hash(id) % 100 = 0的数据被放到第一个桶中，hash(id) % 100 = 1的记录被放到第二个桶中。创建分桶表的关键语句为：CLUSTER BY语句。

   > 分桶抽样语法：
   >
   > `tablesample(bucket x out of y [on colname])`
   >
   > 其中x是要抽样的桶编号，编号从1开始，colname表示抽样的列，y表示桶的数量。
   >
   > 例如：将表随机分成10组，抽取其中的第一个桶的数据
   >
   > `select * from table1 tablesample(bucket 1 out of 10 on rand())`
   >
   > y必须是table总bucket数的倍数或者因子。hive根据y的大小，决定抽样的比例。例如，table总共分了4份，当y=2时，抽取(4/2=)2个bucket的数据，当y=8时，抽取(4/8=)1/2个bucket的数据。
   >
   > x表示从哪个bucket开始抽取，如果需要取多个分区，以后的分区号为当前分区号加上y。例如，table总bucket数为4，tablesample(bucket 1 out of 2)，表示总共抽取（4/2=）2个bucket的数据，抽取第1(x)个和第3(x+y)个bucket的数据。
   >
   > 注意：x的值必须小于等于y的值，否则
   >
   > FAILED: SemanticException [Error 10061]: Numerator should not be bigger than denominator in sample clause for table stu_buck

3. 随机抽样

   1. 使用`rand()函数`进行随机抽样，limit关键字限制抽样返回的数据，其中rand函数前的distribute 和sort 关键字可以保证数据在mapper和reducer阶段是随机分布的。

      ```sql
      select * from table_n where col=xxx distribute by rand() sort by rand() limit num;
      
      -- 还可以使用order by
      
      select * from table_n where col=xxx order by rand() limit num;
      ```

      

### 4. order by 、 distribute by 、sort by 和 cluster by 四个by的区别

#### 1. order by

全局排序，只有一个reduce；

缺点：当数据量非常大的时候，耗时太长，效率低下，适用于数据量较小的场景。

有点：数据全局排序

```sql
select * from emp order by sal desc;
```

当使用order by 时，默认只走一个reduce，和设置多少个reduce个数无关。

#### 2. sort by

对每一reduce内部的数据进行排序，全局结果集来说不是排序的，即只能保证每一个reduce输出的文件中的数据是按照规定的字段进行排序的；适用于数据量较大，但对排序要求不严格的场合，可以大幅度提升执行效率。

```sql
set mapreduce.job.reduces=3;

select * from emp sort by deptno desc;
```

需要预先设置reduce个数，结果各个reduce文件内部有序，全局无序

#### 3. distribute by

控制特定的key到指定的reducer，方便后续的聚集操作。类似于MR中partiton，一般会结合sort by使用；这边需要设置reduces的数量为分区的数量，否则不会启动相应的reducer去进行任务的执行，这最终会导致不能完全分区。

```java
set mapreduce.job.reduces=3;
select * from emp distribute by deptno sort by empno desc;
```

distribute by的分区规则是根据分区字段的hash码与reduce的个数进行取模后，余数相同的分到一个分区。

hive要求distribute by语句要写在sort by语句之前。



#### 4. Cluster By

当distribute by和sort by字段相同时，可以使用cluster by方式。

cluster by除了具有distribute by的功能外还兼具sort by的功能。但是排序只能是升序排序，不能指定排序规则为ASC或者DESC。
1）以下两种写法等价

```sql
hive (default)> select * from emp cluster by deptno;
hive (default)> select * from emp distribute by deptno sort by deptno;
```

### 5. 函数

#### 5.1 系统内置函数

1. **nvl**：NVL：给值为 NULL 的数据赋值，它的格式是 NVL( value，default_value)。它的功能是如 果 value 为 NULL，则 NVL 函数返回 default_value 的值，否则返回 value 的值，如果两个参数 都为 NULL  ，则返回 NULL。

2. ```sql
   case 	when 条件 then
   		when 条件	then
   		else	
   end	as 字段名
   ```

3. ```sql
   concat_ws('分隔符',字符串1，字符串2,或者 collect_set())
   ```

4. **炸裂函数**：explode

   ```sql
   select 
   		id
   		,catagory
   from 	table
   where	...
   and		...
   lateral view
   		explode(集合) temp_view as catagory
   ;
   ```

5. split()

   ```sql
   split(category,',')
   ```

#### 5.2 窗口函数over()

##### LAG(col, n, default_val)

当前行的前第n行数据

```scala
// 查询顾客上次购买时间
spark.sql(
  """
    |select
    | name, orderdate, cost,
    | lag(orderdate, 1, '1997-03-15') over(partition by name order by orderdate)
    |from business
    |""".stripMargin
).show(false)


结果：
+----+----------+----+----------------------------------------------------------------------------------------------------------------------------------+
|name|orderdate |cost|lag(orderdate, 1, 1997-03-15) OVER (PARTITION BY name ORDER BY orderdate ASC NULLS FIRST ROWS BETWEEN 1 PRECEDING AND 1 PRECEDING)|
+----+----------+----+----------------------------------------------------------------------------------------------------------------------------------+
|mart|2017-04-08|62  |1997-03-15                                                                                                                        |
|mart|2017-04-09|68  |2017-04-08                                                                                                                        |
|mart|2017-04-11|75  |2017-04-09                                                                                                                        |
|mart|2017-04-13|94  |2017-04-11                                                                                                                        |
|jack|2017-01-01|10  |1997-03-15                                                                                                                        |
|jack|2017-01-05|46  |2017-01-01                                                                                                                        |
|jack|2017-01-08|55  |2017-01-05                                                                                                                        |
|jack|2017-02-03|23  |2017-01-08                                                                                                                        |
|jack|2017-04-06|42  |2017-02-03                                                                                                                        |
|tony|2017-01-02|15  |1997-03-15                                                                                                                        |
|tony|2017-01-04|29  |2017-01-02                                                                                                                        |
|tony|2017-01-07|50  |2017-01-04                                                                                                                        |
|neil|2017-05-10|12  |1997-03-15                                                                                                                        |
|neil|2017-06-12|80  |2017-05-10                                                                                                                        |
+----+----------+----+----------------------------------------------------------------------------------------------------------------------------------+
```

##### LEAD(col, n, default_val)

当前行的后第n行数据



##### ntile(n)

把有序窗口的行分发到指定数据的组中，各个组有编号，编号从 1 开始，对 于每一行，NTILE 返回此行所属的组的编号。

可用于返回前%多少的数据，例如分5组，取前20%，那么取组号为1的即可。



##### rank排名函数

三种常用的排名函数

1. rank()
   - 排序相同时会重复，总数不会变
   - 例如：1,2,2,4
2. row_number()
   - 根据顺序计算
   - 例如：1,2,3,4
3. dense_rank()
   - 排序相同时会重复，总数会减少
   - 例如：1,2,2,3



### 6. Hive调优

#### 6.1 开启map端预聚合

```sql
-- 是否在map端及你行聚合，默认为True
set hive.map.aggr=true;
-- 在Map端进行预聚合操作的条目数
set hive.groupby.mapaggr.checkinterval = 100000;
-- 有数据倾斜时进行负载均衡（默认为false）
set hive.groupby.skewindata = true;
```

默认情况下，使用**group by**，Map阶段同一key的数据会分发给一个reduce，当一个key数据过大时，就会产生数据倾斜。

这是可以开启map端预聚合，可以在map端先做一部分的聚合，这样就在一定程度上可以较少进入同一个reducer中的数据了。



#### 6.2 count（distinct ） 优化

由于`count(distinct)`操作需要用一个reduce task来完成，这一个reduce需要处理的数据量太大，就会导致整个Job很难完成。

一般`count distinct`可使用`先group by， 再count`的方式替换。

因为`group by`可以通过增加reduces的数量，加快执行速度

`set mapreduce.job.reduces=5;`



#### 6.3 避免笛卡尔积



#### 6.4 行列过滤

在进行`join`的时候, 尽量先使用`where`条件过滤掉一些不合符业务场景的数据，从而减少join前后的两张表的数据量。

如果，先join后过滤，那么就会先全表关联，之后再过滤



查询列的时候，避免使用*;（实际的SQL规范中，也是禁止使用**）



#### 6.5 合理设置map和reduce的数量

1. 通常情况下，作业会通过input的目录产生一个或者多个map任务，主要决定因素取决于input的文件总个数，input的文件大小，集群设置的文件块大小。（切片数）
2. 是不是map数越多越好呢？
   - 答案hi否定
   - 如果一个任务有很多小文件（远远小于块大小 128m），则每个小文件也会被当做一个块，用一个 map 任务来完成，而一个 map 任务启动和初始化的时间远远大 于逻辑处理的时间，就会造成很大的资源浪费。而且，同时可执行的 map 数是受限的。
3. 是不是保证每个 map 处理接近 128m 的文件块，就高枕无忧了？
   - 答案也是不一定。比如有一个 127m 的文件，正常会用一个 map 去完成，但这个文件只 有一个或者两个小字段，却有几千万的记录，如果 map 处理的逻辑比较复杂，用一个 map 任务去做，肯定也比较耗时。

##### 复杂文件增加map数

当input的文件都很大，任务逻辑复杂，map执行非常慢的时候，可以考虑增加map数，来使得每个map处理的数据量减少，从而提高任务的执行效率。

##### 小文件行合并

在map执行前，对于小文件进行合并，减少不必要的map数（比如一个文件只有一条数据，但是不合并的情况就会启动一个map来处理这个一条数据，但是map

的启动和加载时间远远大于处理时间，反而耗时）

#### 6.6 并行执行

```sql
set hive.exec.parallel=true; -- 打开任务并行执行
set hive.exec.parallel.thread.number=16; -- 同一个SQL允许最大并行度，默认为8
```





# Zookeeper复习

## 1. Zookeeper内部原理

### 1.1 选举机制

1. <font color=red>半数机制：集群中半数以上机器存活，集群可用。所以Zookeeper适合安装奇数台服务器。</font>
2. Zookeeper虽然在配置文件中并没有指定**Master**和**Slave**。但是，Zookeeper工作时，是有一个节点为Leader，其他则为Follower，Leader是通过内部的选举机制临时产生的。

**三个核心的选举原则**

1. Zookeeper集群中只有超过半数以上的服务器启动，集群才能正常工作；
2. 在集群正常工作之前，`myid`小的服务器给`myid`的的服务器投票，知道集群正常工作，选出`leader`；
3. 选出`Leader`之后，之前的服务器状态由`Looking`改变为`Following`，以后的服务器都是`Follower`。

以一个简单的例子来说明整个选举的过程：

![img](source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/webp-20210810162423999)

假设有五台服务器组成的Zookeeper集群，他们的id从1~5，同时他们都是最新启动的，也就是没有历史数据，在存放数据量这一点上，都是一样的。假设这些服务器依序启动。

1. 服务器1启动，发起一次选举

   > 服务器1投自己一票，此时集群不够半数以上（3票），选举无法完成；
   >
   > 服务器1状态保持为`LOOKING`

2. 服务器2启动，再发起一次选举

   > 服务器2投自己一票，然后服务器1与服务器2交换投票信息，此时服务器1发现服务器2的id比自己的大，更改选票投给服务器2.
   >
   > 此时服务器1票数0票，服务器2票数2票，不够半数以上（3票），选举无法完成；（服务器1的票数在投票结束后，清零）
   >
   > 服务器1，2保持`LOOKING`

3. 服务器3启动，发起一次选举

   > 与上面过程一样，服务器1和服务器2先投自己一票，服务器3也投自己一票。
   >
   > 在交换投票信息的时候，发现此时还是LOOKING状态（还没有选取出leader），且服务器3的id最大，那么服务器3就会得到3票，此时服务器3的票数已经超过半数了（3票）
   >
   > 服务器1，2更新状态为FOLLOWING，服务器3更改状态为LEADING；

4. 服务器4启动，发起一次选举

   > 此时服务器1，2，3已经不是`LOOKING`状态，不会改变选票信息。交换选票信息结果：服务器3为3票，服务器4为1票。
   >
   > 此时服务器4服从多数，更改选票信息为服务器3；服务器3此时有4票
   >
   > 服务器4并更改状态为`FOLLOWING`;

5. 服务器5启动，同4一样投票给3，此时服务器3一共5票，服务器5为0票；

   > 服务器5更改状态为`FOLLOWING`

### 2. 节点类型

- 持久（Persistent）：客户端和服务器端断开连接后，创建的节点不删除
- 短暂（Ephemeral）：客户端与服务器端断开连接后，创建的节点自己会自动删除

说明：创建znode时，设置顺序标识，znode名称后会附加一个值，顺序号是一个单调递增的计数器，由父节点维护。

<font color=red>注意：分布式系统中，顺序号可以被用于为所有的事件进行全局排序，这样客户端可以通过顺序号推断事件的顺序。</font>

### 3. Stat结构体







### 4. 监听器原理

![img](source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/src=http%253A%252F%252Fimage.bubuko.com%252Finfo%252F202002%252F20200209175734687167.png&refer=http%253A%252F%252Fimage.bubuko.com&app=2002&size=f9999,10000&q=a80&n=0&g=0n&fmt=jpeg)

1. 首先要有一个main()线程
2. 在main线程中创建Zookeeper客户端，这时就会创建两个线程：一个负责网络连接通信（connect），一个负责监听（Listener）。
3. 通过connect线程，将注册的监听事件发送给Zookeeper。
4. 在Zookeeper的注册监听器列表中将注册的监听事件添加到列表中
5. Zookeeper监听到数据或路径变化，就会将这个消息发送给listener线程
6. listener线程内部调用了process()方法。

常见的监听：

1. 监听节点数据的变化

   `get path[watch]`

2. 监听子节点增减的变化

   `ls path[watch]`



### 5. Zookeeper写数据流程

![img](source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/src=http%253A%252F%252Fimg2018.cnblogs.com%252Fi-beta%252F1201165%252F202002%252F1201165-20200226224404735-1637370414.png&refer=http%253A%252F%252Fimg2018.cnblogs.com&app=2002&size=f9999,10000&q=a80&n=0&g=0n&fmt=jpeg)

1. client向zookeeper的Server1上写数据，发送一个写请求。
2. 如果server1不是leader，那么server1会把接收到的请求进一步转发给Leader，因为每个Zookeeper的Server里面有一个Leader。这个Leader会将写请求广播给各个Server，比如Server1，Server2，Server4，Server5；各个Server写成功后就会通知Leader。
3. 当Leader收到大多数Server数据写成功了，那么就说明数据写成功了。写成功之后，Leader会告诉Server1数据写成功了。
4. Server1会进一步通知Client数据写成功了，这时救人位整个操作成功了。

### 6. Zookeeper的常用命令

| 命令基本语法    | 功能描述                                                |
| --------------- | ------------------------------------------------------- |
| ls path[watch]  | 使用ls命令来查看当前znode中所包含的内容                 |
| ls2 path[watch] | 查看当前节点数据并能看到更新次数等数据                  |
| create          | 普通创建<br />-s含有序列<br />-e 临时（重启或超时消失） |
| get path[watch] | 获得节点的值                                            |
| set             | 设置节点的具体值                                        |
| stat            | 查看节点状态                                            |
| delete          | 删除节点                                                |
| rmr             | 递归删除节点                                            |

















# Kafka复习

kafka是一个分布式的，基于消息订阅-发布模式的消息队列。Kafka对消息保存时根据Topic进行归类。每个消费者组可以订阅不同的Topic；且同一个消费者组的一个消费者只能消费一个分区中的数据。

无论是Kafka集群，还是consumer都依赖于**zookeeper**集群保存一些meta信息，来保证系统可用性。

**点对点模式（一对一，消费者主动拉取数据，消息收到后消息清除）**

![在这里插入图片描述](source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3Mjc2ODM1,size_16,color_FFFFFF,t_70.png)

消息生产者生产消息发送到Queue，然后消息消费者从Queue中取到并且消费消息。消息被消费后，queue中不再有存储，所以消息消费者不可能消费已经被消费的消息。Queue支持存在多个消费者，但是对一个消费者而言，只会有一个消费者可以消费。

**发布/订阅模式（一对多，消费者消费数据之后不会清除消息）**

![在这里插入图片描述](source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3Mjc2ODM1,size_16,color_FFFFFF,t_70-20210811102106660.png)

消息生产者（发布）将消息发布到topic中，同时有多个消息消费者（订阅）消费该消息。和点对点方式不同，发布到topic的消息会被所有订阅者消费。

而发布/订阅模式的消息队列还可以分为两种：

1. 队列推送消息给消费者
2. 消费者主动拉取消息

如果采用第一种 队列推送消息给消费者，但是每个消费者的消费速度是不一定一致的，因此会发生消息的浪费情况。

在Kafka中则采用的第二种：消费者主动拉取消息。

## 1. 消息队列的作用（为什么使用消息队列？）

### 削峰

举个例子：双十一用户，假设在某一时间段内用户的下单请求峰值达到了一亿多，如果不使用消息队列，服务器端就要同时处理这一亿多的请求，如果服务器性能不够，就会崩掉！
如果使用了消息队列，就可以实现削峰的作用，限制同时访问服务器的请求数量，从而降低服务器端的负担。



### 解耦

通常生产者和消费者的业务逻辑是不同的，当消费者的消费业务逻辑发生变化时，如果不使用消息队列，就需要把整个生产-消费的逻辑改变。
如果使用了消息队列，生产者和消费者就实现了解耦，当需求发生变化时，只需要更改需要更改的一方，而不是修改整个 生产-消费的过程。



### 异步

很多时候，用户不想也不需要立即处理消息。消息队列提供了异步处理机制，允许用户
把一个消息放入队列，但并不立即处理它。想向队列中放入多少消息就放多少，然后在需要
的时候再去处理它们。



## 2. Kafka的特点

- 类似于消息队列和商业的消息系统，kafka提供对流式数据的发布和订阅
- kafka提供一种持久的容错的方式存储流式数据
- kafka拥有良好的性能，可以及时地处理流式数据



## 3. Kafka基础架构

![在这里插入图片描述](source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3Mjc2ODM1,size_16,color_FFFFFF,t_70-20210811102926988.png)

1. producer：消息生产者，就是向Kafka broker发消息的客户端；
2. consumer：消息消费者，想kafka broker拉取消息的客户端；
3. Topic：可以理解为一个队列；
4. consumer group（CG）：













# Flink复习















# 数仓复习





















































































